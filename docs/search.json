[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Beispielhafte Auswertungen",
    "section": "",
    "text": "Was wollen wir hier gemeinsam machen? Ich werde in den folgenden Kapiteln Daten auswerten. Manche Sachen sind dort auch doppelt drin oder aber kennst du schon aus meinen Vorlesungen. Mir macht es einfach Spaß Daten auszuwerten und ich muss mir die Sachen immer aufschreiben, damit ich auch das Thema durchdringe. Daher dient die Sammlung an Beispielen zum einen als ein Fundus für Beispiel in meinen Vorlesungen oder aber für dich als eine Inspiration was alles möglich ist. Eventuell findest du ja dein Problem wieder… oder hast eine andere Idee wie du noch deine Daten auswerten könntest. Guck also einfach mal durch ob du was passendes findest.\nAlle Beispiele sind aus meiner beratenden Tätigkeit hier an der Hochschule Osnabrück entstanden. Keins der Beispiele ist so echt. Ich habe die Daten geändert und Behandlungen entfernt oder dazu genommen. Teilweise habe ich mir die Geschichte um die Fragestellung auch einfach ausgedacht. Also die Beispiel sind dann biologisch inhaltlich vielleicht dann doch eher Unsinn, die statistsiche Methode darum sollte aber meistens korrekt sein."
  },
  {
    "objectID": "index.html#sec-contact-mail",
    "href": "index.html#sec-contact-mail",
    "title": "Beispielhafte Auswertungen",
    "section": "Kontakt",
    "text": "Kontakt\nWie erreichst du mich? Am einfachsten über die gute, alte E-Mail. Bitte bachte, dass gerade kurz vor den Prüfungen ich mehr E-Mails kriege. Leider kann es dann einen Tick dauern.\n\n\n\n\n\nEinfach an j.kruppa@hs-osnabrueck.de schreiben. Du findest hier auch eine kurze Formulierungshilfe. Einfach auf den Ausklapppfeil klicken.\n\n\n\n\n\n\nE-Mailvorlage mit beispielhafter Anrede\n\n\n\n\n\nHallo Herr Kruppa,\n… ich belege gerade Ihr Modul Modulname und hätte eine Bitte/Frage/Anregung…\n… ich benötige Hilfe bei der Planung/Auswertung meiner Bachelorarbeit…\nMit freundlichen Grüßen\nM. Muster"
  },
  {
    "objectID": "example-analysis-preface.html",
    "href": "example-analysis-preface.html",
    "title": "Beispielhafte Auswertungen",
    "section": "",
    "text": "Version vom May 13, 2023 um 10:29:29\n\n\n\n\n\n\nBeispielhafte Auswertungen per Video\n\n\n\nDu findest auf YouTube in der Playlist Spielweise in R (Level 3) viele der Analysen hier einmal als Video. Ich werde zwar alles nochmal hier als Text aufschreiben, aber manchmal ist das Sehen und Hören dann einfacher.\n\n\nWir wollen folgende R Pakete in diesem Skript nutzen.\n\npacman::p_load(tidyverse, magrittr, readxl, janitor,\n               broom, multcomp, emmeans, effectsize, report,\n               see, metR, parameters, multcompView,\n               modelsummary, rstatix, corrplot, psych,\n               ordinal, rcompanion, lme4, performance,\n               scales, lubridate,\n               conflicted)\n## resolve some conflicts with same function naming\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"chisq.test\", \"stats\")\nconflict_prefer(\"%+%\", \"ggplot2\")\nconflict_prefer(\"eta_squared\", \"effectsize\")\n\nWorum geht es in diesen Kapiteln? Ich rechne hier fröhlich Dinge und präsentiere dann die Ergebnisse. Das heißt, du findest hier beispielhafte Auswertungen, die eventuell auch deine Problemstellung betreffen. Da ich selber meist keine Ahnung von der Biologie hinter den Experimenten habe, musst du mir nachsehen, dass ich hier nie zu einem echten biologischen Schluss oder Auswertung komme.\n\nIn Kapitel 1 schauen wir uns einmal die Erstellung eines Isoplethendiagramms für Münster & Osnabrück aus den frei verfügbaren Daten des Deutschen Wetterdienstes an.\nIn Kapitel 2 rechnen wir einmal ein simples Beispiel für Zähldaten in zwei Gruppen. Wir haben hier nicht wiederholt gezählt, sondern nur jeweils einmal an einer Stelle.\nIn Kapitel 3 rechnen wir nochmal eine zweifaktorielle ANOVA mit Interaktionsterm durch. Wir kriegen dann unser Compact letter display wie auch die Konfidenzintervalle wieder.\nIn Kapitel 4 rechnen wir einen Games Howell Test für normalverteilte Daten mit Varianzheterogenität. Hier hauen wir ein wenig auf die Pauke und rechnen alles in wenigen Zeilen mit der Funktion map(). War eine spaßige Auswerung für mich, da ich hier mal wieder programmieren üben konnte.\nIn Kapitel 5 schauen wir uns ein größeres Beispiel für die Analyse von Wurzelbonituren an. Im Prinzip geht das natürlich auch alles für jede andere Bonitur. Wenn du eine andere Bonitur hast, kannst du natürlich auch den Weg der Analyse gehen.\nIn Kapitel 6 haben wir ein etwas komplexeres Beispiel für vier Outcomes, die an acht Terminen gemessen wurden. Wir sehen hier einmal, wie die Daten eingelesen werden und ein Korrelationsplot über die vier Outcomes erstellt wird.\nIn Kapitel 7 schauen wir uns den Tukey Test für paaweise Vergleiche einmal an. Wir nutzen hier die schnelle Variante mit der Funktion aov(). Für das Compact letter display müssen wir uns dann ein wenig strecken, aber es geht dann auch.\nIn Kapitel 8 haben wir nochmal ein schönes Beispiel für eine Analyse mit mehreren Faktoren und verschiedenen Outcomes. Wir haben also einen Blumenstrauß an Daten, die wir sortieren und auswerten müssen. Da ich selber keine Ahnung vom Pflügen habe, gehen wir die Sache distanziert durch.\nIn Kapitel 9 schauen wir uns ein lineares gemischtes Modell an. Das heißt, wir wollen einen Gruppenvergleich rechnen und dabei die verschiedenen Zeitpunkte mit berücksichtigen. Anstatt also jeden Zeitpunkt einzeln, rechnen wir alle Zeitpunkte zusammen.\nIn Kapitel 10 wollen wir uns die Konzentration von drei Spurenelementen als Outcome in Blättern und Stielen der Spinatpflanze anschauen. Wir haben hierfür dann die Konzentration von Fe, Cd und Zn gemessen. Insgesamt gab es sieben verschiedene Behandlungsformen.\nIn Kapitel 11 schauen wir uns die Daten eines automatischen Log-Gerätes einmal an. Wir haben die Daten von über 35000 Messungen in über knapp zwei Wochen erhoben. Wir wollen jetzt diese Daten auf verschiedene Art und Weise visualisieren und die Ergebnisse der Plots vergleichen.\nIn Kapitel 12 betrachten wir das Wachstum von Pilzen auf zwei Grasflächen und vier Blöcken. Insgesamt wurden fünfzehn Behandlungen auf den Rasen aufgebracht, was natürlich zu einer gewaltigen Anzahl an Vergleichen führt.\nIn Kapitel 13 Steuerung der vegetativen Entwicklung krautiger Pflanzen (44B0608) - Teil 1\nIn Kapitel 14 Steuerung der vegetativen Entwicklung krautiger Pflanzen (44B0608) - Teil 2\nIn Kapitel 15 Projektkurs im Master MLP"
  },
  {
    "objectID": "example-analysis-01.html",
    "href": "example-analysis-01.html",
    "title": "1  Isoplethendiagramm für Münster & Osnabrück",
    "section": "",
    "text": "Version vom Mai 10, 2023 um 14:10:51\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr)\n\nIm Folgenden zeige ich ein Beispiel für die Nutzung der entgeltfreien Informationen auf der DWD-Website. Wir finden dort auf der Seite die Klimadaten für Deutschland und natürlich auch die Daten für Münster/Osnabrück. Ich habe mir flux die Tageswerte runtergeladen und noch ein wenig den Header der txt-Datei angepasst. Du findest die Datei day_values_osnabrueck.txt wie immer auf meiner GitHub Seite. Du musst dir für andere Orte die Daten nur entsprechend zusammenbauen. Am Ende brauchen wir noch die Informationen zu den Tages- und Monatswerten damit wir auch verstehen, was wir uns da von der DWD runtergeladen haben. Ich nutze gleich nur einen Ausschnitt aus den Daten.\n\n\nWenn wir Geocomputation with R machen wollen, dann haben wir natürlich noch viele andere Möglichkeiten. Das verlinkte Buch hilft da weiter.\nDann lesen wir die Daten einmal ein und müssen dann eine Winkelzüge machen, damit wir aus dem Datum JJJJMMDD dann jeweils den Monat und den Tag extrahiert kriegen. Dann müssen wir die Monatszahl und die Tageszahl noch in eine Zahl umwandeln. Sonst geht es schlecht mit dem Zeichnen des Konturplots. Wir nehmen dann die Temperaturen TG, TN, TM und TX um diese Temperaturen in vier Konturplots zu zeigen.\n\nweather_tbl <- read_table(\"data/day_values_osnabrueck.txt\") %>% \n  mutate(JJJJMMDD = as.Date(as.character(JJJJMMDD), \"%Y%m%d\"),\n         day = as.numeric(format(JJJJMMDD, \"%d\")), \n         month = as.numeric(format(JJJJMMDD, \"%m\")), \n         year = as.numeric(format(JJJJMMDD, \"%Y\"))) %>% \n  select(month, day, TG, TN, TM, TX) %>% \n  na.omit() %>% \n  gather(temp, grad, TG:TX) %>% \n  mutate(temp = factor(temp, \n                       labels = c(\"Minimum der Temperatur in 5 cm (TG)\",\n                                  \"Minimum der Temperatur in 2 m (TN)\",\n                                  \"Mittel der Temperatur in 2 m (TM)\",\n                                  \"Maximum der Temperatur in 2 m (TX)\")))\n\nNachdem wir ordentlich an den Daten geschraubt haben können wir jetzt in Abbildung 1.1 die vier Konturplots sehen. Wir mussten noch das Spektrum der Farben einmal drehen, damit es auch mit den Temperaturfarben passt und wir haben noch ein paar Hilfslinien miteingezeichnet.\n\nggplot(weather_tbl, aes(month, day, z = grad)) +\n  theme_minimal() +\n  geom_contour_filled(bins = 13) +\n  geom_contour(binwidth = 2, color = \"black\") +\n  facet_wrap(~ temp, ncol = 2) + \n  scale_fill_brewer(palette = \"Spectral\", direction = -1) +\n  scale_x_continuous(breaks = 1:12) +\n  geom_vline(xintercept = 1:12, alpha = 0.9, linetype = 2) +\n  geom_hline(yintercept = c(5, 10, 15, 20, 25, 30), \n             alpha = 0.9, linetype = 2)\n\n\n\n\nAbbildung 1.1— Konturplot der verschiedenen Temperaturen."
  },
  {
    "objectID": "example-analysis-02.html",
    "href": "example-analysis-02.html",
    "title": "2  Analyse von Anzahlen in zwei Gruppen",
    "section": "",
    "text": "Version vom Mai 10, 2023 um 14:10:54\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, effectsize, see)\n\nIn dieser sehr simplen Analyse haben wir zwei Gruppen vorliegen. Die Gruppe 1 ist hat zwei Level oder Behandlungen abgekürzt mit I und II. Die Gruppe 2 hat insgesamt vier Level oder eben Behandlungen, die wir mit A, B, C und D bezeichnen. Wir haben jetzt für die jeweiligen Kombinationen auf dem Feld etwas gezählt. Wir haben also für jede dieser Kombinationen nur eine Zahl. Es ergbit sich somit die folgende Matrix an Zahlen.\n\nrel_mat <- matrix(c(45, 14, 4, 0,\n                    25, 32, 5, 1), nrow = 2, byrow = TRUE,\n                  dimnames = list(c(\"I\", \"II\"), c(\"A\", \"B\", \"C\", \"D\")))\nrel_mat\n\n    A  B C D\nI  45 14 4 0\nII 25 32 5 1\n\n\nNun können wir den \\(\\mathcal{X}^2\\)-Test nutzen, um zu testen, ob die Zahlen in der Matrix bzw. auf unseren Feld gelcihverteilt sind. Die Nullhypothese lautet, dass es keinen Zusammenhang zwischen der Gruppe 1 und der Gruppe 2 auf dem Feld gibt. Die Zahlen sind also rein zufällig in dieser Anordnung.\n\nchisq.test(rel_mat)\n\n\n    Pearson's Chi-squared test\n\ndata:  rel_mat\nX-squared = 13.869, df = 3, p-value = 0.003089\n\n\nWir erhalten einen sehr kleinen \\(p\\)-Wert mit \\(0.003\\). Wir können daher die Nullhypothese ablehnen, da der \\(p\\)-Wert kleiner ist als das Signifikanzniveau \\(\\alpha\\) mit 5%. Wir haben ein signifikantes Ergebnis. Wir können von einen Zusamenhang zwischen den beiden Gruppen ausgehen.\nMit Cramers V können wir auch noch die Effektstärke für einen \\(\\mathcal{X}^2\\)-Test berechnen.\n\ncramers_v(rel_mat) \n\nCramer's V (adj.) |       95% CI\n--------------------------------\n0.29              | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nDer Effekt ist mit \\(0.33\\) nicht besonders stark. Du kannst Cramers V wie die Korrelation interpretieren. Ein V von 0 bedeutet keinen Zusammenhang und ein V von 1 einen maximalen Zusammenhang. Wir wollen uns die Daten dann nochmal in einer Abbidlung anschauen. Dafür müssen wir die Matrix erstmal in einen Datensatz umwandeln und die Gruppen zu Faktoren machen.\n\nplot_tbl <- rel_mat %>% \n  as_tibble(rownames = \"group1\") %>% \n  gather(A:D, key = \"group2\", value = \"value\") %>% \n  mutate(group1 = as_factor(group1),\n         group2 = as_factor(group2))\n\nIn Abbildung 2.1 sehen wir die Matrix der Zähldaten für die beiden Gruppen nochmal visualisiert. Beim betrachten fällt auf, dass die beiden Level C und D kaum Zähldaten enthalten. Hier wäre zu überlegen die beiden Level aus der Analyse herauszunehmen und einen klassischen \\(\\mathcal{X}^2\\)-Test auf einer 2x2 Kreuztabelle zu rechnen.\n\nggplot(plot_tbl, aes(x = group2, y = value, fill = group1)) +\n  theme_bw() +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  labs(x = \"Gruppe 2\", y = \"Anzahl\", fill = \"Gruppe 1\") +\n  scale_fill_okabeito() \n\n\n\n\nAbbildung 2.1— Barplot der Zähldaten aus der Matrix."
  },
  {
    "objectID": "example-analysis-03.html",
    "href": "example-analysis-03.html",
    "title": "3  Auswertung zweifaktorielle ANOVA mit Interaktion",
    "section": "",
    "text": "Version vom Mai 10, 2023 um 14:10:58\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, see, readxl, knitr,\n               modelsummary, parameters, multcomp,\n               multcompView, emmeans, \n               conflicted)\n## resolve some conflicts with same function naming\nconflict_prefer(\"select\", \"dplyr\")\n\nHier kommt jetzt ein schönes Beispiel für eine Auswertung von einem dreifaktoriellen Design mit einer ANOVA. Passenderweise haben wir auch einen Interaktionsterm vorliegen. Unser dreifaktoriellen Design ist auch kein echtes dreifaktorielles Design. Wir müssen uns hier entscheiden, welcher der zwei Blockfaktoren nun unsere Wiederholung sein soll. Aber schreiben wir erstmal unser Modell auf, bevor wir das Modell mit Inhalt füllen.\n\\[\ny \\sim  f_1 + b_1 + b_2\n\\]\nIn unserem Beispiel schauen wir uns das Pflanzenwachstum growth in einer Klimakammer mit verschiedenen Belichtungsstufen light_intensity sowie der Position der Pflanze in der Lichtkammer. Die Pflanze hat eine Position im rack und dann im layer. So ergibt sich dann für uns folgendes ausgeschriebenes Modell.\n\\[\ngrowth \\sim light\\_intensity + layer + rack\n\\]\nIn dieser Form wird unser Modell aber leider nicht funktionieren. Wir hätten dann keine Wiederholungen mehr. Jede Pflanze würe dann exakt durch eine Faktorkombination beschrieben. Wir sehen gleich das Problem visualisiert. Vorher müssen wir uns aber einmal die Daten einlesen und eine Menge Faktoren erschaffen. Achtung, das Erschaffen der Faktoren ist hier sehr wichtig! Im Orginaldatensatz stehen nur Zahlen für die Faktoren. Wir kriegen dann ein echtes Problem.\n\nlight_tbl <- read_excel(\"data/light_intensity_data.xlsx\") %>% \n  mutate(rack = factor(rack, labels = c(\"left\", \"middle\", \"right\")),\n         layer = factor(layer, labels = c(\"1st\", \"2nd\", \"3rd\")),\n         light_intensity = factor(light_intensity, labels = c(\"low\", \"mid\", \"high\")),\n         growth = as.numeric(growth))\n\nIn der Tabelle 3.1 sehen wir nochmal einen Ausschnitt aus den Daten.\n\n\n\n\nTabelle 3.1— Auszug aus dem Daten zu der Lichtintensität.\n\n\nrack\nlayer\nlight_intensity\ngrowth\n\n\n\n\nleft\n1st\nlow\n17.1\n\n\nleft\n1st\nlow\n14.9\n\n\nleft\n1st\nlow\n10.3\n\n\nleft\n1st\nlow\n23.5\n\n\nleft\n1st\nlow\n0.3\n\n\nleft\n1st\nlow\n3.6\n\n\n…\n…\n…\n…\n\n\nright\n3rd\nlow\n19.5\n\n\nright\n3rd\nlow\n25.9\n\n\nright\n3rd\nlow\n35.5\n\n\nright\n3rd\nlow\n44.1\n\n\nright\n3rd\nlow\n10.7\n\n\nright\n3rd\nlow\n18.3\n\n\n\n\n\n\nNachdem wir die Daten eingelesen haben, schauen wir uns den Sachverhalt einmal für die drei Faktoren über die Level der einzelnen Faktoren an. Wir nutzen dafür die Funktion datasummary_crosstab() aus dem R Paket modelsummary. Wir können uns hier die Anzahl der Beobachtungen je Faktorlevelkombination einmal anschauen.\n\ndatasummary_crosstab(light_intensity ~ layer * rack, data = light_tbl,\n                     statistic = NULL)\n\n\n\n \n\n\n1st\n2nd\n3rd\n\n  \n    light_intensity \n    left \n    middle \n    right \n    left \n    middle \n    right \n    left \n    middle \n    right \n  \n \n\n  \n    low \n    6.0 \n    0.0 \n    0.0 \n    0.0 \n    6.0 \n    0.0 \n    0.0 \n    0.0 \n    6.0 \n  \n  \n    mid \n    0.0 \n    0.0 \n    6.0 \n    6.0 \n    0.0 \n    0.0 \n    0.0 \n    6.0 \n    0.0 \n  \n  \n    high \n    0.0 \n    6.0 \n    0.0 \n    0.0 \n    0.0 \n    6.0 \n    6.0 \n    0.0 \n    0.0 \n  \n\n\n\n\n\nWir sehen eine Menge Nullen. Das heißt, dass diese Faktorlevelkombinationen keine Beobachtungen haben. Dann können wir auch über diese Kombinationen keine Aussage treffen. Wenn wir entweder rack oder layer entfernen, sieht die Sache schon besser aus. Wir haben jetzt alle Faktorlevelkombinationen belegt. Wir müssen uns dann nur noch entscheiden, welchen Faktor wir ins Modell nehmen wollen.\n\ndatasummary_crosstab(light_intensity ~ layer, data = light_tbl,\n                     statistic = NULL)\ndatasummary_crosstab(light_intensity ~ rack, data = light_tbl,\n                     statistic = NULL)\n\n\n\n\n\n \n  \n    light_intensity \n    1st \n     2nd \n     3rd \n  \n \n\n  \n    low \n    6.0 \n    6.0 \n    6.0 \n  \n  \n    mid \n    6.0 \n    6.0 \n    6.0 \n  \n  \n    high \n    6.0 \n    6.0 \n    6.0 \n  \n\n\n\n\n\n\n\n \n  \n    light_intensity \n    left \n    middle \n    right \n  \n \n\n  \n    low \n    6.0 \n    6.0 \n    6.0 \n  \n  \n    mid \n    6.0 \n    6.0 \n    6.0 \n  \n  \n    high \n    6.0 \n    6.0 \n    6.0 \n  \n\n\n\n\n\n\n\n\n\nDas R Paket modelsummary bietet hier eine sehr große Auswahl an tollen Funktionen an um seine Daten übersichtlich zu gestalten.\nFür die Entscheidung welcher der beiden Faktoren rack oder layer mit ins Modekll soll, schauen wir uns einmal die Boxplots für die jeweiligen Fakoten an. In Abbildung 3.1 sehen wir einmal die Boxplots aufgeteilt nach rack.\n\nggplot(light_tbl, aes(light_intensity, growth, fill = rack)) +\n  theme_bw() +\n  geom_boxplot() +\n  scale_fill_okabeito()\n\n\n\n\nAbbildung 3.1— Boxplots des Pflanzenwachstums aufgeteilt nach rack.\n\n\n\n\nUnd wir sehen schon, da stimmt was nicht. Die Annahme der ANOVA ist, dass sich der Trend im ersten Faktorlevel für alle im Faktor über die anderen Faktoren gleicht. Das liest sich kryptisch, aber verdeutlichen wir es mal. Im Level low steigen alle Level des Faktors rack an. Wenn keine Interaktion vorliegen würde, dann müssten dieses Muster in dem Level mid und high ebenfalls annährend zu beobachten sein. Tut es aber nicht. Wir haben eine Interaktion zwischen light_intensity und rack visuell bestätigt.\n\nggplot(light_tbl, aes(light_intensity, growth, fill = layer)) +\n  theme_bw() +\n  geom_boxplot()  +\n  scale_fill_okabeito()\n\n\n\n\nAbbildung 3.2— Boxplots des Pflanzenwachstums aufgeteilt nach layer.\n\n\n\n\nDieses wirre Muster sehen wir dann auch in Abbildung 3.2. Hier passen die Trends des Faktors layer über die Faktorlevel low, mid und high auch wieder nicht. Schauen wir uns jetzt nochmal die ganze Sache aufgeteilt nach rack und layer an. Vielelicht werden wir dann etwas schlauer oder das Problem wird noch klarer.\n\nggplot(light_tbl, aes(light_intensity, growth, fill = rack)) +\n  theme_bw() +\n  geom_boxplot() +\n  facet_wrap(~ layer)  +\n  scale_fill_okabeito()\n\n\n\n\nAbbildung 3.3— Boxplots des Pflanzenwachstums aufgeteilt nach rack und layer.\n\n\n\n\nJetzt sehen wir etwas mehr. Im 1st Level liegen alle rack-Level auf einer Ebene. Ebenso liegen alle rack-Level auf einer Ebene im 2n Level. Das ganze Problem der Interaktion entsteht im 3rd Level. Hier ging etwas drunter und drüber im Pflanzenwachstum. Wir wissen jetzt, dass das dritte Layer anscheinend defekt war oder irgendwas dort mit den Racks nicht gestimmt hat.\nWir könnten jetzt das dritte Layer aus der Analyse werfen. Das wäre aber nur eine Möglichkeit. Wenn wir das tuen würden, dann würde wir auch die Interaktion los werden. Das wollen wir hier aber nicht, wir ziehen jetzt die Analyse einmal mit der Interaktion durch. Dafür bauen wir uns jetzt das lineare Modell und schauen uns einmal die ANOVA an.\n\nfit_1 <- lm(growth ~ light_intensity + layer + light_intensity:layer, \n            data = light_tbl)\nfit_1 %>% model_parameters()\n\nParameter                            | Coefficient |   SE |           95% CI | t(45) |      p\n---------------------------------------------------------------------------------------------\n(Intercept)                          |       11.62 | 2.80 | [  5.97,  17.26] |  4.14 | < .001\nlight intensity [mid]                |        1.43 | 3.96 | [ -6.55,   9.42] |  0.36 | 0.719 \nlight intensity [high]               |        3.70 | 3.96 | [ -4.28,  11.68] |  0.93 | 0.356 \nlayer [2nd]                          |        5.68 | 3.96 | [ -2.30,  13.67] |  1.43 | 0.159 \nlayer [3rd]                          |       14.05 | 3.96 | [  6.07,  22.03] |  3.54 | < .001\nlight intensity [mid] * layer [2nd]  |       -3.33 | 5.61 | [-14.62,   7.96] | -0.59 | 0.555 \nlight intensity [high] * layer [2nd] |       -5.92 | 5.61 | [-17.21,   5.37] | -1.06 | 0.297 \nlight intensity [mid] * layer [3rd]  |       -9.60 | 5.61 | [-20.89,   1.69] | -1.71 | 0.094 \nlight intensity [high] * layer [3rd] |      -25.90 | 5.61 | [-37.19, -14.61] | -4.62 | < .001\n\n\nErstmal sehen wir an den Modellparameters, dass hier wieder etwas nicht stimmt. Wir würden erwarten, dass der Effekt des Layers immer gleich ist. Hier ist der Effekt von dem 2nd Layer zu dem 3rd Layer fast dreimal so stark. Und eigentlich sollten die Layer den gleichen Effekt haben. Nämlich eigentlich keinen oder einen Effekt weit unter dem von der Lichtintensität. Das Layer ist eine technische Komponente.\n\nfit_1 %>% anova() %>% model_parameters()\n\nParameter             | Sum_Squares | df | Mean_Square |    F |      p\n----------------------------------------------------------------------\nlight_intensity       |      433.15 |  2 |      216.57 | 4.60 | 0.015 \nlayer                 |       70.92 |  2 |       35.46 | 0.75 | 0.477 \nlight_intensity:layer |     1138.80 |  4 |      284.70 | 6.04 | < .001\nResiduals             |     2120.85 | 45 |       47.13 |      |       \n\nAnova Table (Type 1 tests)\n\n\nWir sehen die visuelle Interaktion auch in der ANOVA Ausgabe als hoch signifikanten Term light_intensity:layer mit dem \\(p\\)-Wert \\(<0.001\\). Im Anschluss rechnen wir jetzt die paarweisen Vergleiche mit der Funktion emmeans(). Mit dem | geben wir an, dass wir die paarweisen Vergleiche für die Level von light_intensity getrennt für die Level vom layer rechnen wollen. Wenn du keine Adjustierung des \\(\\alpha\\)-Niveaus für die multiplen Vergleiche möchtest, dann wähle einfach die Option adjust = \"none\". Wir nutzen dann die Ausgabe nicht direkt sondern werden noch die Ausgabe etwas aufhübschen.\n\ncomp_1_obj <- fit_1 %>% \n  emmeans(specs = ~ light_intensity | layer) %>% \n  contrast(method = \"pairwise\", adjust = \"bonferroni\") \n\nIn dem Objekt comp_1_obj sind eine Menge Informationen enthalten. Ich kürze mir immer die Informationen und sortiere nochmal die Ergebnisse. Wir erhalten dann eine saubere Wiedergabe.\n\ncomp_1_obj %>% \n  summary %>% \n  as_tibble %>% \n  select(contrast, layer, p.value) %>% \n  mutate(p.value = format.pval(p.value, eps = 0.001, digits = 2))\n\n# A tibble: 9 x 3\n  contrast   layer p.value\n  <fct>      <fct> <chr>  \n1 low - mid  1st   1.0000 \n2 low - high 1st   1.0000 \n3 mid - high 1st   1.0000 \n4 low - mid  2nd   1.0000 \n5 low - high 2nd   1.0000 \n6 mid - high 2nd   1.0000 \n7 low - mid  3rd   0.1355 \n8 low - high 3rd   <0.001 \n9 mid - high 3rd   0.0028 \n\n\nNach der Adjustierung für die multiplen Vergleiche haben wir nur noch einen Effekt in dem 3rd Layer. Sonst haben die Lichtintensitäten keinen Einfluss auf die Wuchshöhe der Pflanzen. Da wir wissen, dass das 3rd Layer auch das defekte Layer war, sehen wir hier schon, dass wir keinen wirklichen Effekt durch das Licht vorliegen haben. Alles was wir gefunden haben, ist eben ein defektes 3rd Layer.\nDie 95% Konfidenzintervalle erhalten wir mit der Funktion confint(). Die Ergebnisse sind natürlich die gleichen. Wir sehen wieder keinen Unterschied zwischen den Lichtintensitäten außer in dem 3rd Layer.\n\nci_obj <- comp_1_obj %>% \n  confint() %>% \n  as_tibble() %>% \n  select(contrast, layer, estimate, conf.low = lower.CL, conf.high = upper.CL) \n\nci_obj\n\n# A tibble: 9 x 5\n  contrast   layer estimate conf.low conf.high\n  <fct>      <fct>    <dbl>    <dbl>     <dbl>\n1 low - mid  1st     -1.43    -11.3       8.42\n2 low - high 1st     -3.70    -13.6       6.16\n3 mid - high 1st     -2.27    -12.1       7.59\n4 low - mid  2nd      1.90     -7.96     11.8 \n5 low - high 2nd      2.22     -7.64     12.1 \n6 mid - high 2nd      0.317    -9.54     10.2 \n7 low - mid  3rd      8.17     -1.69     18.0 \n8 low - high 3rd     22.2      12.3      32.1 \n9 mid - high 3rd     14.0       4.18     23.9 \n\n\nIn der Abbildung 3.4 sehen wir dann die berechneten 95% Konfidenzintervalle nochmal visualisiert. Wenn wir einen Effekt haben, dann im 3rd Layer. In den restlichen 95% Konfidenzintervallen ist die Null mit enthalten, wir können also die Nullhypothese auf Gleichheit des Gruppenvergleiches nicht ablehnen.\n\nggplot(ci_obj, aes(contrast, y=estimate, ymin=conf.low, ymax=conf.high,\n                   color = layer, group = layer)) +\n  geom_hline(yintercept=0, linetype=\"11\", colour=\"grey60\") +\n  geom_errorbar(width=0.1, position = position_dodge(0.5)) + \n  geom_point(position = position_dodge(0.5)) +\n  scale_color_okabeito() +\n  coord_flip() +\n  theme_classic()\n\n\n\n\nAbbildung 3.4— Die 95% Konfidenzintervalle für die paarweisen Vergleiche aufgeteilt layer.\n\n\n\n\nNeben der Darstellung mit 95% Konfidenzintervallen ist auch die Darstellung mit dem compact letter display sehr beliebt. Wir nutzen dafür dann die Funktion cld(). Wir adjustieren uns wieder die Vergleiche nach Bonferroni. Im Weiteren trenne wir die Vergleiche auch wieder nach den Leveln für den Faktor layer auf.\n\ncld_obj <- fit_1 %>% \n  emmeans(specs = ~ light_intensity | layer)  %>%\n  cld(Letters = letters, adjust = \"bonferroni\") \n\ncld_obj\n\nlayer = 1st:\n light_intensity emmean  SE df lower.CL upper.CL .group\n low              11.62 2.8 45     4.65     18.6  a    \n mid              13.05 2.8 45     6.08     20.0  a    \n high             15.32 2.8 45     8.35     22.3  a    \n\nlayer = 2nd:\n light_intensity emmean  SE df lower.CL upper.CL .group\n high             15.08 2.8 45     8.11     22.1  a    \n mid              15.40 2.8 45     8.43     22.4  a    \n low              17.30 2.8 45    10.33     24.3  a    \n\nlayer = 3rd:\n light_intensity emmean  SE df lower.CL upper.CL .group\n high              3.47 2.8 45    -3.50     10.4  a    \n mid              17.50 2.8 45    10.53     24.5   b   \n low              25.67 2.8 45    18.70     32.6   b   \n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 3 estimates \nP value adjustment: bonferroni method for 3 tests \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nWir sehen wieder, dass wir nur in dem 3rd Layer Buchstabenunterschiede haben. Daher haben wir auch nur im 3rd Layer signifikante Ergebnisse. Wichtig ist, dass wir die Buchstaben nur pro Level des Layers vergleichen können, aber auf keinen Fall über die Layer hinweg. Das geht dann leider nicht. Die Ausgabe der Funktion emmeans() schlägt noch andere Darstellungsformen für die Vergleiche vor, du kannst gerne einmal die Funktionen pairs(), pwpp() oder pwpm() ausprobieren und schauen, ob dir die Visualisierung mehr sagt. Im Kapitel 31 - Multiple Vergleiche oder Post-hoc Tests gehe ich nochmal auf die verschiedene Darstellungsformen in emmeans ein.\nWenn wir das compact letter display mit deinem Barplot verbinden wollen, müssen wir uns etwas strecken. Zuerst sortieren wir die Ausgabe von cld_obj wieder in die korrekte Reihenfolge der Faktorenlevel. Dann können wir die Spalte .group direkt in ggplot() verwenden.\n\ncld_sort_obj <- cld_obj %>% \n  as_tibble() %>% \n  select(light_intensity, layer, .group) %>% \n  arrange(layer, light_intensity)\n\nIn Abbildung 3.5 sehen wir die Ausgabe des Barplots für die Daten und dann an die Balken geschrieben das compact letter display. Wichtig ist hier, dass die Buchstaben immer nur für ein Layer gelten. Wir können wegen der Interaktion nicht die Layer untereinander mit den Buchstaben vergleichen. Wir sehen wiederum, dass wir keine relevanten signifikanten Ergebnisse aus dem Experiment mitnehmen können.\n\nstat_tbl <- light_tbl %>% \n  group_by(light_intensity, layer) %>% \n  summarise(mean = mean(growth),\n            sd = sd(growth))\n\nggplot(stat_tbl, aes(x = layer, y = mean, group = light_intensity, \n                     fill = light_intensity)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_errorbar(aes(ymin = mean-sd, ymax = mean+sd),\n                width = 0.2, position = position_dodge(0.9)) +\n  annotate(\"text\", \n           x = c(0.7, 1, 1.3, 1.7, 2, 2.3, 2.7, 3, 3.3), \n           y = c(22, 21, 23, 24, 20, 23, 39, 25, 9), \n           label = pluck(cld_sort_obj, \".group\")) +\n  theme_bw() +\n  labs(fill = \"Behandlung\")  +\n  scale_fill_okabeito()\n\n\n\n\nAbbildung 3.5— Barplots für die Mittelwerte mit den entsprechenden Standardabweichungen und dem compact letter display für die paarweisen Vergleiche. Achtung die letter gelten nur in einem Level des Layers."
  },
  {
    "objectID": "example-analysis-04.html",
    "href": "example-analysis-04.html",
    "title": "4  Multiples Testen mit Games Howell Test",
    "section": "",
    "text": "Version vom Mai 10, 2023 um 14:11:04\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, readxl, knitr,\n               rstatix, multcompView)\n\nIn diesem Abschnitt wollen wir einen einfaktoriellen Datensatz auswerten. Das heißt, wir haben eine Spalte mit unseren Variantenfaktor mit 11 Leveln und dann aber auch neun Outcomes. Wir müssen also für alle neun Outcomes einen Test rechnen. Dementsprechend bauen wir uns Liste in R. In jedem der Listeneinträge ist nur die Spalte für die Variante und einem Outcome. Wir haben dann also am Ende eine Liste mit neun Listeneinträgen und pro Liste einen Datensatz mit zwei Spalten. Wir brauchen noch das R Paket multcompView für die Darstellung des compact letter display und das R Paket rstatix für die Anwendung der R Funktion games_howell_test(). Wir gehen nämlich von einem normalverteilten Outcome mit Varianzheterogenität aus. Das nehmen wir für jedes Outcome an, dann können wir immer das Gleiche auf den Daten rechnen.\nAlso erstmal die Daten einlesen, dann die Varianten bilden und die Zahlen wieder auf eine angemessene Länge runden. Ich nehme da immer auf Zweikommastellen, aber das hat hier eher mit der Übersicht zu tun. Gerne kannst du da auch mehr Kommastellen zulassen.\n\nsoil_tbl <- read_excel(\"data/soil_1fac_data.xlsx\") %>% \n  mutate(variante = str_c(variante, \"_\", amount),\n         variante = as_factor(variante),\n         across(where(is.numeric), round, 2)) %>% \n  select(-amount)\n\nWarning: There was 1 warning in `mutate()`.\ni In argument: `across(where(is.numeric), round, 2)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\nIn der Tabelle 4.1 sehen wir nochmal einen Ausschnitt aus den Daten. Wir schmeißen amount aus den Daten, da die Spalte dann in der Variante aufgeht.\n\n\n\n\nTabelle 4.1— Auszug aus dem Daten zu der Lichtintensität.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariante\nfe\nk\nno3\nno4\nmg\nca\ndrymatter\nfreshweight\nheight\n\n\n\n\nHolzfasern_0\n0.26\n3.7\n3.28\n4.1\n0.43\n0.45\n11.24\n95.3\n22.17\n\n\nHolzfasern_0\n0.33\n3.66\n3.24\n4.05\n0.44\n0.47\n11.48\n86.15\n23\n\n\nHolzfasern_0\n0.27\n3.83\n3.64\n4.3\n0.48\n0.47\n11.11\n86.93\n28.17\n\n\nHolzfasern_0\n0.31\n3.66\n3.24\n3.75\n0.31\n0.43\n11.95\n89.68\n26.83\n\n\nTorf_30\n0.46\n2.89\n5.02\n4.95\n0.37\n0.39\n11.21\n70.82\n23.83\n\n\nTorf_30\n0.37\n3.41\n9.44\n5.01\n0.49\n0.42\n11.29\n66.23\n22.33\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\nAufgefasertes_30\n0.57\n9.65\n18.98\n6.5\n0.33\n0.29\n10.28\n73.8\n22\n\n\nAufgefasertes_30\n0.78\n27.65\n16.96\n7\n0.34\n0.32\n13.91\n74.18\n22.5\n\n\nAufgefasertes_50\n1.78\n22.91\n59.38\n8\n0.34\n0.25\n13.61\n45.58\n18.67\n\n\nAufgefasertes_50\n1.3\n36.06\n58.02\n8\n0.35\n0.25\n9.62\n37.93\n20\n\n\nAufgefasertes_50\n1.08\n3.66\n56.85\n7.99\n0.32\n0.25\n9.5\n43.98\n18.67\n\n\nAufgefasertes_50\n1.06\n3.75\n44.39\n7.99\n0.39\n0.24\n10.67\n35.48\n19.67\n\n\n\n\n\n\nIm Folgenden werden wir immer wieder die Funktion map() aus dem R Paket purrr nutzen um auf Listen zu rechnen. Die Funktion map() erlaubt auf allen Listeneinträgen die gleiche Funktion durchzuführen. Das macht es natürlich sehr angenehm, wenn wir immer das gleiche Modell auf unsere Daten rechnen wollen.\n\n\nWir immer gibt es auch Tutorien im Netz, wenn du mehr über purrr::map() erfahren willst. Es gibt das\npurrr tutorial und die Zusammenfassung in Apply functions with purrr::CHEAT SHEET.\nAls erstes müssen wir aber unsere Daten in das Long-Format kriegen. Dann können wir die Daten nach dem Outcome in der Spalte key die Daten in neuen Listeneinträge aufspalten. Warum neun? Wir haben eben neun Outcomes. Dann haben wir auch neun Listeneinträge. In jedem Listeneintrag ist die Spalte variante und die Spalte value. Der Name des Listeneintrags enthält dann den Namen des Outcomes. Mit dem .x übergeben wir iterativ jeden Listeneintrag in die Funktion select().\n\nsoil_lst <- soil_tbl %>%\n  gather(key, value, fe:height) %>%\n  split(.$key) %>%\n  map(~select(.x, -key))\n\nSchauen wir uns von jedem Listeneintrag einmal die ersten beiden Zeilen an. Das machen wir auch wieder mit map() indem wir die Funktion head() auf jedem Listeneintrag ausführen.\n\nsoil_lst %>% \n  map(~head(.x, 2))\n\n$ca\n# A tibble: 2 x 2\n  variante     value\n  <fct>        <dbl>\n1 Holzfasern_0  0.45\n2 Holzfasern_0  0.47\n\n$drymatter\n# A tibble: 2 x 2\n  variante     value\n  <fct>        <dbl>\n1 Holzfasern_0  11.2\n2 Holzfasern_0  11.5\n\n$fe\n# A tibble: 2 x 2\n  variante     value\n  <fct>        <dbl>\n1 Holzfasern_0  0.26\n2 Holzfasern_0  0.33\n\n$freshweight\n# A tibble: 2 x 2\n  variante     value\n  <fct>        <dbl>\n1 Holzfasern_0  95.3\n2 Holzfasern_0  86.2\n\n$height\n# A tibble: 2 x 2\n  variante     value\n  <fct>        <dbl>\n1 Holzfasern_0  22.2\n2 Holzfasern_0  23  \n\n$k\n# A tibble: 2 x 2\n  variante     value\n  <fct>        <dbl>\n1 Holzfasern_0  3.7 \n2 Holzfasern_0  3.66\n\n$mg\n# A tibble: 2 x 2\n  variante     value\n  <fct>        <dbl>\n1 Holzfasern_0  0.43\n2 Holzfasern_0  0.44\n\n$no3\n# A tibble: 2 x 2\n  variante     value\n  <fct>        <dbl>\n1 Holzfasern_0  3.28\n2 Holzfasern_0  3.24\n\n$no4\n# A tibble: 2 x 2\n  variante     value\n  <fct>        <dbl>\n1 Holzfasern_0  4.1 \n2 Holzfasern_0  4.05\n\n\nJetzt geht kannst du einmal map() in seiner vollen Schönheit sehen. Wir rechnen auf jeden Listeneintrag einmal den Games Howell Test. Dann steht in jedem der Listeneinträge das Ergebnis des Games Howell Test. Wir müssen dann noch die Spalte contrast aus der Ausgabe des Games Howell Test abändern, damit wir das compact letter display über die Funktion multcompLetters() nutzen können. Dann brauchen wir die adjustierten p-Werte und den Kontrast. Beides schieben wir dann in die Funktion multcompLetters() und lassen uns nur die Buchstaben des compact letter display wiedergeben. Am Ende kleben wir noch alle Einträge der einzelnen Listen mit der Funktion bind_rows() zu einem Datensatz zusammen. Vermutlich musst du die einzelnen Funktion selber mal Schritt für Schritt ausführen. Aber das hier war ja auch jetzt mal fortgeschrittene Programmierung.\n\nsoil_lst %>% \n  map(~games_howell_test(value ~ variante, data = .x)) %>% \n  map(~mutate(.x, contrast = str_c(.x$group1, \"-\", .x$group2))) %>% \n  map(~pull(.x, p.adj, contrast)) %>% \n  map(~multcompLetters(.x)$Letters) %>% \n  bind_rows(.id = \"outcome\") \n\n# A tibble: 9 x 12\n  outcome     Holzfasern_0 Torf_30 Kompostiertes_10 Kompostiertes_30\n  <chr>       <chr>        <chr>   <chr>            <chr>           \n1 ca          ab           ab      ab               cd              \n2 drymatter   a            a       ab               ab              \n3 fe          a            abc     ab               abc             \n4 freshweight a            b       ac               abcdef          \n5 height      abcde        abc     abcde            acd             \n6 k           a            a       ab               ab              \n7 mg          ab           ab      a                a               \n8 no3         a            ab      ab               abcd            \n9 no4         a            b       bcdef            c               \n# i 7 more variables: Kompostiertes_50 <chr>, Gehäckseltes_10 <chr>,\n#   Gehäckseltes_30 <chr>, Gehäckseltes_50 <chr>, Aufgefasertes_10 <chr>,\n#   Aufgefasertes_30 <chr>, Aufgefasertes_50 <chr>\n\n\nWie du dann mit dem compact letter display weiterarbeitest und interpretierst, findest du dann im Kapitel 31 - Multiple Vergleiche oder Post-hoc Tests über das compact letter display für den Games Howell Test. Hier ist erstmal Schluss, sonst wird hier alles sehr wiederholend."
  },
  {
    "objectID": "example-analysis-05.html",
    "href": "example-analysis-05.html",
    "title": "5  Wurzelqualität in einem zweifaktoriellen Design",
    "section": "",
    "text": "Version vom Mai 10, 2023 um 14:11:10\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, readxl, see,\n               ordinal, parameters, emmeans, multcomp,\n               conflicted)\n## resolve some conflicts with same function naming\nconflict_prefer(\"select\", \"dplyr\")\n\nSehr häufig haben wir es auch in den Agrarwissenschaften mit Boniturnoten auf der Likert-Skala zu tun. Das heißt wir haben ein Outcome \\(y\\) mit Noten zwischen 1 und 9. Die ganzen Beobachtungen haben wir dann meist noch irgendwie in einem Block wiederholt. Neben den Problemen eine gute und richtige Bonitur praktisch durchzuführen, ist natürlich die Normalverteilung für das Outcome der Noten nicht gegeben. Wir könnten hier zum einen mit einem paarweisen Wilcoxon Rangsummentest ran oder aber wir nutzen die Funktion clm() aus dem R Paket ordnial für die Schätzung der Gruppenunterschiede. Wir nehmen hier mal die Funktion clm() da wir auch den Block mit ins Modell nehmen können.\nZuerst laden wir wieder die Daten. Dabei müssen wir dann noch die Notenspalte grade einmal als numerisch lassen und einmal in einen Faktor mutieren. Wir brauchen ein Faktor für die Funktion clm() aber eine numerische Variable für die Funktion ggplot(). Deshalb eben die Noten in einer doppelten Spalte.\n\nroot_tbl <- read_excel(\"data/root_2fac_data.xlsx\") %>% \n  mutate(treatment = str_c(treatment, \"_\", fert, \"_\", mixture)) %>% \n  select(treatment, block, grade) %>% \n  mutate(grade_fctr = as_factor(grade),\n         block = as_factor(block))\n\nIn Abbildung 5.1 sehen wir einmal die Abbildung der Noten. Wie immer sind über so viele Behandlungsstufen dann die Abbildungen sehr unübersichtlich. Ich nutze hier einen Dotplot, da du dann die einzelnen Beobachtungen besser sehen kannst. Wir sehen aber, dass es Effekte zwischen den Behandlungen gibt.\n\nggplot(root_tbl, aes(treatment, grade, fill = block)) +\n  theme_bw() +\n  geom_dotplot(binaxis = \"y\", stackdir='center', \n              position=position_dodge(0.6), dotsize = 0.75) +\n  scale_y_continuous(breaks = 1:9, limits = c(1,9)) +\n  scale_fill_okabeito() +\n  theme(axis.text.x = element_text(angle = 45, hjust=1))\n\n\n\n\nAbbildung 5.1— Boniturnoten der Wurzel über alle Behandlungslevel.\n\n\n\n\nWir könnten jetzt einen Friedman Test rechen. Der Friedman Test ist das Äquivalent zur einer zweifaktorielle ANOVA. Wenn wir aber die Funktion friedman.test() durchführen sehen wir, dass wir kein complete block design vorliegen haben. Wir können hier also einen Friedman Test nicht rechnen.\n\nfriedman.test(grade_fctr ~ treatment | block, root_tbl)\n\nError in friedman.test: not an unreplicated complete block design\nDa es eben mit dem Friedman Test nicht geht, nutzen wir Cumulative Link Models mit der Funktion clm(). Die Modelle erlauben ein ordinales Outcome zusammen mit einem multiplen linearen Modell zu schätzen. Das ist was wir wollen, damit können wir den Block mit berücksichtigen. Dann geben wir mit threshold = \"symmetric\" noch an, dass die Abstände in unserem Outcome \\(y\\) symmetrisch sind. Die Abstände zwischen den Notenstufen sind ja immer gleich groß.\n\nclm_fit <- clm(grade_fctr ~ treatment + block, data = root_tbl,\n               threshold = \"symmetric\")\n\nMit dem Fit des Modells können wir dann auch eine ANOVA rechnen, um zu schauen, ob wir überhaupt einen signifikanten Effekt unser Behandlung vorliegen haben. Das haben wir, zum einen ist die Behandlung signifikant und leider auch der Block. Das ist natürlich nicht so schön. Eine Interaktion können wir nicht rechnen, da wir zu wenig Fallzahl vorliegen haben.\n\nanova(clm_fit) %>% \n  model_parameters()\n\nParameter |  Chi2 | df |      p\n-------------------------------\ntreatment | 54.46 | 13 | < .001\nblock     | 24.64 |  3 | < .001\n\nAnova Table (Type 1 tests)\n\n\nNun nutzen wir das Modell einmal um uns die compact letters wiedergeben zu lassen. Dafür nutzen wir wieder die Funktion emmeans() aus dem R Paket emmeans. Am Ende sortieren wir nochmal nach der Behandlung und dann lassen wir uns nur die Behandlung und die Buchstaben wiedergeben. Wenn du mehr möchtest, kannst du auch im Kapitel über die Posthoc Tests oder der ordinalen logistischen Regression mehr Optionen anlesen.\n\nclm_fit %>% \n  emmeans(~ treatment) %>% \n  cld(Letters = letters) %>% \n  arrange(treatment) %>% \n  select(treatment, .group)\n\n          treatment .group\n1     CRTL_high_100    cde\n2   CRTL_normal_100   bcde\n3    WF0_high_30-70      e\n4    WF0_high_60-40  abcd \n5  WF0_normal_30-70    cde\n6  WF0_normal_60-40  ab   \n7    WF1_high_30-70   bcde\n8    WF1_high_60-40  abcde\n9  WF1_normal_30-70  abc  \n10 WF1_normal_60-40  abcde\n11   WF3_high_30-70  abcde\n12   WF3_high_60-40  a    \n13 WF3_normal_30-70     de\n14 WF3_normal_60-40   bcde\n\n\nWenn wir für den Block mit adjustieren wollen würden, können wir den Aufruf wie folgt ändern emmeans(~ treatment | block). Leider erhalten wir dann in der df Spalte den Eintrag Inf für eng. infinity und das ist nie ein gutes Zeichen. Wir haben auch die sich immer gleich wiederholende Abfolge der compact letters. Also macht es mehr kaputt als heile und wir lassen das Modell so wie es ist. Wir nehmen den Block als reine Wiederholung mit in das Modell rein."
  },
  {
    "objectID": "example-analysis-06.html",
    "href": "example-analysis-06.html",
    "title": "6  Wachstum nach Rückschnitt mit Messwiederholung",
    "section": "",
    "text": "Version vom Mai 10, 2023 um 14:11:14\n\n\n\n\n\n\nBeispielhafte Auswertungen per Video\n\n\n\nDu findest auf YouTube Spielewiese in R - Teil 04.0 - Messwiederholungen einlesen und Outcomes korrelieren die Auswertung einmal als Video. Ich werde zwar alles nochmal hier als Text aufschreiben, aber manchmal ist das Sehen und Hören dann einfacher.\n\n\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, readxl, see, janitor,\n               psych, parameters, effectsize, emmeans,\n               multcomp, conflicted)\n## resolve some conflicts with same function naming\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\n\nIn diesem Beispiel haben vier Outcomes mit shoot, flower, leaf und fruit gemessen. Wir nehmen hier an, dass wir es bei dem Outcome shoot mit einer Pflanzenhöhe zu tun haben und bei den drei anderen Outcomes jeweils mit einer Anzahl. Wir haben jeweils drei Pflanzen in zwei Blöcken randomisiert. Darüber hinaus haben wir vier Behandlungen. Einmal schauen wir uns eine Kontrolle an sowie drei Arten von Rückschnitten. Die Fragestellung ist nun, wie sich die Behandlung durch die Rückschnitte auf die Outcomes auswirkt.\nWir lesen wieder als erstes die Daten ein. Bitte beachte, dass wir den Versuch an acht Terminen ausgewertet haben. Das heißt, wir haben an acht Terminen alle vier Outcomes gemessen. Dadurch ergeben sich immer vier Spalten mit dem Namen des Outcomes ergänzt um eine Zahl für den Messtermin. Wir nutzen noch die Funktion clean_names() um sicherzugehen, dass wir keine Leerzeichen mehr in den Spaltennamen haben.\n\ncutting_raw_tbl <- read_excel(\"data/cutting_data.xlsx\") %>% \n  clean_names()\n\nNun müssen wir noch die Spalten shoot_1 bis zur letzten Spalte fruit_8 in drei Spalten verwandeln. In der einen Spalte rsp sollen nur die numerischen Messwerte stehen. In der Spalte week soll nur die Messwoche hinein und in die Spalte Outcome soll sich der Name der Spalte wiederfinden. Um diese Modifizierung der Daten zu erreichen, nutzen wir die Funktion pivot_longer(). Wir spalten die Namen der Spalten durch den Unterstrich _ in zwei neue Spalten outcome und week. Dann müssen wir noch die Daten sortieren und die Behandlung sowie die Blöcke in einen Faktor umwandeln. Damit wir später eine bessere Abbildung haben, verwandeln wir noch die Wochen in eine numerische Spalte.\n\ncutting_tbl <- cutting_raw_tbl %>% \n  pivot_longer(cols = shoot_1:fruit_8, \n               names_to = c(\"outcome\", \"week\"), \n               names_sep = \"_\",\n               values_to = \"rsp\") %>% \n  arrange(outcome, week, trt, block, rsp) %>% \n  mutate(block = as_factor(block),\n         trt = as_factor(trt),\n         outcome = as_factor(outcome),\n         week = as.numeric(week))\n\nJetzt wollen wir für die Abbildung 6.1 etwas andere Namen für unsere Outcomes. Dafür nutzen wir dann die Funktion recode() in der wir die Level eines Faktors umbenennen können. Nachdem wir das gemacht haben, stellen wir einmal die Verläufe der Outcomes über die Wochen getrennt für die Behandlungen und die Blöcke dar. Die Funktion stat_smooth() erlaubt uns einmal eine Gerade durch die Punkte zu legen.\n\ncutting_plot_tbl <- cutting_tbl %>% \n  mutate(outcome = recode(outcome, \n                          shoot = \"Trieblänge\", \n                          flower = \"Blütenanzahl\", \n                          leaf = \"Blätteranzahl\", \n                          fruit = \"Fruchtanzahl\"))\n\ncutting_plot_tbl %>% \n  ggplot(aes(week, rsp, color = trt, linetype = block)) +\n  theme_minimal() +\n  facet_wrap(~ outcome, scales = \"free_y\") +\n  geom_point() +\n  stat_smooth(se = FALSE) +\n  scale_color_okabeito()\n\n\n\n\nAbbildung 6.1— Abbildung der vier Outcomes über die acht Messtermine mit den vier Behandlungen und den zwei Blöcken.\n\n\n\n\nWir wir erkennen können, sind die Blöcke ähnlich. Hier haben wir also gut randomisiert. Auch scheint keine gravierende Interaktion vorzuliegen. Die Abstände und der Anstieg ist über die Wochen relativ konstant. Wir werden uns am Ende auf die Woche acht konzentrieren, da wir an dem letzten Termin unsere multiplen Vergleiche rechnen wollen.\nHäufig ist es so, dass wir dann noch einen Barplot wollen. Hier ist aber Vorsicht geboten. Ein Barplot mit Mittelwert und Standardabweichung macht eigentlich nur Sinn, wenn das Outcome normalverteilt ist. Sonst können schnell negative oder ungewollte Werte raus kommen. Immerhin haben wir hier mit flower, leaf und fruit drei Outcomes die als Zähldaten einer Poissonverteilung folgen.\nUm die Mittelwerte für die Behandlungen getrennt für die Wochen zu berechnen, nutzen wir die Funktion group_by() um uns alle Paare von Behandlung und Woche zu bilden. Dann rechnen wir für jede dieser Paarungen dann den Mittelwert und die Standardabweichung aus. Wir mitteln aber hier über die Blöcke, um dann die Barplots überhaupt sinnvoll darstellen zu können.\n\nstat_tbl <- cutting_plot_tbl %>% \n  group_by(trt, outcome, week) %>% \n  summarise(mean = mean(rsp),\n            sd = sd(rsp)) \n\nIn Abbildung 6.2 sehen wir die Barplots. Wir zu erwarten erhalten wir bei den Zäldaten negatuve Werte durch den Fehlerbalken angezeigt. Die Streuung ist hier zu groß und so sind die Fehlerbalken auch negativ. Barplots sind keine gute Lösung für die Darstellung von Zähldaten durch den Mittelwert und die Standardabweichung. Da wären dann Boxplots besser. Aber es ist immer gut mal was zu zeigen, was dann auch nicht so gut geht, also hier einmal die Barplots.\nAchtung, wenn du nicht normalverteilte Daten mit Barplots und den Fehlerbalken darstellst, dann kann es zu Fehlerbalken führen, die negativ werden\n\nggplot(stat_tbl, aes(x = week, y = mean, group = trt, fill = trt)) + \n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_errorbar(aes(ymin = mean-sd, ymax = mean+sd),\n                width = 0.2, position = position_dodge(0.9)) +\n  theme_bw() +\n  labs(fill = \"Behandlung\") +\n  facet_wrap(~ outcome, scales = \"free_y\") +\n  scale_fill_okabeito()\n\n\n\n\nAbbildung 6.2— Barplots mit Fehlerbalken der vier Outcomes über die acht Messtermine mit den vier Behandlungen und den zwei Blöcken.\n\n\n\n\nEine Frage, die sich im Rahmen der Analyse noch gestellt hat, war ob die Outcomes untereinander stark korrelieren. Daher wurde nochmal entschieden sich die Korrelation der vier Outcomes anzuschauen. Wir machen das jetzt erstmal über die Woche sechs bis Woche acht. In den früheren Wochen haben die Pflanzen noch keine Früchte und dann sehen die Abbildungen sehr seltsam verzerrt aus.\n\n\nEs gibt eine große Auswahl an R Paketen und damit auch Plots für die Korrelation. In dem Tutorial Correlation plot in R werden dir eine große Auwahl präsentiert.\nWir filtern uns also die Wochen sechs bis acht und nutzen dann die Funktion pivot_wider() um die Outcomes wieder nebeneinander in vier Spalten anzuordnen. Da pivot_wider() die wiederholenden Zeilen von Behandlung und Block zusammenfasst, müssen wir die Spalten nochmal wieder ausklappen. Dieses Ausklappen machen wir mit der Funktion unnest().\n\ncutting_cor_tbl <- cutting_tbl %>% \n  filter(week %in% c(6, 7, 8)) %>% \n  pivot_wider(names_from = outcome, values_from = rsp) %>% \n  unnest(cols = c(flower, fruit, leaf, shoot))\n\nIn Abbildung 6.3 sehen wir jetzt ein Beispiel für ein Korrelationsplot. Es gibt wirklich sehr viele Möglichkeiten in R sich die Korrelation von numerischen Spalten anzeigen zu lassen. Wir wählen hier nur den Korrelationskoeffizient nach Kendall, da wir hier nicht nur normalverteilte Outcomes vorliegen haben. In dem Histogramm auf der Diagonalen kannst du auch sehr gut die Poissonverteilungen von der Normalverteilung unterscheiden. Die Trieblänge ist normalverteilt und die anderen Outcomes mit den Anzahlen folgen einer Poissonverteilung.\n\ncutting_cor_tbl %>% \n  select(flower:shoot) %>% \n  pairs.panels(smooth = TRUE, density = TRUE, method = \"kendall\", lm = FALSE, \n               cor = TRUE, ellipses = FALSE, stars = TRUE)    \n\n\n\n\nAbbildung 6.3— Abbildung der Scatterplots, Histogramme und Korrelation zwischen den vier Outcomes.\n\n\n\n\nIm Folgenden wollen wir nochmal den Zusammenhang zwischen der Anzahl der Blätter über alle Termine und der Anzahl an Blüten über alle Termine darstellen. Dafür müssen wir einmal unsere Spalten nach leaf oder flower auswählen und dann zusammenklappen. Danach klappen wir dann die Spalte outcome wieder auseinander. Dann können wir uns die Abbildung erstellen.\n\ncutting_leaf_flower_tbl <- cutting_raw_tbl %>% \n  select(trt, block, matches(\"leaf|flower\")) %>% \n  pivot_longer(cols = leaf_1:last_col(), \n               names_to = c(\"outcome\", \"week\"), \n               names_sep = \"_\",\n               values_to = \"rsp\") %>% \n  pivot_wider(names_from = outcome,\n              values_from = rsp) %>% \n  unnest(cols = c(flower, leaf))\n\nIn Abbildung 6.4 sehen wir dann den Zusammenhang zwischen der Anzahl der Blätter und der Anzahl an Blüten. Mit der Funktion stat_smooth() haben wir dann noch eine Gerade durch die Punkte gelegt.\n\ncutting_leaf_flower_tbl %>% \n  ggplot(aes(leaf, flower, color = trt)) +\n  theme_minimal() +\n  geom_point() +\n  stat_smooth(method = \"lm\")\n\n\n\n\nAbbildung 6.4— Zusammenhang zwischen der Anzahl an Blättern und der Anzahl an Blüten für die vier Behandlungen.\n\n\n\n\nWunderbar, jetzt können wir mit der Analyse der Outcomes anfangen. Wir rechnen als erstes die Trieblänge als eine multiple lineare Gaussian Regression. Danach dann die Anzahlen mit einer multiplen linearen Poisson Regression. Dann wollen wir noch das Compact letter display für die multiplen Vergleiche wiedergegeben haben.\n\n6.0.1 Trieblänge\nBeginnen wir damit, dass wir uns zuerst den Datensatz filtern. Wir wollen nur die Trieblänge und die achte Woche in unseren Analysedatensatz. Dann sind wir sicher, dass wir nicht ausversehen eine andere Analyse rechnen.\n\nshoot_length_tbl <- cutting_tbl %>% \n  filter(outcome == \"shoot\" & week == 8)\n\nWir bauen uns unser lineares Modell mit der Annahme einers normalverteilten Outcomes mit der Funktion lm(). Wir nehmen noch den Interaktionsterm für die Behandlung und Blöcke mit in das Modell.\n\nshoot_length_fit <- lm(rsp ~ trt + block + trt:block, \n                       data = shoot_length_tbl)\n\nEin kurzer Blick in eine zweifaktorielle ANOVA um zu schauen ob wir überhaupt einen Behandlunsgeffekt haben und eventuell auch eine signifikante Interaktion.\n\nshoot_length_fit %>% \n  anova() %>% \n  model_parameters()\n\nParameter | Sum_Squares | df | Mean_Square |     F |      p\n-----------------------------------------------------------\ntrt       |     1651.86 |  3 |      550.62 | 18.45 | < .001\nblock     |        1.67 |  1 |        1.67 |  0.06 | 0.816 \ntrt:block |       73.67 |  3 |       24.56 |  0.82 | 0.500 \nResiduals |      477.57 | 16 |       29.85 |       |       \n\nAnova Table (Type 1 tests)\n\n\nWir haben einen signifikanten Behandlungseffekt vorliegen und keine signifikante Interaktion zwischen der Behandlung und den Blöcken. Das ist wie wir es wollten und dann können wir noch kurz schauen, wie gut unser Experiment funktioniert hat. Wieviel Varianz können wir mit dem Behandlungen erklären? Wir berechnen dazu \\(\\eta^2\\).\n\nshoot_length_fit %>% \n  eta_squared()\n\n# Effect Size for ANOVA (Type I)\n\nParameter | Eta2 (partial) |       95% CI\n-----------------------------------------\ntrt       |           0.78 | [0.55, 1.00]\nblock     |       3.49e-03 | [0.00, 1.00]\ntrt:block |           0.13 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nMit einem \\(\\eta^2\\) von \\(0.78\\) können wir 78% der Varianz in unserem Experiment mit der Behandlung erklären. Das ist mehr als gut. Das spricht für ein gut geplantes und durchgeführtes Experiment.\nNachdem wir also sicher sind, dass wir etwas in den Daten haben, nutzen wir wieder das Paket emmeans um die paarweisen Vergleiche zu rechnen. Dafür legen wir erstmal wieder ein emmeans Objekt an. Wir müssen uns hier nicht um die Blöcke scheren, wir haben ja keine signifikante Interaktion vorliegen.\n\nshoot_emm_obj <- shoot_length_fit %>% \n  emmeans(specs = ~ trt) \n\nNOTE: Results may be misleading due to involvement in interactions\n\n\nAls nächstes rechnen wir dann schon die paarweisen Vergleiche und erhalten die Bonferroni adjustierten \\(p\\)-Werte zurück. Wenn du nicht adjustieren \\(p\\)-Werte willst, dann nehme einfach die Option adjust = \"none\".\n\nshoot_emm_obj %>% \n  contrast(method = \"pairwise\", adjust = \"bonferroni\")\n\n contrast                estimate   SE df t.ratio p.value\n control - nodium_3rd       18.53 3.15 16   5.874  0.0001\n control - nodium_5th       11.22 3.15 16   3.558  0.0157\n control - strong           21.53 3.15 16   6.825  <.0001\n nodium_3rd - nodium_5th    -7.31 3.15 16  -2.316  0.2049\n nodium_3rd - strong         3.00 3.15 16   0.951  1.0000\n nodium_5th - strong        10.31 3.15 16   3.267  0.0291\n\nResults are averaged over the levels of: block \nP value adjustment: bonferroni method for 6 tests \n\n\nWir können uns dann auch das compact letter display wiedergeben lassen.\n\nshoot_emm_obj %>%\n  cld(Letters = letters, adjust = \"bonferroni\") \n\n trt        emmean   SE df lower.CL upper.CL .group\n strong       34.8 2.23 16     28.5     41.1  a    \n nodium_3rd   37.8 2.23 16     31.5     44.1  ab   \n nodium_5th   45.1 2.23 16     38.8     51.4   b   \n control      56.3 2.23 16     50.1     62.6    c  \n\nResults are averaged over the levels of: block \nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 4 estimates \nP value adjustment: bonferroni method for 6 tests \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nInsbesondere in diesme Beispiel macht es auch Sinn sich nicht alle paarweisen Vergleiche anzuschauen sondern eben nur die Vergleiche zu der Kontrolle. Dafür nutzen wir dann die Methode trt.vs.ctrlk und wählen mit der Option ref = 1 unser Referenzlevel des Behandlungsfaktors. Da unsere Kontrolle schon das erste Level ist, schreiben wir also ref = 1 und erhalten die Vergleiche zu der Kontrolle.\n\nshoot_emm_obj %>% \n  contrast(method = \"trt.vs.ctrlk\", ref = 1, \n           adjust = \"bonferroni\")\n\n contrast             estimate   SE df t.ratio p.value\n nodium_3rd - control    -18.5 3.15 16  -5.874  0.0001\n nodium_5th - control    -11.2 3.15 16  -3.558  0.0079\n strong - control        -21.5 3.15 16  -6.825  <.0001\n\nResults are averaged over the levels of: block \nP value adjustment: bonferroni method for 3 tests \n\n\nWie wir sehen rechnen wir nur noch die Vergleiche zu der Kontrolle. Alle Vergleiche sind auch signifikant und die Kontrolle ist immer größer als die anderen Behandlungen. Wenn du das Minus gedreht haben möchtest, dann nutze reverse = TRUE als Option. Dann rechnest du Kontrolle minus den anderen Behandlungen.\nZum Abschluss können wir uns für die achte Woche noch die Barplots in der Abbildung 6.5 für die vier Behandlungen anschauen. Wir mitteln dabei über die beiden Blöcke. Wir wollen jetzt das Compact letter display noch ergänzen. Dafür bauen wir uns einen letter_vec in dem wir die richtig sortierten Buchstaben dann eintragen.\n\nletter_vec <- c(\"c\", \"ab\", \"b\", \"a\")\n\nDann bauen wir uns noch ein Objekt mit den Mittelwerten und der Stndardabweichung für die vier Behandlungen.\n\nshoot_stat_tbl <- shoot_length_tbl %>% \n  group_by(trt) %>% \n  summarise(mean = mean(rsp),\n            sd = sd(rsp))\n\nAbschließend können wir dann die Barplots mit den Fehlerbalken und dem Compact letter display darstellen. Ich habe hier mal eine rote Farbe für die Buchstaben gewählt, aber du kannst da gerne eine andere Farbe nutzen. so sieht man aber die Buchstaben auf den ersten Blick. Die Position der Buchstaben auf der y-Achse habe ich mit Try&Error dann selber rausgefunden. Geht sicherlich noch schöner, aber so ist es dann auch gut.\n\nggplot(shoot_stat_tbl, aes(x = trt, y = mean, fill = trt)) + \n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_errorbar(aes(ymin = mean-sd, ymax = mean+sd),\n                width = 0.2, position = position_dodge(0.9)) +\n  theme_bw() +\n  labs(y = \"Mittlere Anzahl an Blüten\",\n       x = \"\", fill = \"\") +\n  scale_fill_okabeito() +\n  annotate(\"text\", x = c(1, 2, 3, 4), c(70, 48, 55, 45),\n           label = letter_vec, size = 5, color = \"red\")\n\n\n\n\nAbbildung 6.5— Barplots der vier Behandlungen gemittelt über die Blöcke zusammen mit dem Compact letter display.\n\n\n\n\n\n\n6.0.2 Blütenanzahl\nIm Folgenden schaue ich mir nur die Blütenanzahl flowers als Outcome an. Die anderen Outcomes sind dann einfach nur Copy&Paste von dieser Analyse der Blütenanzahl. Wenn wir eine Analyse für eine Poissonregression rechnen können, dann können wir das auch ohne weiteres für die anderen beiden Outcomes.\nAls erstes filtern wir wieder die Daten. Wir wollen nur die Blüten und die achte Woche in unseren Analysedatensatz. Dann sind wir sicher, dass wir nicht ausversehen eine andere Analyse rechnen.\n\nflowers_tbl <- cutting_tbl %>% \n  filter(outcome == \"flower\" & week == 8)\n\nJetzt kommt der größte Unterschied. Wir rechnen kein lm() mit der implizieten Annahme eines normalverteilten Outcomes sondern ein glm() in dem wir dann die Verteilunsgfamilie des Outcomes definieren können. In unserem Fall dann für die Zähldaten die Quasipoissonverteilung. Sonst ist das Modell gleich wie ebi unserer Trieblänge.\n\nflowers_fit <- glm(rsp ~ trt + block + trt:block, \n                   data = flowers_tbl, \n                   family = quasipoisson())\n\nSchauen wir uns einmal die ANOVA an. Da wir ein GLM rechnen, können wir nicht die Funktion anova() nutzen sondern müssen die Funktion Anova() aus dem R Paket car anwenden. Dann erhalten wir auch die \\(p\\)-Werte die wir wollen.\n\nflowers_fit %>% \n  car::Anova() %>% \n  model_parameters()\n\nParameter |  Chi2 | df |      p\n-------------------------------\ntrt       | 19.41 |  3 | < .001\nblock     |  0.49 |  1 | 0.484 \ntrt:block |  1.80 |  3 | 0.615 \n\nAnova Table (Type 2 tests)\n\n\nIm Prinzip das gleiche Ergebnis wie schon bei der Trieblänge. Wir haben einen signifikanten Effekt der Behandlung auf die Blütenanzahl und keine signifikante Interaktion. Wir rechnen also mit dem Modell jetzt weiter. Wiederum nutzen wir das R Paket emmeans. Wichtig ist hier, dass wir als Typ dann die response angeben. Sonst erhalten wir alle Effektschätzer auf der Link-Scale und die Link-Scale können wir dann nicht interpretieren.\n\nflowers_emm_obj <- flowers_fit %>% \n  emmeans(specs = ~ trt, type = \"response\") \n\nWir machen auch hier wieder so weiter wie schon bei der Treiblänge. Einmal die paarweisen Vergleiche mit Bonferroni adjustierten \\(p\\)-Werten.\n\nflowers_emm_obj %>% \n  contrast(method = \"pairwise\", adjust = \"bonferroni\") \n\n contrast                ratio    SE  df null z.ratio p.value\n control / nodium_3rd    1.702 0.444 Inf    1   2.037  0.2498\n control / nodium_5th    1.087 0.249 Inf    1   0.363  1.0000\n control / strong        3.278 1.081 Inf    1   3.601  0.0019\n nodium_3rd / nodium_5th 0.639 0.169 Inf    1  -1.698  0.5374\n nodium_3rd / strong     1.926 0.683 Inf    1   1.847  0.3883\n nodium_5th / strong     3.016 1.002 Inf    1   3.323  0.0053\n\nResults are averaged over the levels of: block \nP value adjustment: bonferroni method for 6 tests \nTests are performed on the log scale \n\n\nDann lassen wir uns noch das Compact letter display für die Blütennzahl wiedergeben.\n\nflowers_emm_obj %>%\n  cld(Letters = letters, adjust = \"bonferroni\") \n\n trt         rate   SE  df asymp.LCL asymp.UCL .group\n strong      8.91 2.57 Inf      4.34      18.3  a    \n nodium_3rd 17.16 3.54 Inf     10.24      28.7  ab   \n nodium_5th 26.88 4.43 Inf     17.81      40.6   b   \n control    29.21 4.67 Inf     19.60      43.5   b   \n\nResults are averaged over the levels of: block \nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 4 estimates \nIntervals are back-transformed from the log scale \nP value adjustment: bonferroni method for 6 tests \nTests are performed on the log scale \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nAuch hier könnten wir nur zu der Kontrolle vergleichen, wenn wir das wollen würden. Hier ist aber das Rüchschneiden nicht so signifikant wie bei der Trieblänge. Hier sehen wir dann auch die Rate der Blüten. Wir haben also im Verhätnis zur Kontrolle nur knapp die Häfte an Blüten bei der Behandlung nodium_3rd und fast nur ein Drittel der Anzahl an Blüten bei der Behandlung strong.\n\nflowers_emm_obj %>% \n  contrast(method = \"trt.vs.ctrlk\", ref = 1, \n           adjust = \"bonferroni\")\n\n contrast             ratio    SE  df null z.ratio p.value\n nodium_3rd / control 0.587 0.153 Inf    1  -2.037  0.1249\n nodium_5th / control 0.920 0.211 Inf    1  -0.363  1.0000\n strong / control     0.305 0.101 Inf    1  -3.601  0.0010\n\nResults are averaged over the levels of: block \nP value adjustment: bonferroni method for 3 tests \nTests are performed on the log scale \n\n\nJetzt kannst du den R Code im Prinzip auch nochmal für die zwei anderen Outcomes durchführen."
  },
  {
    "objectID": "example-analysis-07.html",
    "href": "example-analysis-07.html",
    "title": "7  Einen TukeyHSD Test rechnen",
    "section": "",
    "text": "Version vom Mai 10, 2023 um 14:11:21\n\n\n\n\n\n\nBeispielhafte Auswertungen per Video\n\n\n\nDu findest auf YouTube Spielewiese in R - Teil 03.0 - ANOVA zusammen mit TukeyHSD die Auswertung einmal als Video. Ich werde zwar alles nochmal hier als Text aufschreiben, aber manchmal ist das Sehen und Hören dann einfacher.\n\n\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, readxl, \n               multcompView)\n\nIn diesem sehr kurzen Beispiel wollen wir uns einem den Tukey Test anschauen. Eigentlich ist es der Tukey HSD Test für Tukey Honest Significant Differences. Die von dieser Funktion zurückgegebenen \\(p\\)-Werte und 95% Konfidenzintervalle basieren auf der t-Test Verteilung. Wir nehmen also ein normalverteiltes Outcome \\(y\\) sowie eine homogene Varianz an.\nDie auf diese Weise konstruierten 95% Konfidenzintervalle gelten nur für balancierte Designs, bei denen in jedem Level des Faktors die gleiche Anzahl von Beobachtungen gemacht wird. Die Funktion TukeyHSD() enthält eine Anpassung für den Stichprobenumfang, die sinnvolle 95% Konfidenzintervalle für leicht unbalancierte Designs erzeugt. Ja, und da geht es schon wieder los. Was heißt sinnvoll? Oder andersherum, was ist eine leichte Abweichung? Deshalb mag ich persönlich nicht die Funktion TukeyHSD() und ziehe das R Paket emmeans vor. Du siehst auch in den anderen Beispielen immer die Anwendung von emmeans.\nAber gut, hier soll es ja um den Tukey Test gehen. Also wir nehmen uns einmal ein simples Beispiel mit verschiedenen Bodenarten und dann wollen wir als Outcome \\(y\\) die Pflanzenhöhe miteinander vergleichen.\n\nsoil_tbl <- read_excel(\"data/soil_1fac_data.xlsx\") %>% \n  mutate(variante = as_factor(variante)) %>% \n  select(variante, height)\n\nDie Funktion TukeyHSD() ist alt. Deshalb kann die Funktion nur den Modellfit aus der Funktion aov() verarbeiten. Die Funktion aov() wiederum ist eigentlich nur eine Funktion, die die beiden Funktionen anova() und lm() in ungünstiger Art und Weise kombiniert. Aber gut, geht eben nicht anders.\n\naov_fit <- aov(height ~ variante, data = soil_tbl)\n\nDann können wir den Modellfit in die Funktion TukeyHSD() stecken und benutzen. Wir kriegen eine etwas seltsame Ausgabe, aber wir wollen auch hier dann weiter zu dem compact letter display.\n\ntukey_obj <- aov_fit %>% \n  TukeyHSD()\n\nDas compact letter display können wir über die Funktion multcompLetters() erstellen. Wir müssen dafür die \\(p\\)-Werte extrahieren und dann an die Funktion weiterleiten. Hier ist wichtig, dass wir nur adjustierte \\(p\\)-Werte wiederbekommen. Wenn du unadjustierte \\(p\\)-Werte möchtest, dann musst du nochmal in die Funktion emmeans() schauen.\n\ntukey_obj %>% \n  pluck(\"variante\") %>% \n  magrittr::extract( , \"p adj\") %>% \n  multcompLetters()\n\n         Torf Kompostiertes  Gehäckseltes Aufgefasertes    Holzfasern \n         \"ab\"          \"ab\"           \"a\"          \"ab\"           \"b\" \n\n\nMehr zum compact letter display und der Interpretation kannst du im Kapitel zu Multiple Vergleiche oder Post-hoc Tests nachlesen. Dort findest du auch eine Alternative zu dem Tukey Test."
  },
  {
    "objectID": "example-analysis-08.html",
    "href": "example-analysis-08.html",
    "title": "8  Auswertung von Pflugdaten mit ggplot Templates",
    "section": "",
    "text": "Version vom Mai 10, 2023 um 14:11:24\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, readxl, see)\n\nIn diesem Beispiel wollen wir eine Exceldatei mit mehreren Tabellenblättern in eine Liste einlesen. Dann werden wir noch das Template in ggplot kennen lernen. Wir können nämlich auf Daten mit den gleichen Spaltennamen auch immer wieder den gleichen Plot anwenden. Das macht es natürlich recht einfach, denn wir müssen nicht immer den gleichen R Code tippen und Kopieren.\nFangen wir also alle Tabellenblätter einer Exceldatei einzulesen. Wir brauchen dafür den Pfad zu der Datei. Hier bei mit ist es recht einfach, aber bei dir mag es etwas komplizierter sein. Du kannst auch einfach die Datei einmal über das RStudio einlesen und dann den Pfad kopieren.\n\nplowing_file <- file.path(\"data/plowing_data.xlsx\")\n\nIn Folgenden leiten wir den Pfad in die Funktion excel_sheets() die uns dann die einzelnen Tabellenblätter wiedergibt. Dann müssen wir noch den Vektor mit den Tabellenblätternmane benennen. Dann können wir die Funktion map() nutzen um alle Blätter, die sich in der Exceldatei im Pfad befinden, in eine Liste laden.\n\nplowing_lst <- plowing_file %>% \n  excel_sheets() %>% \n  rlang::set_names() %>% \n  map(read_excel, path = plowing_file)\n\nWir wollen jetzt auf der Liste mit der Funktion map() noch ein paar Anpassungen durchführen. Die Funktion map() erlaubt uns andere Funktion auf alle Listeneinträge der Liste anzuwenden. Als erstes Sortieren wir alle Tibble in jedem Listeneintrag und verwandeln dann die Spalten in Faktoren, die eben auch Faktoren repräsentieren.\n\nplowing_lst <- plowing_lst %>% \n  map(~arrange(.x, day, block, variant)) %>% \n  map(~mutate(.x, \n              block = as_factor(block),\n              variant = as_factor(variant),\n              day = as_factor(day))) \n\nJetzt brauchen wir aber die Datensätze wieder einzeln und wollen uns daher einmal die beiden Listen as und water mit der Funktion pluck() aus der Liste rausziehen und in ein eigenes Objekt abspeichern.\n\nas_tbl <- pluck(plowing_lst, \"as\")\nwater_tbl <- pluck(plowing_lst, \"water\")\n\nNun kommen wir zu der Möglichkeit ein Template in ggplot anzulegen. Wir bauen uns dafür einfach genau so wie immer ein ggplot. Mit der Ausnahme, dass wir keine Daten in die Funktion ggplot() stecken sondern eine leere Funktion ggplot() mit der Funktion aes() verbinden. Das machen wir dann für alle Layer, die wir dann in dem ggplot() sehen wollen. Dann speichern wir das ggplot Objekt für unseren Barplot ab.\n\np_template <- ggplot() +\n  aes(x = variant, y = mean, group = day, fill = day) + \n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_errorbar(aes(ymin = mean-sd, ymax = mean+sd),\n                width = 0.2, position = position_dodge(0.9)) +\n  theme_bw() +\n  labs(fill = \"Tag\") +\n  scale_fill_okabeito()\n\nWir wollen jetzt einmal für das Outcome as den Barplot erstellen. Dafür brauchen wir wieder die Daten, die wir uns mit der Funktion summarise() zusammenbauen. Wir haben dann die Mittelwerte und die Standardabweichung für die Behandlungen.\n\nas_stat_tbl <- as_tbl %>% \n  group_by(day, variant) %>% \n  summarise(mean = mean(as),\n            sd = sd(as))\n\n`summarise()` has grouped output by 'day'. You can override using the `.groups`\nargument.\n\n\nIn der Abbildung 8.1 sehen wir dann den Barplot für unser Outcome as. Der Barplot ist jetzt erstmal nichts besonderes, aber wie wir den Plot bauen schon. Wir nutzen dafür den Operator %+%. Mit dem Operator %+% können wir einen leeren ggplot mit einem neuen Datensatz verbinden. Wichtig ist nur, dass die Spaltennamen dann jeweils übereinstimmen.\n\np_template %+% as_stat_tbl\n\n\n\n\nAbbildung 8.1— Barplot für das Outcome as.\n\n\n\n\nDas ganze können wir dann auch einmal für das Outcome water machen.\n\nwater_stat_tbl <- water_tbl %>% \n  group_by(day, variant) %>% \n  summarise(mean = mean(water),\n            sd = sd(water))\n\n`summarise()` has grouped output by 'day'. You can override using the `.groups`\nargument.\n\n\nIn der Abbildung 8.2 sehen wir dann die Anwendung des Operators %+% auf den Datensatz für unser Outcome water. Das ist jetzt natürlich sehr effizient. Wir können das Template auf die anderen beiden Outcomes ebenfalss anwenden. Damit sind wir sicher, das wir immer die gleiche Beschriftung und eben auch die gleichen Farben haben.\n\np_template %+% water_stat_tbl\n\n\n\n\nAbbildung 8.2— Barplot für das Outcome water.\n\n\n\n\nWie immer musst du bei jedem Beispiel schauen, ob du alles so stark automatisieren willst. Meistens ist die Automatisierung nicht notwendig, aber wenn du wie ich dann teilweise sehr viele Datensätze auswertest, dann geht die Auswertung natürlich mit einer Automatisierung sehr viel schneller."
  },
  {
    "objectID": "example-analysis-09.html",
    "href": "example-analysis-09.html",
    "title": "9  Auswertung mit Messwiederholungen",
    "section": "",
    "text": "Wir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, readxl, see, janitor,\n               lme4, performance, emmeans, multcomp,\n               conflicted)\n## resolve some conflicts with same function naming\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\n\nIn diesem Abschnitt wollen wir ein multiples lineares Gaussion gemischtes Modell rechnen. Das heißt, wir wollen die Messwiederholungen über die Wochen mit in unser Modell als einen zufälligen Effekt aufnehmen. Dafür brauchen wir wieder unsere Daten zu dem Rückschnitt. Wir nehmen aber hier nur die Trieblänge als normalverteiltes Outcome. Dann wollen wir herausfinden, ob die drei Arten des Rückschnitts und die Kontrolle einen Einfluss auf die Trieblänge haben.\n\ncutting_raw_tbl <- read_excel(\"data/cutting_data.xlsx\") %>% \n  clean_names()\n\nWir in Kapitel 6 müssen wir die Daten wieder von dem Wide-Format in das Long-Format umwandeln. Wir wollen aber vorher nur die Spalten wähen die shoot in dem Spaltennamen haben. Diese Auswahl können wir über die Funktion matches() erreichen. Dann können wir die Funktion pivot_longer() nutzen um unseren Datensatz in dem Long_Format zu bauen.\n\ncutting_tbl <- cutting_raw_tbl %>% \n  select(trt, block, matches(\"shoot\")) %>% \n  pivot_longer(cols = shoot_1:last_col(), \n               names_to = c(\"outcome\", \"week\"), \n               names_sep = \"_\",\n               values_to = \"rsp\") %>% \n  arrange(outcome, week, trt, block, rsp) %>% \n  mutate(block = as_factor(block),\n         trt = as_factor(trt),\n         outcome = as_factor(outcome),\n         week = as.numeric(week))\n\nSchauen wir uns in der Abbildung 9.1 nochmal die Daten an. Wir haben einen Effekt zwischen den Behandlungen an jedem Messtermin. Die Pflanzen an den jeweiligen Messterminen sind aber nicht unabhängig voneinander. Wir messen ja immer wieder die gleiche Pflanze. So sind die Messtermine untereinander korreliert. Um diese Korrelation oder Abhängigkeit der Mestermine zu modellieren, nutzen wir lineare gemischte Modelle.\n\ncutting_tbl %>% \n  ggplot(aes(week, rsp, color = trt, linetype = block)) +\n  theme_minimal() +\n  geom_point() +\n  stat_smooth(se = FALSE) +\n  scale_color_okabeito()\n\n\n\n\nAbbildung 9.1— Abbildung der vier Outcomes über die acht Messtermine mit den vier Behandlungen und den zwei Blöcken.\n\n\n\n\nIn R nutzen wir das Paket lme4 mit der Funktion lmer() um ein lineares gemisches Modell mit Normalverteilungsannahme zu fitten. Der vordere Teil des Modells ist gleich wie in einem normalen linearen Modell. Was sich hier ändert ist der zufällige Effekt, den wir durch (1|week) beschreiben. Unsere Wochen sind untereinander korreliert und deshalb wandert der Faktor Woche in den zufälligen Term.\n\nlmer_fit <- lmer(rsp ~ trt + block + trt:block + (1|week), \n                 data = cutting_tbl)\n\nSchauen wir einmal an, wie gut unser Modell funktioniert hat. Wieviel der Varianz kann denn unser Modell erklären? Wir nutzen dazu die Funktion r2() aus dem R Paket performance.\n\nlmer_fit %>% r2\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.940\n     Marginal R2: 0.470\n\n\nDas \\(R^2_{conditional}\\) ist der erklärte Anteil der Varianz von den festen und zufälligen Effekten zusammen. Das \\(R^2_{marginal}\\) ist der erklärte Anteil der Varianz von den festen Effekten alleine.\nDas \\(R^2_{conditional}\\) ist der erklärte Anteil der Varianz von den festen und zufälligen Effekten zusammen. Da wir hier ein \\(R^2\\) von fast 95% haben, können wir mit unserem Modell fast die gesamte Variabilität in unserem Experiment erklären. Das ist selten zu beobachten, aber sehr schön. Das \\(R^2_{marginal}\\) ist der erklärte Anteil der Varianz von den festen Effekten alleine. Hier haben wir dann gut 50% erklärte Varianz. Damit wissen wir immerhin, dass gut die Hälfte der Variabilität in unseren Daten von unserer behandlung kommt, unabhängig von dem zufälligen Effekt.\nWir können dann noch den Intraclass Correlation Coefficient (abk. ICC) berechnen. Der ICC beschreibt den Anteil der Varianz, der durch die Gruppierungsstruktur in der Stichprobe erklärt wird. Wir können den Wert direkt aus der Summary des lmer Objektes berechnen.\n\nlmer_fit %>% summary()\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: rsp ~ trt + block + trt:block + (1 | week)\n   Data: cutting_tbl\n\nREML criterion at convergence: 1067.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.5902 -0.5207 -0.0094  0.4956  4.6832 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n week     (Intercept) 109.13   10.447  \n Residual              13.82    3.717  \nNumber of obs: 192, groups:  week, 8\n\nFixed effects:\n                      Estimate Std. Error t value\n(Intercept)           47.28472    3.77062  12.540\ntrtnodium_3rd        -21.29861    1.07309 -19.848\ntrtnodium_5th        -15.57639    1.07309 -14.515\ntrtstrong            -29.03472    1.07309 -27.057\nblock2                -0.01403    1.07309  -0.013\ntrtnodium_3rd:block2  -2.19417    1.51758  -1.446\ntrtnodium_5th:block2  -1.57611    1.51758  -1.039\ntrtstrong:block2       2.97944    1.51758   1.963\n\nCorrelation of Fixed Effects:\n            (Intr) trtn_3 trtn_5 trtstr block2 tr_3:2 tr_5:2\ntrtnodm_3rd -0.142                                          \ntrtnodm_5th -0.142  0.500                                   \ntrtstrong   -0.142  0.500  0.500                            \nblock2      -0.142  0.500  0.500  0.500                     \ntrtndm_3r:2  0.101 -0.707 -0.354 -0.354 -0.707              \ntrtndm_5t:2  0.101 -0.354 -0.707 -0.354 -0.707  0.500       \ntrtstrng:b2  0.101 -0.354 -0.354 -0.707 -0.707  0.500  0.500\n\n\nDafür nehmen wir die Varianz der Gruppe week und teilen die Varianz durch die gesamte Varianz bestehend aus den Residuen plus der Varianz der Gruppe. Damit erfahren wir dann, dass in der zufälligen Effekten unsere Gruppe week fast 90% der Varianz erklärt. Was wiederum ein wirklich guter Wert ist.\n\\[\nICC = 109.134/(109.134 + 13.818) = 0.888\n\\]\nNatürlich können wir das ICC auch direkt in R mit einer Funktion berechnen lassen. Wir nutzen nur den adjustierten ICC und ignorieren in diesem Fall den anderen Wert.\n\nlmer_fit %>% icc\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.888\n  Unadjusted ICC: 0.471\n\n\nJetzt können wir noch flott die paarweisen Vergleiche rechnen, denn praktischerweose ändert sich hier nichts mehr. Wir können die den Fit aus unserem linearen gemischten Modell einfach in die Funktionalität von emmeans pipen und dann geht alles seinen normalen Gang.\n\nemm_obj <- lmer_fit %>% \n  emmeans(specs = ~ trt) \n\nWir machen auch hier wieder so weiter wie wir es schon gewöhnt sind. Einmal rechnen wir die paarweisen Vergleiche mit Bonferroni adjustierten \\(p\\)-Werten. Spannend ist jetzt, dass wir hier über alle Messzeitpunkte hinweg einen Unterschied zwischen allen Behandlungen sehen.\n\nemm_obj %>% \n  contrast(method = \"pairwise\", adjust = \"bonferroni\") \n\n contrast                estimate    SE  df t.ratio p.value\n control - nodium_3rd       22.40 0.759 177  29.515  <.0001\n control - nodium_5th       16.36 0.759 177  21.567  <.0001\n control - strong           27.55 0.759 177  36.301  <.0001\n nodium_3rd - nodium_5th    -6.03 0.759 177  -7.949  <.0001\n nodium_3rd - strong         5.15 0.759 177   6.786  <.0001\n nodium_5th - strong        11.18 0.759 177  14.735  <.0001\n\nResults are averaged over the levels of: block \nDegrees-of-freedom method: kenward-roger \nP value adjustment: bonferroni method for 6 tests \n\n\nDann lassen wir uns noch das Compact letter display für die Trieblänge wiedergeben. Auch hier sehen wir, dass sich alle Behandlungen voneinander über alle Zeitpunkte hinweg voneinander unterscheiden. Das stimmt dann auch mit unserer Abbildung überein, die Geraden der Behandlungen laufen ja alle nebeneinander mit Abstand.\n\nemm_obj %>%\n  cld(Letters = letters, adjust = \"bonferroni\") \n\n trt        emmean   SE   df lower.CL upper.CL .group\n strong       19.7 3.73 7.22     7.41     32.1  a    \n nodium_3rd   24.9 3.73 7.22    12.56     37.2   b   \n nodium_5th   30.9 3.73 7.22    18.59     43.2    c  \n control      47.3 3.73 7.22    34.95     59.6     d \n\nResults are averaged over the levels of: block \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 4 estimates \nP value adjustment: bonferroni method for 6 tests \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nUnd dann nochmal der Vergleich zu der Kontrolle, falls das mehr gewünscht sein sollte. Auch hier sehen wir, dass alle Vergleiche zu der Kontrolle signifikant sind. Über alle Messzeitpunkte hinweg ist die Kontrolle immer Größer als die Pflanzen mit den Rückschnittbehandlungen.\n\nemm_obj %>% \n  contrast(method = \"trt.vs.ctrlk\", ref = 1, \n           adjust = \"bonferroni\")\n\n contrast             estimate    SE  df t.ratio p.value\n nodium_3rd - control    -22.4 0.759 177 -29.515  <.0001\n nodium_5th - control    -16.4 0.759 177 -21.567  <.0001\n strong - control        -27.5 0.759 177 -36.301  <.0001\n\nResults are averaged over the levels of: block \nDegrees-of-freedom method: kenward-roger \nP value adjustment: bonferroni method for 3 tests"
  },
  {
    "objectID": "example-analysis-10.html",
    "href": "example-analysis-10.html",
    "title": "10  Spurenelemente in Spinatblättern und Stielen",
    "section": "",
    "text": "Version vom Mai 10, 2023 um 14:11:32\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, readxl, see, janitor,\n               effectsize, emmeans, multcomp, psych,\n               parameters, scales,\n               #psych, parameters, effectsize, emmeans,\n               #multcomp, \n               conflicted)\n## resolve some conflicts with same function naming\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\n\nIn der folgenden Datenanalyse schauen wir uns die Konzentrationen von den Spurenelementen Fe, Cd und Zn in Spinat an. Dabei haben wir zum einen in den Blätter und zum anderen in den Stielen gemessen. Daher haben wir insgesamt drei Outcomes an zwei Orten und somit sechs Kombinationen auszuwerten. Wir haben uns darüber hinaus noch sieben Behandlungen in je vier Blöcken als Wiederholung angeschaut. Laden wir also einmal die Daten.\n\nspinach_tbl <- read_excel(\"data/spinach_metal_data.xlsx\") %>% \n  mutate(trt = as_factor(trt),\n         sample = as_factor(sample),\n         block = as_factor(block))\n\nWir haben die Daten im Wide-Format vorliegen, daher müssen wir die Daten über die Funktion pivot_longer() noch in das Long-Format umwandeln. Zum einen brauchen wir das Long-Format für unsere Abbildungen und zum anderen dann auch für unsere Analysen.\n\nspinach_plot_tbl <- spinach_tbl %>% \n  pivot_longer(cols = fe:zn,\n               names_to = \"outcome\",\n               values_to = \"rsp\") %>% \n  mutate(outcome = as_factor(outcome))\n\nspinach_plot_tbl %>% \n  head\n\n# A tibble: 6 x 5\n  trt   sample block outcome     rsp\n  <fct> <fct>  <fct> <fct>     <dbl>\n1 1     leaf   1     fe      16.9   \n2 1     leaf   1     cd       9.28  \n3 1     leaf   1     zn       0.127 \n4 1     stem   1     fe       7.95  \n5 1     stem   1     cd       7.97  \n6 1     stem   1     zn       0.0579\n\n\nWir sehen, dass wir für jede Outcome/Sample Kombination ein zweifaktorielles Modell für Behandlung und Block rechnen müssen. Schauen wir uns zuerst wie immer einmal die Abbildung 10.1 in ggplot() an und machen dann mit der Auswertung weiter.\n\nggplot(spinach_plot_tbl, aes(trt, rsp, shape = block, color = trt)) +\n  theme_minimal() +\n  geom_jitter() +\n  facet_wrap(~ outcome*sample, scales = \"free_y\", ncol = 2) +\n  labs(x = \"Behandlung\", y = \"Gemessenes Outcome\",\n       shape = \"Block\", color = \"Behandlung\") +\n  scale_color_okabeito()\n\n\n\n\nAbbildung 10.1— Abbildung der drei Outcomes in den zwei Samples für die sieben Behandlungen in jeweils vier Blöcken.\n\n\n\n\nDa wir nur eine Beobachtung je Block/Behandlung Kombination haben, können wir später keine Interaktion rechnen. Wir bräiuchten dafür Wiederholungen auf der Ebene der Blöcke. Also nicht nur eine Pflanze pro Block und Behandlung. Im Folgenden gibt es einmal die umständliche Copy&Paste Variante und einmal die etwas komplexere Lösung über map() und nest().\n\n10.0.1 Eisen (Fe) und Blatt\nMachen wir es uns erstmal einfach. Wir müssen ja für jede der sechs Outcome/Sample Kombinationen eine ANOVA rechnen, dann rechnen wir einen multiplen Vergleich und lassen uns das Compact letter display wiedergeben. Das machen wir jetzt alles einmal für die Kombination Eisen und Blatt.\nAlso brauchen wir als erstes unseren Datensatz mit nur dem Outcome gleich fe und das Sample gleich leaf. Das Selektieren machen wir dann über die Funktion filter().\n\nfe_leaf_tbl <- spinach_plot_tbl %>% \n  filter(outcome == \"fe\" & sample == \"leaf\")\n\nDan müssen wir das lineare Modell schätzen. Das Schätzen der Koeffizienten übernimmt wie immer die Funktion lm(). Wir nehmen hier an, dass unsere Konzentrationen ungefähr normalverteilt sind. Wir können den Interaktionsterm trt:block nicht mit ins Modell nehmen, da wir nur eine Beobachtung je Behandlung/Block Kombination haben.\n\nfe_leaf_fit <- lm(rsp ~ trt + block, data = fe_leaf_tbl)\n\nJetzt können wir die ANOVA rechnen und schauen was wir da haben.\n\nfe_leaf_fit %>% anova()\n\nAnalysis of Variance Table\n\nResponse: rsp\n          Df  Sum Sq Mean Sq F value    Pr(>F)    \ntrt        6 1015.45 169.242 19.0531 6.759e-07 ***\nblock      3   84.35  28.117  3.1654   0.04975 *  \nResiduals 18  159.89   8.883                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWir haben einen ganz schwachen signifikanten Effekt des Blocks und einen starken signifikanten Effekt der Behandlung. Der Effekt des Blocks ist nicht so schön, wir würden eigentlich erwarten, dass der Block keinen Effekt hat. Wir haben ja die Zuordnung der Behandlungen zu den Blöcken zufällig durchgeführt. Da sollte also eigentlich kein Effekt des Blocks auftreten. Schauen wir nochmal wie stark die Effekte sind in dem wir uns das \\(\\eta^2\\) berechnen.\n\nfe_leaf_fit %>% eta_squared()\n\n# Effect Size for ANOVA (Type I)\n\nParameter | Eta2 (partial) |       95% CI\n-----------------------------------------\ntrt       |           0.86 | [0.73, 1.00]\nblock     |           0.35 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nDer Effekt der behandlung it auf jeden Fall größer als der Effelt des Blocks. Gut 86% der Vrainz wird durch die Behandlung erklärt, dass passt dann soweit.\nWir rechnen jetzt mit der Funktion emmeans() weiter und berücksichtigen dabei die unterschiedlichen Mittelwerte der Blöcke für die einzelnen Behandlungen.\n\nfe_leaf_emm <- fe_leaf_fit %>% \n  emmeans(~ trt)\n\nJetzt können wir un die paarweisen Vergleich über die Funktion contrast() wiedergeben lassen. Wir sehen, dass wir einiges an signifikanten Ergebnissen vorliegen haben. Du kannst die Ausgabe in ein Tibble mit as_tibble() umwandeln und dir dann mit der Funktion print(n = 21) alle Zeilen ausgeben lassen.\n\nfe_leaf_emm %>% \n  contrast(method = \"pairwise\", adjust = \"bonferroni\") \n\n contrast    estimate   SE df t.ratio p.value\n trt1 - trt2   -7.844 2.11 18  -3.722  0.0328\n trt1 - trt3  -12.156 2.11 18  -5.768  0.0004\n trt1 - trt4  -17.429 2.11 18  -8.270  <.0001\n trt1 - trt5  -11.218 2.11 18  -5.323  0.0010\n trt1 - trt6  -16.877 2.11 18  -8.008  <.0001\n trt1 - trt7  -18.222 2.11 18  -8.646  <.0001\n trt2 - trt3   -4.312 2.11 18  -2.046  1.0000\n trt2 - trt4   -9.585 2.11 18  -4.548  0.0052\n trt2 - trt5   -3.374 2.11 18  -1.601  1.0000\n trt2 - trt6   -9.033 2.11 18  -4.286  0.0093\n trt2 - trt7  -10.378 2.11 18  -4.924  0.0023\n trt3 - trt4   -5.273 2.11 18  -2.502  0.4664\n trt3 - trt5    0.938 2.11 18   0.445  1.0000\n trt3 - trt6   -4.721 2.11 18  -2.240  0.7970\n trt3 - trt7   -6.066 2.11 18  -2.878  0.2101\n trt4 - trt5    6.211 2.11 18   2.947  0.1809\n trt4 - trt6    0.552 2.11 18   0.262  1.0000\n trt4 - trt7   -0.793 2.11 18  -0.376  1.0000\n trt5 - trt6   -5.659 2.11 18  -2.685  0.3175\n trt5 - trt7   -7.004 2.11 18  -3.323  0.0794\n trt6 - trt7   -1.345 2.11 18  -0.638  1.0000\n\nResults are averaged over the levels of: block \nP value adjustment: bonferroni method for 21 tests \n\n\nSchauen wir uns für diesen Vergleich dann noch das Compact letter display an. Bitte beachte, dass du dir mit der Funktion arrange() immer die Reihenfolge der Behandlungen ausgeben lassen kannst. Sonst ist die Ausgabe nach der Spalte .group sortiert und nicht nach den Behandlungen. Wenn die Buchstaben nicht gleich sind, dann unterscheiden sich die Behandlungen.\n\nfe_leaf_emm %>%\n  cld(Letters = letters, adjust = \"none\") %>% \n  arrange(trt)\n\n trt emmean   SE df lower.CL upper.CL .group\n 1     11.8 1.49 18     8.66     14.9  a    \n 2     19.6 1.49 18    16.50     22.8   b   \n 3     23.9 1.49 18    20.81     27.1   b   \n 4     29.2 1.49 18    26.09     32.3    c  \n 5     23.0 1.49 18    19.88     26.1   b   \n 6     28.7 1.49 18    25.53     31.8    c  \n 7     30.0 1.49 18    26.88     33.1    c  \n\nResults are averaged over the levels of: block \nConfidence level used: 0.95 \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nAuch hier schauen wir uns einmal die Korrelation zwischen den Outcomes in der Abbildung 10.2 an. Wir sehen in der Abbildung 10.2 (a) die Korrelation zwischen den beiden Outcomes Fe und Zn zusammen in den Blättern und den Stielen. Wir würden meinen, dass wir eine positive Korrelation vorliegen haben. Wir sehen, dass mit mehr Zn auch mehr Fe auftritt. Was uns aber etwas stutzig werden lässt, sind die beiden Punktewolken in der linken unteren Abbildung. Daher einmal schnell aufgetrennt für die Blätter in Abbildung 10.2 (b) und für die Stiele in Abbildung 10.2 (c). Wir sehen, dass wir nichts sehen. Denn getrennt für die Blätter und Stiele haben wir dann keine Korrelation mehr für die beiden Outcomes vorliegen.\n\nspinach_tbl %>% \n  select(fe, zn) %>% \n  pairs.panels(smooth = TRUE, density = TRUE, method = \"kendall\", lm = TRUE, \n               cor = TRUE, ellipses = FALSE, stars = TRUE) \n\nspinach_tbl %>% \n  filter(sample == \"leaf\") %>% \n  select(fe, zn) %>% \n  pairs.panels(smooth = TRUE, density = TRUE, method = \"kendall\", lm = TRUE, \n               cor = TRUE, ellipses = FALSE, stars = TRUE)    \n\nspinach_tbl %>% \n  filter(sample == \"stem\") %>% \n  select(fe, zn) %>% \n  pairs.panels(smooth = TRUE, density = TRUE, method = \"kendall\", lm = TRUE, \n               cor = TRUE, ellipses = FALSE, stars = TRUE) \n\n\n\n\n\n\n\n(a) Blätter und Stiele.\n\n\n\n\n\n\n\n(b) Nur Blätter.\n\n\n\n\n\n\n\n(c) Nur Stiele.\n\n\n\n\nAbbildung 10.2— Korrelation und Verteilung zwischen den Outcomes Fe und Zn.\n\n\n\n\n\n10.0.2 Und alles aufeinmal…\nNun gut, jetzt haben wir unsere Analyse für das Blatt und den Eisengehalt gerechnet. Wir müssten jetzt die Analyse nochmal für alle anderen fünf Kombinationen durchführen. Das würde einiges an Zeit kosten und auch sehr viel Copy&Paste Aufwand. Kann man machen, aber wir können die Analyse auch in einem Rutsch durchführen. Dafür nutzen wir die Funktion nest() und dann anschließend die Funktion map() um auf den genesteten Daten die Analysen zu rechnen.\nIm ersten Schritt müssen wir unsere Daten gruppieren. Wir haben dann die sechs Kombinationen aus Outcome und Sample vorliegen. Dann nesten wir den Datensatz in sechs Zeilen. Wir klappen sozusagen die Daten für jede der sechs Kombinationen zusammen. Alles fällt dann in eine Zelle zusammen.\n\nspinach_nest_tbl <- spinach_plot_tbl %>% \n  group_by(sample, outcome) %>% \n  nest() \n\nSchauen wir uns den genesteten Datensatz einmal an. Wir sehen, dass wir die gesamten Daten in der Spalte data zusammengefaltet haben. Wir haben also sechs Tibbles mit den Daten der jeweiligen Outcome/Sample Kombinationen in der Spalte data vorliegen.\n\nspinach_nest_tbl \n\n# A tibble: 6 x 3\n# Groups:   sample, outcome [6]\n  sample outcome data             \n  <fct>  <fct>   <list>           \n1 leaf   fe      <tibble [28 x 3]>\n2 leaf   cd      <tibble [28 x 3]>\n3 leaf   zn      <tibble [28 x 3]>\n4 stem   fe      <tibble [28 x 3]>\n5 stem   cd      <tibble [28 x 3]>\n6 stem   zn      <tibble [28 x 3]>\n\n\nWir können jetzt auf den Tibbles in der Spalte data weiter rechnen. Wir nutzen für das Weiterrechnen die Funktion map(), die in jeder Zeile der Spalte .data die gleiche Funktion ausführt. Unser Ergebnis speichern wir dann in einer neuen Spalte und dafür nutzen wir die Funktion mutate().\nKonkret erstellen wir uns jetzt eine neue Spalte model in der das lineare Modell der Funktion lm() abliegt.\n\nspinach_model_tbl <- spinach_nest_tbl %>%\n  mutate(model = map(data, ~lm(rsp ~ trt + block, data = .x))) \n\nIm weiteren Schritt rechnen wir jetzt auf der Spalte model eine ANOVA und lassen uns dann die schönere Ausgabe über die Funktion model_parameters() wiedergeben. Der wichtigste Tiel ist die Funktion unnest() die uns die Zellen mit den ANOVA Ergebnissen dann wieder ausklappt. Der Rest ist dann noch filtern und anpassen. Ich möchte das die Ausgabe reduziert ist und die p-Werte sollen auch schön formatiert werden.\n\nspinach_model_tbl %>% \n  mutate(anova = map(model, anova)) %>% \n  mutate(parameter = map(anova, model_parameters)) %>% \n  select(sample, outcome, parameter) %>% \n  unnest(parameter) %>% \n  filter(Parameter != \"Residuals\") %>% \n  select(sample, outcome, Parameter, p) %>% \n  mutate(p = pvalue(p))\n\n# A tibble: 12 x 4\n# Groups:   sample, outcome [6]\n   sample outcome Parameter p     \n   <fct>  <fct>   <chr>     <chr> \n 1 leaf   fe      trt       <0.001\n 2 leaf   fe      block     0.050 \n 3 leaf   cd      trt       0.047 \n 4 leaf   cd      block     <0.001\n 5 leaf   zn      trt       0.424 \n 6 leaf   zn      block     0.290 \n 7 stem   fe      trt       0.021 \n 8 stem   fe      block     0.016 \n 9 stem   cd      trt       0.325 \n10 stem   cd      block     <0.001\n11 stem   zn      trt       0.512 \n12 stem   zn      block     <0.001\n\n\nAuch können wir un die \\(\\eta^2\\) für die Modelle berechnen lassen. Die Funktion unnest() klappt uns dann die Ergebnisse wieder aus. Dann müssen wir noch etwas aufräumen und schon haben wir für alle Kombinationen dann den Anteil der erklärten Varianz.\n\nspinach_model_tbl %>%  \n  mutate(eta = map(model, eta_squared)) %>% \n  unnest(eta) %>% \n  clean_names() %>% \n  select(sample, outcome, eta2_partial) \n\n# A tibble: 12 x 3\n# Groups:   sample, outcome [6]\n   sample outcome eta2_partial\n   <fct>  <fct>          <dbl>\n 1 leaf   fe             0.864\n 2 leaf   fe             0.345\n 3 leaf   cd             0.475\n 4 leaf   cd             0.775\n 5 leaf   zn             0.260\n 6 leaf   zn             0.183\n 7 stem   fe             0.529\n 8 stem   fe             0.430\n 9 stem   cd             0.295\n10 stem   cd             0.652\n11 stem   zn             0.232\n12 stem   zn             0.589\n\n\nIm letzten Schritt bauen wir uns die Spalten für die Funktion emmeans(), dann die Kontraste und das Compact letter display. Hier nutzen wir die Schreibweise map(<Spalte>, <Funktion>, <Optionen>). Daher definieren wir erst welche Spalte map() bearbeiten soll. Dann die Funktion die map() nutzen soll und anschließend die Optionen für die Funktion. Wir können hier auch mehrere Optionen nacheinander angeben.\n\nspinach_emm_tbl <- spinach_model_tbl %>%  \n  mutate(emm = map(model, emmeans, ~trt)) %>% \n  mutate(contrast = map(emm, contrast, method = \"pairwise\", \n                        adjust = \"none\")) %>% \n  mutate(cld = map(emm, cld, Letters = letters, adjust = \"none\"))\n\nJetzt lassen wir uns die Spalte contrast wiedergeben. Wir müssen aber vorher die Spalte noch in ein Tibble umwandeln. Dann wollen wir noch die p-Werte schöner haben. Wichtig ist auch immer, dass wir über die Funktion select() die für uns wichtigen Spalten auswählen.\n\nspinach_emm_tbl %>% \n  mutate(contrast = map(contrast, as_tibble)) %>% \n  unnest(contrast) %>% \n  select(sample, outcome, contrast, p.value) %>% \n  mutate(p.value = pvalue(p.value))\n\n# A tibble: 126 x 4\n# Groups:   sample, outcome [6]\n   sample outcome contrast    p.value\n   <fct>  <fct>   <chr>       <chr>  \n 1 leaf   fe      trt1 - trt2 0.002  \n 2 leaf   fe      trt1 - trt3 <0.001 \n 3 leaf   fe      trt1 - trt4 <0.001 \n 4 leaf   fe      trt1 - trt5 <0.001 \n 5 leaf   fe      trt1 - trt6 <0.001 \n 6 leaf   fe      trt1 - trt7 <0.001 \n 7 leaf   fe      trt2 - trt3 0.056  \n 8 leaf   fe      trt2 - trt4 <0.001 \n 9 leaf   fe      trt2 - trt5 0.127  \n10 leaf   fe      trt2 - trt6 <0.001 \n# i 116 more rows\n\n\nNachdem wir uns die Kontraste für die paarweisen Vergleiche wiedergeben haben lassen, wollen wir jetzt noch die ganzen Compact letter displays haben. Auch hier nutzen wir dann die Funktion unnest() und wolle dann nicht alle Spalten haben.\n\nspinach_emm_tbl %>% \n  mutate(cld = map(cld, arrange, trt)) %>% \n  unnest(cld) %>% \n  select(sample, outcome, trt, .group) %>% \n  print(n = 15)\n\n# A tibble: 42 x 4\n# Groups:   sample, outcome [6]\n   sample outcome trt   .group\n   <fct>  <fct>   <fct> <chr> \n 1 leaf   fe      1     \" a  \"\n 2 leaf   fe      2     \"  b \"\n 3 leaf   fe      3     \"  b \"\n 4 leaf   fe      4     \"   c\"\n 5 leaf   fe      5     \"  b \"\n 6 leaf   fe      6     \"   c\"\n 7 leaf   fe      7     \"   c\"\n 8 leaf   cd      1     \" a  \"\n 9 leaf   cd      2     \"  bc\"\n10 leaf   cd      3     \"  bc\"\n11 leaf   cd      4     \" ab \"\n12 leaf   cd      5     \"   c\"\n13 leaf   cd      6     \" abc\"\n14 leaf   cd      7     \"  bc\"\n15 leaf   zn      1     \" a\"  \n# i 27 more rows\n\n\nMit der Option print(n = 15) kannst du dir die ersten fünfzehn Zeilen ausgeben lassen. du musst also schauen, wie viele Zeilen dein Tibble hat und dann kannst du dir das ganze Tibble über die Funktion print() ausgeben lassen. Ich nutze immer diese Art der Ausgabe mit print() da es sicherer ist, als sich immer den ganzen Datensatz wiedergeben zu lassen. Mit sicherer meine ich, dass ich mir nicht die ganze R Console mit der Ausgabe zubaue."
  },
  {
    "objectID": "example-analysis-11.html",
    "href": "example-analysis-11.html",
    "title": "11  Daten eines automatischen Loggers",
    "section": "",
    "text": "Version vom Mai 10, 2023 um 14:11:38\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, readxl,\n               lubridate)\n\nDie Daten des Loggers liegen in einer Exceldatei ab. Wir haben jede Minute die Konzentration von CO\\(_2\\), die Temperatur und die relative Feuchte messen lassen. Da wir das Datum in einem ungünstigen Format vorliegen haben, müssen wir der Funktion as_date() die Tage, Monate und das Jahr als Format mitgeben. Wir schreiben '%d.%m.%Y' und meinen damit, dass erst der Tag, dann der Monat und dann das Jahr angeben ist. Getrennt sind die Tage, Monate und Jahre durch einen Punkt. Dann verbinden wir noch die Spalte des Datums mit der Spalte des Uhrzeit und bauen uns daraus eine date_time Spalte. Die date_time Spalte brauchen wir um später für jeden Zeitpunkt die gemessenen Werte darstellen zu können.\n\nlog_tbl <- read_excel(\"data/log_data.xlsx\") %>% \n  mutate(date = as_date(date, format = '%d.%m.%Y'),\n         date_time = as_datetime(str_c(date, \" \", time))) %>% \n  select(date_time, everything())\n\nInsgesamt hat unsere Datei \\(n = 35761\\) Beobachtungen für die Zeit vom 21. November 2022 bis zum 16. Dezember 2022. Dank unserem Tibble wird nicht die ganze Datei wiedergegeben sondern nur die ersten zehn Zeilen.\n\nlog_tbl\n\n# A tibble: 35,761 x 6\n   date_time           date       time       co2  temp  relh\n   <dttm>              <date>     <chr>    <dbl> <dbl> <dbl>\n 1 2022-11-21 12:30:00 2022-11-21 12:30:00   896  18.2  61.7\n 2 2022-11-21 12:31:00 2022-11-21 12:31:00   893  18.2  66.3\n 3 2022-11-21 12:32:00 2022-11-21 12:32:00   893  18.4  67.8\n 4 2022-11-21 12:33:00 2022-11-21 12:33:00   939  18.6  70  \n 5 2022-11-21 12:34:00 2022-11-21 12:34:00   939  18.8  70.6\n 6 2022-11-21 12:35:00 2022-11-21 12:35:00   975  19    70.3\n 7 2022-11-21 12:36:00 2022-11-21 12:36:00   980  19.2  71.6\n 8 2022-11-21 12:37:00 2022-11-21 12:37:00   986  19.4  71.8\n 9 2022-11-21 12:38:00 2022-11-21 12:38:00   994  19.5  71.9\n10 2022-11-21 12:39:00 2022-11-21 12:39:00   986  19.7  72.2\n# i 35,751 more rows\n\n\nWir immer müssen wir die Daten noch aus dem Wide-Format in das Long-Format überführen. Wir wollen dann auch die Outcomes nochmal anders benennen und machen die Umbenennung gleich bei der Erstellung der Faktoren. Dann sind wir auch schon fertig mit der Datenvorbereitung und können dann mit den Abbilungen beginnen.\n\nlog_plot_tbl <- log_tbl %>% \n  pivot_longer(cols = co2:relh, \n               names_to = \"outcome\",\n               values_to = \"rsp\") %>% \n  mutate(outcome = factor(outcome, \n                          labels = c(\"CO2\", \"Temperatur\", \"Rel. Luftfeuchtigkeit\"))) \n\nIn Abbildung 11.1 sehen wir alle Messdaten aufgetragen. Die Zeit für den Plot ist relativ überschaubar und wir erhalten das Ergebnis in wenigen Augenblicken. Daher spricht eigentlich nichts für diese Darstellung über alle Messdaten. Wir sehen, dass die Temperatur am Anfang höher war, als im späteren Verlauf. Auch haben wir einen Peak an CO\\(_2\\) um den 29. November sowie ein paar Tage später. Die Luftfeuchte zeigt im Laufe der Messungen eine immer höhere Variabilität.\n\nggplot(log_plot_tbl, aes(date_time, rsp, color = outcome)) +\n  theme_minimal() +\n  geom_line() +\n  facet_wrap(~ outcome, scales = \"free_y\", nrow = 3) +\n  scale_x_datetime(date_labels = \"%b %d\", date_breaks = \"3 day\",\n                   date_minor_breaks = \"1 day\") +\n  theme(legend.position = \"none\") +\n  labs(x = \"Datum\", y = \"\")\n\n\n\n\nAbbildung 11.1— Abbildung der drei Outcomes über alle Messpunkte.\n\n\n\n\nWir könnten über die summarise Funktion auch die Werte für jeden Tag mitteln und uns die Standardabweichung über den Tag als Wert für die Variabilität wiedergeben lassen.\n\nlog_plot_sum_tbl <- log_plot_tbl %>% \n  group_by(date, outcome) %>% \n  summarise(mean = mean(rsp),\n            median = median(rsp),\n            sd = sd(rsp))\n\nIn der Abbildung 11.2 plotten wir uns einmal die Mittelwerte für jeden Tag und die entsprechende Standardabweichung als schattierte Fläche. Hier sehen wir sehr schön den Abfall der Luftfeuchte sowie die eher konstante Temperatur ab dem 24. November. Auch wird der Peak in der CO\\(_2\\) Konzentration etwas ausgeglichen. Wir sehen den Effekt aber immer noch gut.\n\nggplot(log_plot_sum_tbl, aes(date, mean)) +\n  theme_minimal() +\n  geom_line(aes(color = outcome)) +\n  geom_ribbon(aes(ymin = mean - sd, \n                  ymax = mean + sd, \n                  fill = outcome), alpha = .2) +\n  facet_wrap(~ outcome, scales = \"free_y\", nrow = 3) +\n  scale_x_date(date_labels = \"%b %d\", date_breaks = \"3 day\",\n               date_minor_breaks = \"1 day\") +\n  theme(legend.position = \"none\") +\n  labs(x = \"Datum\", y = \"\")\n\n\n\n\nAbbildung 11.2— Abbildung der drei Outcomes gemittelt für jeden Tag mit der Standardabweichung für den jeweiligen Tag über alle Zeitpunkte.\n\n\n\n\nAbschließend könnten wir uns auch nur gewisse Zeitpunkte anschauen. Wir wollen also zum Beispiel nur die Werte für die Zeitpunkte um 6 Uhr, 12 Uhr und 18 Uhr visualisieren. Dann filtern wir nur diese Zeitpunkte heraus und können diese Zeitpunkte dann als eigenständige Linien einzeichnen.\n\nlog_plot_filter_tbl <- log_plot_tbl %>% \n  filter(time %in% c(\"06:00:00\", \"12:00:00\", \"18:00:00\"))\n\nIn Abbildung 11.3 sehen wir die Verläufe für die Messzeitpunkte um 6 Uhr, 12 Uhr und dann 18 Uhr. Spannender weise fällt um 12 Uhr die Luftfeuchtigkeit am 9. Dezember. Auch sind die Werte etwas anders für jede Uhrzeit. Da ich mich hier nicht auskenne, was wo und wie gemessen wurde, lassen wir das hier mal so stehen.\n\nggplot(log_plot_filter_tbl, aes(date, rsp, color = time)) +\n  theme_minimal() +\n  geom_line() +\n  facet_wrap(~ outcome, scales = \"free_y\", nrow = 3) +\n  scale_x_date(date_labels = \"%b %d\", date_breaks = \"3 day\",\n               date_minor_breaks = \"1 day\") +\n  labs(x = \"Datum\", y = \"\")\n\n\n\n\nAbbildung 11.3— Abbildung der drei Outcomes für den Zeitpunkt 6 Uhr, 12 Uhr und 18 Uhr."
  },
  {
    "objectID": "example-analysis-12.html",
    "href": "example-analysis-12.html",
    "title": "12  Pilze auf dem Rasen",
    "section": "",
    "text": "Version vom Mai 10, 2023 um 14:11:43\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, readxl, janitor,\n               psych, effectsize, parameters,\n               multcomp, emmeans, \n               conflicted)\n## resolve some conflicts with same function naming\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\n\nIm Folgenden wollen wir uns einen Datensatz mit einem etwas missglückten Design anschauen. In der Abbildung 12.1 sehen wir das Design einmal aus der Vogelperspektive. Wir haben auf zwei Grüns jweils zwei Blöcke angelegt. Leider finden wir nur Block I und II auf dem Grün 1 sowie die Blöcke III und IV auf dem Grün 2. Zwar haben wir dann unsere fünfzehn Behandlungen sauber auf jedem der beiden Grüns randomisiert, aber die ungüntige Blockeinteilung wird uns noch etwas Kopfzerbrechen bereiten.\n\n\n\nAbbildung 12.1— Vogelperspektive für unseren Versuch mit Pilzbefall. Leider finden wir nur den Block I und II auf dem Grün 1 sowie die Blöcke III und IV auf dem Grün 2.\n\n\nLaden wir einmal die Daten und teilen die Daten dann auch gleich für die Jahre 2020 und 2021 auf. Wir haben als Endpunkt AUDPC vorliegen. Es handelt sich hier um einen Art Summenscore, der je höher ist, desto stärker der Befall mit Pilzen war. Dann musst du noch wissen, dass wir es hier mit einer Art Beobachtungsstudie zu tun haben. Wir haben den Befall nicht induziert, sondern messen was an Pilzbefall so auf den beiden Golfgrüns passiert.\n\ngolf_tbl <- read_excel(\"data/golf_green_data.xlsx\") %>% \n  clean_names() %>% \n  mutate(block = as_factor(block),\n         trt = as_factor(trt),\n         golfgreen = as_factor(golfgreen)) \n\nSchauen wir uns einmal die Korrelation zwischen den AUDPC-Werten für die beiden Jahre an. In der Hoffnung, dass wenn wir immer die gleiche Position gemessen haben, gleiche Bedingungen in beiden Jahren hatten. In Abbildung 12.2 sehen wir die Korrelation einmal abgebildet. Zwar ist der Korrelation signifikant aber auch nicht sehr hoch. Das ist jetzt nicht so toll, da wir ja eigentlich hoffen würden, den gleichen Pilzbefall in beiden Jahren zu haben.\n\ngolf_tbl %>% \n  select(audpc_2020, audpc_2021) %>% \n  pairs.panels(smooth = TRUE, density = TRUE, method = \"kendall\", lm = TRUE, \n               cor = TRUE, ellipses = FALSE, stars = TRUE)    \n\n\n\n\nAbbildung 12.2— Korrelation zwischen den beiden Jahren der Messung.\n\n\n\n\nNun nutzen wir die Funktion pivot_longer() um unsere Daten dann für die Analye über die beiden Jahre getrennt vorzubereiten. Mir ist die Korrelation zu gering und ich glaube nicht daran, dass sich die Effeket in beiden Jahren sehr ähneln.\n\ngolf_tbl %<>% \n  pivot_longer(cols = c(audpc_2020, audpc_2021),\n               names_sep = \"_\",\n               names_to = c(\"prefix\", \"year\"),\n               values_to = \"audpc\") %>% \n  select(-prefix)\n\nIn der Abbildung 12.3 schauen wir uns auf zwei Arten einmal die Daten an. Zwar macht es nicht so viel Sinn die Behandlungen untereinander mit einer Linie zu verbinden, aber ich will hier nochmal schauen, wie verschieden die Jahre 2020 und 2021 sind. Hier verstärkt sich nochmal das Bild aus der Korrelationsanalyse. Die beiden Jahre sind sehr unterschiedlich, ich rechne die Analyse daher für die Jahre getrennt.\n\nggplot(golf_tbl, aes(trt, audpc, group = block, shape = block:golfgreen,\n                     color = golfgreen)) +\n  theme_minimal() +\n  geom_point() +\n  geom_line() +\n  facet_wrap(~ year, nrow = 2)\n\nggplot(golf_tbl, aes(trt, audpc, group = block, shape = block:golfgreen,\n                     color = golfgreen)) +\n  theme_minimal() +\n  geom_point() +\n  stat_smooth(se = FALSE) +\n  facet_wrap(~ year, nrow = 2)\n\n\n\n\n\n\n\n(a) Die einzenlnen Behandlungen verbunden\n\n\n\n\n\n\n\n(b) Verlauf über die Behandlungen.\n\n\n\n\nAbbildung 12.3— Visuelle Abschätzung des Vergleichs der beiden Jahre 2020 und 2021 über die Behandlungen.\n\n\n\nNun können wir uns das Problem einmal näher anschauen. Wenn wir das Modell mit den Behandlungen, den Block und dem Golfplatzgrün schätzen dann erhalten wir folgende Koeffizienten. Ganz am Ende sehen wir, dass wir für das golfgreen ein NA erhalten. Durch unser schlechtes Design, können wir dann den Effekt vom Golfgrün nicht mehr schätzen. Entweder den Effekt der Blöcke oder den Effekt der Golfgrüns. Ich wähle hier die Blöcke. Davon haben wir immerhin zwei pro Grün und hier wurde dann auch sauber randomisiert.\n\ngolf_tbl %>% \n  filter(year == \"2021\") %>% \n  lm(audpc ~ trt + block + golfgreen, data = .) %>% \n  coef() \n\n(Intercept)        trt2        trt3        trt4        trt5        trt6 \n  2836.8658  -2907.0125  -2581.6125  -2256.0500   -981.6875   -141.8000 \n       trt7        trt8        trt9       trt10       trt11       trt12 \n  -136.3125     95.4875   -650.0875    971.8375  -1366.9375   -757.6125 \n      trt13       trt14       trt15      block2      block3      block4 \n   923.0000   -393.1875   -208.5125    134.0333    603.8800   1427.1233 \n golfgreen2 \n         NA \n\n\nDann geht es wie immer los. Leider können wir nicht den Interaktionsterm schätzen, da unsere Blöcke nicht sauber randomisiert wurden. Daher kriegen wir einen Fehler, wenn wir den Interaktionsterm mit in das Modell nehmen. Also dann eben weniger. Wir nutzen die Funktion split() und dann die Funktion map() um die Analysen parallel über beide Jahre durchzufühen.\n\ngolf_lm_lst <- golf_tbl %>% \n  split(.$year) %>% \n  map(~lm(audpc ~ trt + block, data = .x))\n\nWir schauen uns einmal die \\(\\eta^2\\) an und sehen, dass die Behandlungen immerhin im Jahr 2020 gut 50% der Varianz erklären und im Jahr 2021 dann ut 40%. Der Blockeffekt ist auch im Jahr 2021 sehr viel mehr ausgeprägt.\n\ngolf_lm_lst %>%\n  map(eta_squared)\n\n$`2020`\n# Effect Size for ANOVA (Type I)\n\nParameter | Eta2 (partial) |       95% CI\n-----------------------------------------\ntrt       |           0.49 | [0.15, 1.00]\nblock     |           0.05 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n$`2021`\n# Effect Size for ANOVA (Type I)\n\nParameter | Eta2 (partial) |       95% CI\n-----------------------------------------\ntrt       |           0.39 | [0.00, 1.00]\nblock     |           0.14 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nWenn wir jetzt die ANOVA rechnen, sehen wir eine Punktlandung im Jahr 2021 für die Behandlungen. Wir haben einen \\(p\\)-Wert von 0.05, der exakt auf dem Signifikanzniveau liegt. Der Block ist aber auch sehr nahe dran an dem Signifikanzniveau.\n\ngolf_lm_lst %>% \n  map(anova) %>% \n  map(model_parameters)\n\n$`2020`\nParameter | Sum_Squares | df | Mean_Square |    F |     p\n---------------------------------------------------------\ntrt       |    5.25e+07 | 14 |    3.75e+06 | 2.93 | 0.004\nblock     |    2.74e+06 |  3 |    9.12e+05 | 0.71 | 0.550\nResiduals |    5.38e+07 | 42 |    1.28e+06 |      |      \n\nAnova Table (Type 1 tests)\n\n$`2021`\nParameter | Sum_Squares | df | Mean_Square |    F |     p\n---------------------------------------------------------\ntrt       |    7.55e+07 | 14 |    5.39e+06 | 1.94 | 0.050\nblock     |    1.87e+07 |  3 |    6.24e+06 | 2.24 | 0.098\nResiduals |    1.17e+08 | 42 |    2.79e+06 |      |      \n\nAnova Table (Type 1 tests)\n\n\nDann rehcnen wir noch die paarweisen Vergleiche und lassen uns aber das Compact letter display wiedergeben, da wir sonst bei 15 Vergleichen nicht mehr viel erkennen. Wir rechnen das Compact letter display auch hier getrennt für die beiden Jahre. Dann sortiere ich die Ausgabe noch nach der Behandlung, damit das Ergebnis auch vergleichbar ist.\n\ngolf_cld_lst <- golf_lm_lst %>% \n  map(emmeans, spec = ~trt) %>% \n  map(cld, Letters = letters, adjust = \"none\") %>% \n  map(arrange, trt)\n\ngolf_cld_lst\n\n$`2020`\n trt emmean  SE df lower.CL upper.CL .group \n 1     3375 566 42     2233     4517      ef\n 2      839 566 42     -303     1981  a     \n 3     1370 566 42      229     2512  ab    \n 4     1630 566 42      488     2772  abc   \n 5     2884 566 42     1742     4026   bcdef\n 6     3584 566 42     2442     4725      ef\n 7     2084 566 42      942     3226  abcde \n 8     3623 566 42     2481     4765      ef\n 9     3060 566 42     1918     4202    cdef\n 10    4237 566 42     3095     5379       f\n 11    2392 566 42     1250     3534  abcde \n 12    2528 566 42     1386     3670   bcde \n 13    3264 566 42     2122     4406     def\n 14    3333 566 42     2191     4475     def\n 15    1737 566 42      595     2879  abcd  \n\nResults are averaged over the levels of: block \nConfidence level used: 0.95 \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n$`2021`\n trt emmean  SE df lower.CL upper.CL .group\n 1     3378 835 42     1694     5062    cd \n 2      471 835 42    -1213     2155  a    \n 3      797 835 42     -888     2481  ab   \n 4     1122 835 42     -562     2806  abc  \n 5     2396 835 42      712     4081  abcd \n 6     3236 835 42     1552     4921    cd \n 7     3242 835 42     1558     4926    cd \n 8     3474 835 42     1789     5158    cd \n 9     2728 835 42     1044     4412  abcd \n 10    4350 835 42     2666     6034     d \n 11    2011 835 42      327     3695  abcd \n 12    2621 835 42      936     4305  abcd \n 13    4301 835 42     2617     5985     d \n 14    2985 835 42     1301     4669   bcd \n 15    3170 835 42     1485     4854   bcd \n\nResults are averaged over the levels of: block \nConfidence level used: 0.95 \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nIch möchte mich dann nochmal etwas strecken und extrahiere mir die Behandlungen und das Compact letter display für beide Jahre. Dann kann ich die beiden Datensätze nach den Behandlungen zusammenfügen und das Compact letter display direkt vergleichen. Wir sehen, dass wir hier einen ganz schönen Unterschied vorliegen haben. Es war also eine gute Idee, die beiden Jahre nicht zusammen auszuwerten. Ins Detail möchte ich hier nicht gehen, mir sagen ja die Behandlungen nichts.\n\ngolf_cld_lst %>% \n  map(select, trt, .group) %>% \n  reduce(left_join, by = \"trt\", suffix = c(\"_2020\", \"_2021\"))\n\n   trt .group_2020 .group_2021\n1    1          ef          cd\n2    2      a             a   \n3    3      ab            ab  \n4    4      abc           abc \n5    5       bcdef        abcd\n6    6          ef          cd\n7    7      abcde           cd\n8    8          ef          cd\n9    9        cdef        abcd\n10  10           f           d\n11  11      abcde         abcd\n12  12       bcde         abcd\n13  13         def           d\n14  14         def         bcd\n15  15      abcd           bcd"
  },
  {
    "objectID": "example-analysis-13.html#barplot-mit-compact-letter-display-und-abspeichern",
    "href": "example-analysis-13.html#barplot-mit-compact-letter-display-und-abspeichern",
    "title": "13  Steuerung der vegetativen Entwicklung krautiger Pflanzen (44B0608) - Teil 1",
    "section": "13.1 Barplot mit compact letter display und abspeichern",
    "text": "13.1 Barplot mit compact letter display und abspeichern\n\nstat_tbl <- basi_tbl %>% \n  group_by(versuchsgruppe) %>% \n  summarise(mean = mean(frischmasse),\n            sd = sd(frischmasse),\n            se = sd/sqrt(n()))\n\n\nggplot(stat_tbl, aes(x = versuchsgruppe, y = mean, \n                     fill = versuchsgruppe)) + \n  theme_bw() +\n  geom_bar(stat = \"identity\") +\n  geom_errorbar(aes(ymin = mean-sd, ymax = mean+sd),\n                width = 0.2) +\n  labs(x = \"Versuchsgruppe\", y = \"Frischmasse in [g]\") +\n  theme(legend.position = \"none\") +\n  scale_fill_okabeito() +\n  annotate(\"text\", \n           x = 1:4, \n           y = c(19, 31, 27, 37), \n           label = c(\"a\", \"bc\", \"b\", \"c\")) +\n  annotate(\"text\", x = 1, y = 35,\n           label = \"ANOVA = <0.001\", size = 3)\nggsave(\"img/barplot_frischmasse.png\", \n       width = 5, height = 3)\n\n\n\n\nAbbildung 13.1— Korrelation zwischen den beiden Jahren der Messung.\n\n\n\n\n\nggsave(\"img/barplot_trockenmasse.png\", \n       width = 5, height = 3)\n\n\nbasi_time_tbl <- basi_tbl %>% \n  select(versuchsgruppe, t1:t4) %>% \n  pivot_longer(cols = t1:t4,\n               values_to = \"values\",\n               names_to = \"timepoint\") %>% \n  mutate(timepoint = as_factor(timepoint),\n         time_num = as.numeric(timepoint))\n\n\nggplot(basi_time_tbl, aes(time_num, values, color = versuchsgruppe)) +\n  theme_bw() +\n  scale_color_okabeito() +\n  geom_jitter(position=position_dodge(0.3), shape = 4) +\n  stat_summary(fun.data=\"mean_sdl\", , fun.args = list(mult = 1), \n               geom=\"pointrange\", position=position_dodge(0.3))  +\n  stat_summary(fun = \"mean\", fun.min = \"min\", fun.max = \"max\", geom = \"line\",\n               position=position_dodge(0.3)) +\n  theme(legend.position = c(0.8, 0.2),\n        legend.background = element_rect(color=\"black\", \n                                         size=0.5, linetype=\"solid\"))\n\n\n\n\nAbbildung 13.2— Korrelation zwischen den beiden Jahren der Messung.\n\n\n\n\n\nbasi_time_fit <- lm(values ~ versuchsgruppe + timepoint + versuchsgruppe:timepoint, basi_time_tbl)\n\n\nbasi_time_fit %>% \n  emmeans(specs = ~ versuchsgruppe | timepoint) %>% \n  contrast(method = \"pairwise\", adjust = \"none\") %>% \n  as_tibble %>% \n  select(contrast, timepoint, p.value) %>% \n  mutate(p.value = format.pval(p.value, eps = 0.001, digits = 2)) %>% \n  print(n = Inf)\n\n# A tibble: 24 × 3\n   contrast                              timepoint p.value\n   <fct>                                 <fct>     <chr>  \n 1 Erde - (Erde+Fließ)                   t1        0.2265 \n 2 Erde - (Erde+Perlite)                 t1        0.2265 \n 3 Erde - (Erde+Perlite+Fließ)           t1        0.0053 \n 4 (Erde+Fließ) - (Erde+Perlite)         t1        1.0000 \n 5 (Erde+Fließ) - (Erde+Perlite+Fließ)   t1        0.1007 \n 6 (Erde+Perlite) - (Erde+Perlite+Fließ) t1        0.1007 \n 7 Erde - (Erde+Fließ)                   t2        0.9119 \n 8 Erde - (Erde+Perlite)                 t2        0.0499 \n 9 Erde - (Erde+Perlite+Fließ)           t2        0.1538 \n10 (Erde+Fließ) - (Erde+Perlite)         t2        0.0388 \n11 (Erde+Fließ) - (Erde+Perlite+Fließ)   t2        0.1250 \n12 (Erde+Perlite) - (Erde+Perlite+Fließ) t2        0.5807 \n13 Erde - (Erde+Fließ)                   t3        0.8250 \n14 Erde - (Erde+Perlite)                 t3        0.0388 \n15 Erde - (Erde+Perlite+Fließ)           t3        0.2710 \n16 (Erde+Fließ) - (Erde+Perlite)         t3        0.0229 \n17 (Erde+Fließ) - (Erde+Perlite+Fließ)   t3        0.1875 \n18 (Erde+Perlite) - (Erde+Perlite+Fließ) t3        0.3214 \n19 Erde - (Erde+Fließ)                   t4        0.1007 \n20 Erde - (Erde+Perlite)                 t4        0.2710 \n21 Erde - (Erde+Perlite+Fließ)           t4        0.7402 \n22 (Erde+Fließ) - (Erde+Perlite)         t4        0.0072 \n23 (Erde+Fließ) - (Erde+Perlite+Fließ)   t4        0.1875 \n24 (Erde+Perlite) - (Erde+Perlite+Fließ) t4        0.1538 \n\n\n\nbasi_time_fit %>% \n  emmeans(specs = ~ versuchsgruppe | timepoint) %>%\n  cld(Letters = letters, adjust = \"none\") \n\ntimepoint = t1:\n versuchsgruppe     emmean   SE df lower.CL upper.CL .group\n Erde                 15.0 1.27 64     12.5     17.5  a    \n Erde+Perlite         17.2 1.27 64     14.7     19.7  ab   \n Erde+Fließ           17.2 1.27 64     14.7     19.7  ab   \n Erde+Perlite+Fließ   20.2 1.27 64     17.7     22.7   b   \n\ntimepoint = t2:\n versuchsgruppe     emmean   SE df lower.CL upper.CL .group\n Erde+Fließ           19.8 1.27 64     17.3     22.3  a    \n Erde                 20.0 1.27 64     17.5     22.5  a    \n Erde+Perlite+Fließ   22.6 1.27 64     20.1     25.1  ab   \n Erde+Perlite         23.6 1.27 64     21.1     26.1   b   \n\ntimepoint = t3:\n versuchsgruppe     emmean   SE df lower.CL upper.CL .group\n Erde+Fließ           20.4 1.27 64     17.9     22.9  a    \n Erde                 20.8 1.27 64     18.3     23.3  a    \n Erde+Perlite+Fließ   22.8 1.27 64     20.3     25.3  ab   \n Erde+Perlite         24.6 1.27 64     22.1     27.1   b   \n\ntimepoint = t4:\n versuchsgruppe     emmean   SE df lower.CL upper.CL .group\n Erde+Fließ           20.2 1.27 64     17.7     22.7  a    \n Erde+Perlite+Fließ   22.6 1.27 64     20.1     25.1  ab   \n Erde                 23.2 1.27 64     20.7     25.7  ab   \n Erde+Perlite         25.2 1.27 64     22.7     27.7   b   \n\nConfidence level used: 0.95 \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same."
  },
  {
    "objectID": "example-analysis-14.html",
    "href": "example-analysis-14.html",
    "title": "14  Steuerung der vegetativen Entwicklung krautiger Pflanzen (44B0608) - Teil 2",
    "section": "",
    "text": "Version vom May 13, 2023 um 10:17:09\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, readxl, parameters,\n               effectsize, magrittr, multcomp,\n               multcompView, rcompanion, rstatix,\n               emmeans, see, performance, janitor,\n               patchwork, \n               conflicted)\n## resolve some conflicts with same function naming\nconflicts_prefer(dplyr::select)\nconflicts_prefer(dplyr::filter)\nconflicts_prefer(effectsize::eta_squared)\n\nEs geht hier um das Modul Steuerung der vegetativen Entwicklung krautiger Pflanzen (44B0608)\n\ngurke_raw_tbl <- read_excel(\"data/wachstum_gurke.xlsx\") %>% \n  clean_names() %>% \n  mutate(versuchsgruppe = as_factor(versuchsgruppe),\n         erntegewicht = ifelse(erntegewicht == 0, yes = NA, no = erntegewicht))\n\n\ngurke_len_tbl <- gurke_raw_tbl %>% \n  filter(str_detect(versuchsgruppe, \"L$\")) %>% \n  mutate(versuchsgruppe = factor(versuchsgruppe, \n                                 labels = c(\"Proloog\", \"Quarto\", \"Katrina\")))\n\ngurke_dia_tbl <- gurke_raw_tbl %>% \n  filter(str_detect(versuchsgruppe, \"D$\")) %>% \n  mutate(versuchsgruppe = factor(versuchsgruppe, \n                                 labels = c(\"Proloog\", \"Quarto\", \"Katrina\")))\n\n\ngurke_ernte_tbl <- gurke_len_tbl %>% \n  select(versuchsgruppe, erntegewicht)\n\n\nggplot(gurke_ernte_tbl, aes(versuchsgruppe, erntegewicht)) +\n  theme_bw() +\n  geom_point()\n\n\n\n\nAbbildung 14.1— Korrelation zwischen den beiden Jahren der Messung.\n\n\n\n\n\nfit <- lm(erntegewicht ~ versuchsgruppe, data = gurke_ernte_tbl)\n\nfit %>% \n  anova() %>% \n  parameters()\n\nParameter      | Sum_Squares | df | Mean_Square |     F |      p\n----------------------------------------------------------------\nversuchsgruppe |    5.35e+05 |  2 |    2.68e+05 | 44.36 | < .001\nResiduals      |    72413.12 | 12 |     6034.43 |       |       \n\nAnova Table (Type 1 tests)\n\nfit %>% \n  eta_squared()\n\n# Effect Size for ANOVA\n\nParameter      | Eta2 |       95% CI\n------------------------------------\nversuchsgruppe | 0.88 | [0.73, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\n\ngurke_ernte_tbl %$%\n  pairwise.t.test(erntegewicht, versuchsgruppe,\n                  pool.sd = TRUE, \n                  p.adjust.method = \"none\")\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  erntegewicht and versuchsgruppe \n\n        Proloog Quarto\nQuarto  8.4e-07 -     \nKatrina 5.0e-05 0.042 \n\nP value adjustment method: none \n\n\n\ngurke_ernte_tbl %$%\n  pairwise.t.test(erntegewicht, versuchsgruppe,\n                  pool.sd = TRUE, \n                  p.adjust.method = \"none\") %>% \n  extract2(\"p.value\") %>% \n  fullPTable() %>% \n  multcompLetters()\n\nProloog  Quarto Katrina \n    \"a\"     \"b\"     \"c\" \n\n\n\nstat_tbl <- gurke_ernte_tbl %>% \n  group_by(versuchsgruppe) %>% \n  summarise(mean = mean(erntegewicht, na.rm = TRUE),\n            sd = sd(erntegewicht, na.rm = TRUE),\n            se = sd/sqrt(n()))\n\n\nggplot(stat_tbl, aes(x = versuchsgruppe, y = mean, \n                     fill = versuchsgruppe)) + \n  theme_bw() +\n  geom_bar(stat = \"identity\") +\n  geom_errorbar(aes(ymin = mean-sd, ymax = mean+sd),\n                width = 0.2) +\n  labs(x = \"Versuchsgruppe\", y = \"Erntegewicht in [g]\") +\n  theme(legend.position = \"none\") +\n  annotate(\"text\", \n           x = 1:3, \n           y = c(635, 110, 260), \n           label = c(\"a\", \"b\", \"c\")) +\n  annotate(\"text\", x = 3, y = 700,\n           label = \"ANOVA = <0.001\", size = 5) +\n  scale_fill_okabeito()\n\n\n\n\nAbbildung 14.2— Korrelation zwischen den beiden Jahren der Messung.\n\n\n\n\n\nggsave(\"img/barplot_erntegewicht.png\", \n       width = 5, height = 3)\n\n\ngurke_time_len_tbl <- gurke_len_tbl %>% \n  select(-pfl, -erntegewicht) %>% \n  pivot_longer(cols = t1:t17,\n               values_to = \"length\",\n               names_to = \"time\") %>% \n  mutate(time_fct = as_factor(time),\n         time_num = as.numeric(time_fct))\n\n\nggplot(gurke_time_len_tbl, aes(time_num, length, color = versuchsgruppe)) +\n  theme_bw() +\n  geom_point() +\n  stat_summary(fun = \"mean\", fun.min = \"min\", fun.max = \"max\", geom = \"line\") +\n  scale_color_okabeito()\n\n\n\n\nAbbildung 14.3— Korrelation zwischen den beiden Jahren der Messung.\n\n\n\n\n\nggplot(gurke_time_len_tbl, aes(time_num, length, color = versuchsgruppe)) +\n  theme_bw() +\n  geom_point() +\n  stat_summary(fun = \"mean\", fun.min = \"min\", fun.max = \"max\", geom = \"line\") +\n  facet_wrap(~ versuchsgruppe) +\n  scale_color_okabeito() \n\n\n\n\nAbbildung 14.4— Korrelation zwischen den beiden Jahren der Messung.\n\n\n\n\n\nggplot(gurke_time_len_tbl, aes(time_fct, length, color = versuchsgruppe)) +\n  theme_bw() +\n  geom_boxplot() +\n  scale_color_okabeito()\n\n\n\n\nAbbildung 14.5— Korrelation zwischen den beiden Jahren der Messung.\n\n\n\n\n\nggplot(gurke_time_len_tbl, aes(time_num, length, color = versuchsgruppe)) +\n  theme_bw() +\n  geom_jitter(position=position_dodge(0.3), shape = 4) +\n  stat_summary(fun.data=\"mean_sdl\", , fun.args = list(mult = 1), \n               geom=\"pointrange\", position=position_dodge(0.3))  +\n  stat_summary(fun = \"mean\", fun.min = \"min\", fun.max = \"max\", geom = \"line\",\n               position=position_dodge(0.3)) +\n  scale_color_okabeito()\n\n\n\n\nAbbildung 14.6— Korrelation zwischen den beiden Jahren der Messung.\n\n\n\n\n\nlm(length ~ versuchsgruppe + time + versuchsgruppe:time, gurke_time_len_tbl) %>% \n  anova()\n\nAnalysis of Variance Table\n\nResponse: length\n                     Df Sum Sq Mean Sq F value  Pr(>F)    \nversuchsgruppe        2 2705.7 1352.84 64.7450 < 2e-16 ***\ntime                 16 3407.8  212.99 10.1933 < 2e-16 ***\nversuchsgruppe:time  29  882.8   30.44  1.4569 0.06953 .  \nResiduals           219 4576.0   20.89                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\ngurke_time_len_tbl %>% \n  filter(time_fct == \"t14\") %$%\n  pairwise.t.test(length, versuchsgruppe,\n                  pool.sd = FALSE, \n                  p.adjust.method = \"none\") %>% \n  extract2(\"p.value\") %>% \n  fullPTable() %>% \n  multcompLetters()\n\nProloog  Quarto Katrina \n    \"a\"     \"b\"    \"ab\" \n\n\n\nstat_tbl <- gurke_time_len_tbl %>% \n  group_by(versuchsgruppe, time_fct) %>% \n  summarise(mean = mean(length, na.rm = TRUE),\n            sd = sd(length, na.rm = TRUE),\n            se = sd/sqrt(n()),\n            cld_pos = mean + sd + 2)\n\n\np1 <- ggplot(stat_tbl, aes(x = time_fct, y = mean, \n                     fill = versuchsgruppe)) + \n  theme_bw() +\n  geom_bar(stat = \"identity\", position = position_dodge2(preserve = \"single\")) +\n  geom_errorbar(aes(ymin = mean-sd, ymax = mean+sd),\n                width = 0.5, position = position_dodge(0.9)) +\n  labs(x = \"Zeitpunkt\", fill = \"Versuchsgruppe\", y = \"Erntegewicht in [g]\") +\n  annotate(\"text\", x = 2, y = 30,\n           label = \"ANOVA = <0.001\", size = 3) +\n  theme(legend.position = \"top\") +\n  scale_fill_okabeito()\n\n\nstat_t14_tbl <- stat_tbl %>% \n  filter(time_fct == \"t14\")\n\n\np2 <- ggplot(stat_t14_tbl, aes(x = time_fct, y = mean, \n                       fill = versuchsgruppe)) + \n  theme_bw() +\n  geom_bar(stat = \"identity\", position = position_dodge2(preserve = \"single\")) +\n  geom_errorbar(aes(ymin = mean-sd, ymax = mean+sd),\n                width = 0.5, position = position_dodge(0.9)) +\n  labs(x = \"Zeitpunkt\", fill = \"Versuchsgruppe\", y = \"Erntegewicht in [g]\") +\n  annotate(\"text\", \n           x = c(0.7, 1, 1.3), \n           y = stat_t14_tbl$cld_pos, \n           label = c(\"a\", \"b\", \"ab\")) +\n  theme(legend.position = \"none\") +\n  scale_fill_okabeito()\n\n\np1 + p2 + \n  plot_layout(widths = c(7, 1))\n\n\n\n\nAbbildung 14.7— Korrelation zwischen den beiden Jahren der Messung.\n\n\n\n\n\nggsave(\"img/time_barplot.png\", \n       width = 8, height = 5)"
  },
  {
    "objectID": "example-analysis-15.html",
    "href": "example-analysis-15.html",
    "title": "15  Laser auf Erdbeeren",
    "section": "",
    "text": "Version vom May 11, 2023 um 08:11:36\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, readxl, parameters,\n               effectsize, magrittr, multcomp,\n               multcompView, rcompanion, rstatix,\n               emmeans, see, performance, fs,\n               janitor, broom,\n               conflicted)\n## resolve some conflicts with same function naming\nconflicts_prefer(dplyr::select)\nconflicts_prefer(dplyr::filter)\nconflicts_prefer(effectsize::eta_squared)\nconflicts_prefer(magrittr::set_names)\n\n\n\n\nAbbildung 15.1— Vogelperspektive für unseren Versuch mit Pilzbefall. Leider finden wir nur den Block I und II auf dem Grün 1 sowie die Blöcke III und IV auf dem Grün 2.\n\n\n\n## laden der spektren\nberry_files <- list.files(\"data/strawberry\",\n                          pattern = \"^E\", full.names = TRUE)\n\n\n\nberry_lst <- map(berry_files, read_table, \n                 skip = 2, col_names = FALSE, col_types = cols())\n\nberry_lst <- map(berry_files, function(x){\n  tmp_tbl <- read_table(x, \n                        skip = 2, col_names = FALSE, col_types = cols()) \n  file_name <- basename(x) %>% \n    path_ext_remove() %>% \n    str_replace_all(\"\\\\s\", \"_\")\n  tmp_tbl <- tmp_tbl %>% \n    set_names(c(\"wave\", file_name)) \n  return(tmp_tbl)\n})\n\n\nberry_tbl <- berry_lst %>% \n  reduce(left_join, by = \"wave\") %>% \n  pivot_longer(cols = E_1.1._w1:last_col(),\n               names_sep = \"\\\\._\",\n               values_to = \"values\",\n               names_to = c(\"E\", \"rep\")) %>% \n  group_by(wave, E) %>% \n  summarise(mean = mean(values))\n\n`summarise()` has grouped output by 'wave'. You can override using the\n`.groups` argument.\n\nggplot(berry_tbl, aes(wave, mean, color = E)) +\n  theme_bw() +\n  geom_line() +\n  theme(legend.position = \"none\")\n\n\n\n## laden des zuckergehalts\n\nsugar_tbl <- read_excel(\"data/strawberry_sugar.xlsx\") %>% \n  clean_names() %>% \n  select(-brixwert, -brix_mittel_note, -messwiederholung,\n         -g_zucker_l_saft_mittel_note, -oe_einzelfrucht) %>% \n  filter(!is.na(brix_einzelfrucht)) %>% \n  mutate(E = str_c(\"E_\", boniturnote, \".\", fruchtnummer))\n\n## beide Datensätze zusammen\nberry_sugar_tbl <- left_join(berry_tbl, sugar_tbl,\n                             by = c(\"E\" = \"E\")) %>% \n  #filter(boniturnote %in% c(1, 5)) %>% \n  mutate(boniturnote = as_factor(boniturnote))\n\n\nwave_vec <- berry_sugar_tbl %>% pull(wave) %>% unique()\n\nwave_vec <- wave_vec[1:20]\n\nlength(wave_vec)\n\n[1] 20\n\nberry_sugar_tbl %>% \n  filter(wave == 841) %>% \n  ggplot(aes(x = mean, y = g_zucker_l_saft, color = boniturnote)) +\n  theme_bw() +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = FALSE) #+\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n # facet_wrap(~ wave)\n\nberry_sugar_tbl %>% \n  filter(wave == 231) %>% \n  ggplot(aes(x = brix_einzelfrucht, y = mean)) +\n  theme_bw() +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~ wave)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nberry_sugar_tbl %>% \n  filter(wave == 231) %$% \n  lm(mean ~ brix_einzelfrucht) %>% \n  glance() %>% \n  pull(r.squared)\n\n[1] 0.01267805\n\nrsquare_vec <- map_dbl(wave_vec, function(x){\n  rsquare <- berry_sugar_tbl %>% \n    filter(wave == x) %$% \n    lm(brix_einzelfrucht ~ mean + boniturnote) %>%\n    glance() %>% \n    pull(adj.r.squared)\n  return(rsquare)\n}, .progress = TRUE) %>% \n  set_names(wave_vec) \n\nwhich.max(rsquare_vec)\n\n238 \n 15 \n\nrsquare_vec[15]\n\n      238 \n0.6111646 \n\nberry_sugar_tbl %>% \n  filter(wave == wave_vec[15]) %>% \n  ggplot(aes(x = mean, y = brix_einzelfrucht,\n             color = boniturnote)) +\n  theme_bw() +\n  geom_point() +\n  geom_text(aes(label = E)) +\n  stat_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~ wave)\n\n`geom_smooth()` using formula = 'y ~ x'"
  }
]