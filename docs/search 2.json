[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Beispielhafte Auswertungen",
    "section": "",
    "text": "Was wollen wir hier gemeinsam machen? Ich werde in den folgenden Kapiteln Daten auswerten. Manche Sachen sind dort auch doppelt drin oder aber kennst du schon aus meinen Vorlesungen. Mir macht es einfach Spaß Daten auszuwerten und ich muss mir die Sachen immer aufschreiben, damit ich auch das Thema durchdringe. Daher dient die Sammlung an Beispielen zum einen als ein Fundus für Beispiel in meinen Vorlesungen oder aber für dich als eine Inspiration was alles möglich ist. Eventuell findest du ja dein Problem wieder… oder hast eine andere Idee wie du noch deine Daten auswerten könntest. Guck also einfach mal durch ob du was passendes findest.\nAlle Beispiele sind aus meiner beratenden Tätigkeit hier an der Hochschule Osnabrück entstanden. Keins der Beispiele ist so echt. Ich habe die Daten geändert und Behandlungen entfernt oder dazu genommen. Teilweise habe ich mir die Geschichte um die Fragestellung auch einfach ausgedacht. Also die Beispiel sind dann biologisch inhaltlich vielleicht dann doch eher Unsinn, die statistsiche Methode darum sollte aber meistens korrekt sein."
  },
  {
    "objectID": "example-analysis-preface.html",
    "href": "example-analysis-preface.html",
    "title": "Beispielhafte Auswertungen",
    "section": "",
    "text": "Version vom March 16, 2023 um 09:01:23\n\n\n\n\n\n\nBeispielhafte Auswertungen per Video\n\n\n\nDu findest auf YouTube in der Playlist Spielweise in R (Level 3) viele der Analysen hier einmal als Video. Ich werde zwar alles nochmal hier als Text aufschreiben, aber manchmal ist das Sehen und Hören dann einfacher.\n\n\nWir wollen folgende R Pakete in diesem Skript nutzen.\n\npacman::p_load(tidyverse, magrittr, readxl, janitor,\n               broom, multcomp, emmeans, effectsize, report,\n               see, metR, parameters, multcompView,\n               modelsummary, rstatix, corrplot, psych,\n               ordinal, rcompanion, lme4, performance,\n               scales, lubridate,\n               conflicted)\n## resolve some conflicts with same function naming\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"chisq.test\", \"stats\")\nconflict_prefer(\"%+%\", \"ggplot2\")\nconflict_prefer(\"eta_squared\", \"effectsize\")\n\nWorum geht es in diesen Kapiteln? Ich rechne hier fröhlich Dinge und präsentiere dann die Ergebnisse. Das heißt, du findest hier beispielhafte Auswertungen, die eventuell auch deine Problemstellung betreffen. Da ich selber meist keine Ahnung von der Biologie hinter den Experimenten habe, musst du mir nachsehen, dass ich hier nie zu einem echten biologischen Schluss oder Auswertung komme.\n\nIn Kapitel 1 schauen wir uns einmal die Erstellung eines Isoplethendiagramms für Münster & Osnabrück aus den frei verfügbaren Daten des Deutschen Wetterdienstes an.\nIn Kapitel 2 rechnen wir einmal ein simples Beispiel für Zähldaten in zwei Gruppen. Wir haben hier nicht wiederholt gezählt, sondern nur jeweils einmal an einer Stelle.\nIn Kapitel 3 rechnen wir nochmal eine zweifaktorielle ANOVA mit Interaktionsterm durch. Wir kriegen dann unser Compact letter display wie auch die Konfidenzintervalle wieder.\nIn Kapitel 4 rechnen wir einen Games Howell Test für normalverteilte Daten mit Varianzheterogenität. Hier hauen wir ein wenig auf die Pauke und rechnen alles in wenigen Zeilen mit der Funktion map(). War eine spaßige Auswerung für mich, da ich hier mal wieder programmieren üben konnte.\nIn Kapitel 5 schauen wir uns ein größeres Beispiel für die Analyse von Wurzelbonituren an. Im Prinzip geht das natürlich auch alles für jede andere Bonitur. Wenn du eine andere Bonitur hast, kannst du natürlich auch den Weg der Analyse gehen."
  },
  {
    "objectID": "example-analysis-01.html",
    "href": "example-analysis-01.html",
    "title": "1  Isoplethendiagramm für Münster & Osnabrück",
    "section": "",
    "text": "Version vom March 16, 2023 um 08:48:00\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr)\n\nIm Folgenden zeige ich ein Beispiel für die Nutzung der entgeltfreien Informationen auf der DWD-Website. Wir finden dort auf der Seite die Klimadaten für Deutschland und natürlich auch die Daten für Münster/Osnabrück. Ich habe mir flux die Tageswerte runtergeladen und noch ein wenig den Header der txt-Datei angepasst. Du findest die Datei day_values_osnabrueck.txt wie immer auf meiner GitHub Seite. Du musst dir für andere Orte die Daten nur entsprechend zusammenbauen. Am Ende brauchen wir noch die Informationen zu den Tages- und Monatswerten damit wir auch verstehen, was wir uns da von der DWD runtergeladen haben. Ich nutze gleich nur einen Ausschnitt aus den Daten.\n\n\nWenn wir Geocomputation with R machen wollen, dann haben wir natürlich noch viele andere Möglichkeiten. Das verlinkte Buch hilft da weiter.\nDann lesen wir die Daten einmal ein und müssen dann eine Winkelzüge machen, damit wir aus dem Datum JJJJMMDD dann jeweils den Monat und den Tag extrahiert kriegen. Dann müssen wir die Monatszahl und die Tageszahl noch in eine Zahl umwandeln. Sonst geht es schlecht mit dem Zeichnen des Konturplots. Wir nehmen dann die Temperaturen TG, TN, TM und TX um diese Temperaturen in vier Konturplots zu zeigen.\n\nweather_tbl <- read_table(\"data/day_values_osnabrueck.txt\") %>% \n  mutate(JJJJMMDD = as.Date(as.character(JJJJMMDD), \"%Y%m%d\"),\n         day = as.numeric(format(JJJJMMDD, \"%d\")), \n         month = as.numeric(format(JJJJMMDD, \"%m\")), \n         year = as.numeric(format(JJJJMMDD, \"%Y\"))) %>% \n  select(month, day, TG, TN, TM, TX) %>% \n  na.omit() %>% \n  gather(temp, grad, TG:TX) %>% \n  mutate(temp = factor(temp, \n                       labels = c(\"Minimum der Temperatur in 5 cm (TG)\",\n                                  \"Minimum der Temperatur in 2 m (TN)\",\n                                  \"Mittel der Temperatur in 2 m (TM)\",\n                                  \"Maximum der Temperatur in 2 m (TX)\")))\n\nNachdem wir ordentlich an den Daten geschraubt haben können wir jetzt in Abbildung 1.1 die vier Konturplots sehen. Wir mussten noch das Spektrum der Farben einmal drehen, damit es auch mit den Temperaturfarben passt und wir haben noch ein paar Hilfslinien miteingezeichnet.\n\nggplot(weather_tbl, aes(month, day, z = grad)) +\n  theme_minimal() +\n  geom_contour_filled(bins = 13) +\n  geom_contour(binwidth = 2, color = \"black\") +\n  facet_wrap(~ temp, ncol = 2) + \n  scale_fill_brewer(palette = \"Spectral\", direction = -1) +\n  scale_x_continuous(breaks = 1:12) +\n  geom_vline(xintercept = 1:12, alpha = 0.9, linetype = 2) +\n  geom_hline(yintercept = c(5, 10, 15, 20, 25, 30), \n             alpha = 0.9, linetype = 2)\n\n\n\n\nAbbildung 1.1— Konturplot der verschiedenen Temperaturen."
  },
  {
    "objectID": "example-analysis-02.html",
    "href": "example-analysis-02.html",
    "title": "2  Analyse von Anzahlen in zwei Gruppen",
    "section": "",
    "text": "Version vom March 16, 2023 um 08:48:02\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, effectsize, see)\n\nIn dieser sehr simplen Analyse haben wir zwei Gruppen vorliegen. Die Gruppe 1 ist hat zwei Level oder Behandlungen abgekürzt mit I und II. Die Gruppe 2 hat insgesamt vier Level oder eben Behandlungen, die wir mit A, B, C und D bezeichnen. Wir haben jetzt für die jeweiligen Kombinationen auf dem Feld etwas gezählt. Wir haben also für jede dieser Kombinationen nur eine Zahl. Es ergbit sich somit die folgende Matrix an Zahlen.\n\nrel_mat <- matrix(c(45, 14, 4, 0,\n                    25, 32, 5, 1), nrow = 2, byrow = TRUE,\n                  dimnames = list(c(\"I\", \"II\"), c(\"A\", \"B\", \"C\", \"D\")))\nrel_mat\n\n    A  B C D\nI  45 14 4 0\nII 25 32 5 1\n\n\nNun können wir den \\(\\mathcal{X}^2\\)-Test nutzen, um zu testen, ob die Zahlen in der Matrix bzw. auf unseren Feld gelcihverteilt sind. Die Nullhypothese lautet, dass es keinen Zusammenhang zwischen der Gruppe 1 und der Gruppe 2 auf dem Feld gibt. Die Zahlen sind also rein zufällig in dieser Anordnung.\n\nchisq.test(rel_mat)\n\n\n    Pearson's Chi-squared test\n\ndata:  rel_mat\nX-squared = 13.869, df = 3, p-value = 0.003089\n\n\nWir erhalten einen sehr kleinen \\(p\\)-Wert mit \\(0.003\\). Wir können daher die Nullhypothese ablehnen, da der \\(p\\)-Wert kleiner ist als das Signifikanzniveau \\(\\alpha\\) mit 5%. Wir haben ein signifikantes Ergebnis. Wir können von einen Zusamenhang zwischen den beiden Gruppen ausgehen.\nMit Cramers V können wir auch noch die Effektstärke für einen \\(\\mathcal{X}^2\\)-Test berechnen.\n\ncramers_v(rel_mat) \n\nCramer's V (adj.) |       95% CI\n--------------------------------\n0.29              | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nDer Effekt ist mit \\(0.33\\) nicht besonders stark. Du kannst Cramers V wie die Korrelation interpretieren. Ein V von 0 bedeutet keinen Zusammenhang und ein V von 1 einen maximalen Zusammenhang. Wir wollen uns die Daten dann nochmal in einer Abbidlung anschauen. Dafür müssen wir die Matrix erstmal in einen Datensatz umwandeln und die Gruppen zu Faktoren machen.\n\nplot_tbl <- rel_mat %>% \n  as_tibble(rownames = \"group1\") %>% \n  gather(A:D, key = \"group2\", value = \"value\") %>% \n  mutate(group1 = as_factor(group1),\n         group2 = as_factor(group2))\n\nIn Abbildung 2.1 sehen wir die Matrix der Zähldaten für die beiden Gruppen nochmal visualisiert. Beim betrachten fällt auf, dass die beiden Level C und D kaum Zähldaten enthalten. Hier wäre zu überlegen die beiden Level aus der Analyse herauszunehmen und einen klassischen \\(\\mathcal{X}^2\\)-Test auf einer 2x2 Kreuztabelle zu rechnen.\n\nggplot(plot_tbl, aes(x = group2, y = value, fill = group1)) +\n  theme_bw() +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  labs(x = \"Gruppe 2\", y = \"Anzahl\", fill = \"Gruppe 1\") +\n  scale_fill_okabeito() \n\n\n\n\nAbbildung 2.1— Barplot der Zähldaten aus der Matrix."
  },
  {
    "objectID": "example-analysis-03.html",
    "href": "example-analysis-03.html",
    "title": "3  Auswertung zweifaktorielle ANOVA mit Interaktion",
    "section": "",
    "text": "Version vom March 16, 2023 um 08:47:52\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, see, readxl, knitr,\n               modelsummary, parameters, multcomp,\n               multcompView, emmeans, \n               conflicted)\n## resolve some conflicts with same function naming\nconflict_prefer(\"select\", \"dplyr\")\n\nHier kommt jetzt ein schönes Beispiel für eine Auswertung von einem dreifaktoriellen Design mit einer ANOVA. Passenderweise haben wir auch einen Interaktionsterm vorliegen. Unser dreifaktoriellen Design ist auch kein echtes dreifaktorielles Design. Wir müssen uns hier entscheiden, welcher der zwei Blockfaktoren nun unsere Wiederholung sein soll. Aber schreiben wir erstmal unser Modell auf, bevor wir das Modell mit Inhalt füllen.\n\\[\ny \\sim  f_1 + b_1 + b_2\n\\]\nIn unserem Beispiel schauen wir uns das Pflanzenwachstum growth in einer Klimakammer mit verschiedenen Belichtungsstufen light_intensity sowie der Position der Pflanze in der Lichtkammer. Die Pflanze hat eine Position im rack und dann im layer. So ergibt sich dann für uns folgendes ausgeschriebenes Modell.\n\\[\ngrowth \\sim light\\_intensity + layer + rack\n\\]\nIn dieser Form wird unser Modell aber leider nicht funktionieren. Wir hätten dann keine Wiederholungen mehr. Jede Pflanze würe dann exakt durch eine Faktorkombination beschrieben. Wir sehen gleich das Problem visualisiert. Vorher müssen wir uns aber einmal die Daten einlesen und eine Menge Faktoren erschaffen. Achtung, das Erschaffen der Faktoren ist hier sehr wichtig! Im Orginaldatensatz stehen nur Zahlen für die Faktoren. Wir kriegen dann ein echtes Problem.\n\nlight_tbl <- read_excel(\"data/light_intensity_data.xlsx\") %>% \n  mutate(rack = factor(rack, labels = c(\"left\", \"middle\", \"right\")),\n         layer = factor(layer, labels = c(\"1st\", \"2nd\", \"3rd\")),\n         light_intensity = factor(light_intensity, labels = c(\"low\", \"mid\", \"high\")),\n         growth = as.numeric(growth))\n\nIn der Tabelle 3.1 sehen wir nochmal einen Ausschnitt aus den Daten.\n\n\n\n\nTabelle 3.1— Auszug aus dem Daten zu der Lichtintensität.\n\n\nrack\nlayer\nlight_intensity\ngrowth\n\n\n\n\nleft\n1st\nlow\n17.1\n\n\nleft\n1st\nlow\n14.9\n\n\nleft\n1st\nlow\n10.3\n\n\nleft\n1st\nlow\n23.5\n\n\nleft\n1st\nlow\n0.3\n\n\nleft\n1st\nlow\n3.6\n\n\n…\n…\n…\n…\n\n\nright\n3rd\nlow\n19.5\n\n\nright\n3rd\nlow\n25.9\n\n\nright\n3rd\nlow\n35.5\n\n\nright\n3rd\nlow\n44.1\n\n\nright\n3rd\nlow\n10.7\n\n\nright\n3rd\nlow\n18.3\n\n\n\n\n\n\nNachdem wir die Daten eingelesen haben, schauen wir uns den Sachverhalt einmal für die drei Faktoren über die Level der einzelnen Faktoren an. Wir nutzen dafür die Funktion datasummary_crosstab() aus dem R Paket modelsummary. Wir können uns hier die Anzahl der Beobachtungen je Faktorlevelkombination einmal anschauen.\n\ndatasummary_crosstab(light_intensity ~ layer * rack, data = light_tbl,\n                     statistic = NULL)\n\n\n\n \n\n\n1st\n2nd\n3rd\n\n  \n    light_intensity \n    left \n    middle \n    right \n    left \n    middle \n    right \n    left \n    middle \n    right \n  \n \n\n  \n    low \n    6 \n    0 \n    0 \n    0 \n    6 \n    0 \n    0 \n    0 \n    6 \n  \n  \n    mid \n    0 \n    0 \n    6 \n    6 \n    0 \n    0 \n    0 \n    6 \n    0 \n  \n  \n    high \n    0 \n    6 \n    0 \n    0 \n    0 \n    6 \n    6 \n    0 \n    0 \n  \n\n\n\n\n\nWir sehen eine Menge Nullen. Das heißt, dass diese Faktorlevelkombinationen keine Beobachtungen haben. Dann können wir auch über diese Kombinationen keine Aussage treffen. Wenn wir entweder rack oder layer entfernen, sieht die Sache schon besser aus. Wir haben jetzt alle Faktorlevelkombinationen belegt. Wir müssen uns dann nur noch entscheiden, welchen Faktor wir ins Modell nehmen wollen.\n\ndatasummary_crosstab(light_intensity ~ layer, data = light_tbl,\n                     statistic = NULL)\ndatasummary_crosstab(light_intensity ~ rack, data = light_tbl,\n                     statistic = NULL)\n\n\n\n\n\n \n  \n    light_intensity \n    1st \n    2nd \n    3rd \n  \n \n\n  \n    low \n    6 \n    6 \n    6 \n  \n  \n    mid \n    6 \n    6 \n    6 \n  \n  \n    high \n    6 \n    6 \n    6 \n  \n\n\n\n\n\n\n\n \n  \n    light_intensity \n    left \n    middle \n    right \n  \n \n\n  \n    low \n    6 \n    6 \n    6 \n  \n  \n    mid \n    6 \n    6 \n    6 \n  \n  \n    high \n    6 \n    6 \n    6 \n  \n\n\n\n\n\n\n\n\n\nDas R Paket modelsummary bietet hier eine sehr große Auswahl an tollen Funktionen an um seine Daten übersichtlich zu gestalten.\nFür die Entscheidung welcher der beiden Faktoren rack oder layer mit ins Modekll soll, schauen wir uns einmal die Boxplots für die jeweiligen Fakoten an. In Abbildung 3.1 sehen wir einmal die Boxplots aufgeteilt nach rack.\n\nggplot(light_tbl, aes(light_intensity, growth, fill = rack)) +\n  theme_bw() +\n  geom_boxplot() +\n  scale_fill_okabeito()\n\n\n\n\nAbbildung 3.1— Boxplots des Pflanzenwachstums aufgeteilt nach rack.\n\n\n\n\nUnd wir sehen schon, da stimmt was nicht. Die Annahme der ANOVA ist, dass sich der Trend im ersten Faktorlevel für alle im Faktor über die anderen Faktoren gleicht. Das liest sich kryptisch, aber verdeutlichen wir es mal. Im Level low steigen alle Level des Faktors rack an. Wenn keine Interaktion vorliegen würde, dann müssten dieses Muster in dem Level mid und high ebenfalls annährend zu beobachten sein. Tut es aber nicht. Wir haben eine Interaktion zwischen light_intensity und rack visuell bestätigt.\n\nggplot(light_tbl, aes(light_intensity, growth, fill = layer)) +\n  theme_bw() +\n  geom_boxplot()  +\n  scale_fill_okabeito()\n\n\n\n\nAbbildung 3.2— Boxplots des Pflanzenwachstums aufgeteilt nach layer.\n\n\n\n\nDieses wirre Muster sehen wir dann auch in Abbildung 3.2. Hier passen die Trends des Faktors layer über die Faktorlevel low, mid und high auch wieder nicht. Schauen wir uns jetzt nochmal die ganze Sache aufgeteilt nach rack und layer an. Vielelicht werden wir dann etwas schlauer oder das Problem wird noch klarer.\n\nggplot(light_tbl, aes(light_intensity, growth, fill = rack)) +\n  theme_bw() +\n  geom_boxplot() +\n  facet_wrap(~ layer)  +\n  scale_fill_okabeito()\n\n\n\n\nAbbildung 3.3— Boxplots des Pflanzenwachstums aufgeteilt nach rack und layer.\n\n\n\n\nJetzt sehen wir etwas mehr. Im 1st Level liegen alle rack-Level auf einer Ebene. Ebenso liegen alle rack-Level auf einer Ebene im 2n Level. Das ganze Problem der Interaktion entsteht im 3rd Level. Hier ging etwas drunter und drüber im Pflanzenwachstum. Wir wissen jetzt, dass das dritte Layer anscheinend defekt war oder irgendwas dort mit den Racks nicht gestimmt hat.\nWir könnten jetzt das dritte Layer aus der Analyse werfen. Das wäre aber nur eine Möglichkeit. Wenn wir das tuen würden, dann würde wir auch die Interaktion los werden. Das wollen wir hier aber nicht, wir ziehen jetzt die Analyse einmal mit der Interaktion durch. Dafür bauen wir uns jetzt das lineare Modell und schauen uns einmal die ANOVA an.\n\nfit_1 <- lm(growth ~ light_intensity + layer + light_intensity:layer, \n            data = light_tbl)\nfit_1 %>% model_parameters()\n\nParameter                            | Coefficient |   SE |           95% CI | t(45) |      p\n---------------------------------------------------------------------------------------------\n(Intercept)                          |       11.62 | 2.80 | [  5.97,  17.26] |  4.14 | < .001\nlight intensity [mid]                |        1.43 | 3.96 | [ -6.55,   9.42] |  0.36 | 0.719 \nlight intensity [high]               |        3.70 | 3.96 | [ -4.28,  11.68] |  0.93 | 0.356 \nlayer [2nd]                          |        5.68 | 3.96 | [ -2.30,  13.67] |  1.43 | 0.159 \nlayer [3rd]                          |       14.05 | 3.96 | [  6.07,  22.03] |  3.54 | < .001\nlight intensity [mid] × layer [2nd]  |       -3.33 | 5.61 | [-14.62,   7.96] | -0.59 | 0.555 \nlight intensity [high] × layer [2nd] |       -5.92 | 5.61 | [-17.21,   5.37] | -1.06 | 0.297 \nlight intensity [mid] × layer [3rd]  |       -9.60 | 5.61 | [-20.89,   1.69] | -1.71 | 0.094 \nlight intensity [high] × layer [3rd] |      -25.90 | 5.61 | [-37.19, -14.61] | -4.62 | < .001\n\n\nErstmal sehen wir an den Modellparameters, dass hier wieder etwas nicht stimmt. Wir würden erwarten, dass der Effekt des Layers immer gleich ist. Hier ist der Effekt von dem 2nd Layer zu dem 3rd Layer fast dreimal so stark. Und eigentlich sollten die Layer den gleichen Effekt haben. Nämlich eigentlich keinen oder einen Effekt weit unter dem von der Lichtintensität. Das Layer ist eine technische Komponente.\n\nfit_1 %>% anova() %>% model_parameters()\n\nParameter             | Sum_Squares | df | Mean_Square |    F |      p\n----------------------------------------------------------------------\nlight_intensity       |      433.15 |  2 |      216.57 | 4.60 | 0.015 \nlayer                 |       70.92 |  2 |       35.46 | 0.75 | 0.477 \nlight_intensity:layer |     1138.80 |  4 |      284.70 | 6.04 | < .001\nResiduals             |     2120.85 | 45 |       47.13 |      |       \n\nAnova Table (Type 1 tests)\n\n\nWir sehen die visuelle Interaktion auch in der ANOVA Ausgabe als hoch signifikanten Term light_intensity:layer mit dem \\(p\\)-Wert \\(<0.001\\). Im Anschluss rechnen wir jetzt die paarweisen Vergleiche mit der Funktion emmeans(). Mit dem | geben wir an, dass wir die paarweisen Vergleiche für die Level von light_intensity getrennt für die Level vom layer rechnen wollen. Wenn du keine Adjustierung des \\(\\alpha\\)-Niveaus für die multiplen Vergleiche möchtest, dann wähle einfach die Option adjust = \"none\". Wir nutzen dann die Ausgabe nicht direkt sondern werden noch die Ausgabe etwas aufhübschen.\n\ncomp_1_obj <- fit_1 %>% \n  emmeans(specs = ~ light_intensity | layer) %>% \n  contrast(method = \"pairwise\", adjust = \"bonferroni\") \n\nIn dem Objekt comp_1_obj sind eine Menge Informationen enthalten. Ich kürze mir immer die Informationen und sortiere nochmal die Ergebnisse. Wir erhalten dann eine saubere Wiedergabe.\n\ncomp_1_obj %>% \n  summary %>% \n  as_tibble %>% \n  select(contrast, layer, p.value) %>% \n  mutate(p.value = format.pval(p.value, eps = 0.001, digits = 2))\n\n# A tibble: 9 × 3\n  contrast   layer p.value\n  <fct>      <fct> <chr>  \n1 low - mid  1st   1.0000 \n2 low - high 1st   1.0000 \n3 mid - high 1st   1.0000 \n4 low - mid  2nd   1.0000 \n5 low - high 2nd   1.0000 \n6 mid - high 2nd   1.0000 \n7 low - mid  3rd   0.1355 \n8 low - high 3rd   <0.001 \n9 mid - high 3rd   0.0028 \n\n\nNach der Adjustierung für die multiplen Vergleiche haben wir nur noch einen Effekt in dem 3rd Layer. Sonst haben die Lichtintensitäten keinen Einfluss auf die Wuchshöhe der Pflanzen. Da wir wissen, dass das 3rd Layer auch das defekte Layer war, sehen wir hier schon, dass wir keinen wirklichen Effekt durch das Licht vorliegen haben. Alles was wir gefunden haben, ist eben ein defektes 3rd Layer.\nDie 95% Konfidenzintervalle erhalten wir mit der Funktion confint(). Die Ergebnisse sind natürlich die gleichen. Wir sehen wieder keinen Unterschied zwischen den Lichtintensitäten außer in dem 3rd Layer.\n\nci_obj <- comp_1_obj %>% \n  confint() %>% \n  as_tibble() %>% \n  select(contrast, layer, estimate, conf.low = lower.CL, conf.high = upper.CL) \n\nci_obj\n\n# A tibble: 9 × 5\n  contrast   layer estimate conf.low conf.high\n  <fct>      <fct>    <dbl>    <dbl>     <dbl>\n1 low - mid  1st     -1.43    -11.3       8.42\n2 low - high 1st     -3.70    -13.6       6.16\n3 mid - high 1st     -2.27    -12.1       7.59\n4 low - mid  2nd      1.90     -7.96     11.8 \n5 low - high 2nd      2.22     -7.64     12.1 \n6 mid - high 2nd      0.317    -9.54     10.2 \n7 low - mid  3rd      8.17     -1.69     18.0 \n8 low - high 3rd     22.2      12.3      32.1 \n9 mid - high 3rd     14.0       4.18     23.9 \n\n\nIn der Abbildung 3.4 sehen wir dann die berechneten 95% Konfidenzintervalle nochmal visualisiert. Wenn wir einen Effekt haben, dann im 3rd Layer. In den restlichen 95% Konfidenzintervallen ist die Null mit enthalten, wir können also die Nullhypothese auf Gleichheit des Gruppenvergleiches nicht ablehnen.\n\nggplot(ci_obj, aes(contrast, y=estimate, ymin=conf.low, ymax=conf.high,\n                   color = layer, group = layer)) +\n  geom_hline(yintercept=0, linetype=\"11\", colour=\"grey60\") +\n  geom_errorbar(width=0.1, position = position_dodge(0.5)) + \n  geom_point(position = position_dodge(0.5)) +\n  scale_color_okabeito() +\n  coord_flip() +\n  theme_classic()\n\n\n\n\nAbbildung 3.4— Die 95% Konfidenzintervalle für die paarweisen Vergleiche aufgeteilt layer.\n\n\n\n\nNeben der Darstellung mit 95% Konfidenzintervallen ist auch die Darstellung mit dem compact letter display sehr beliebt. Wir nutzen dafür dann die Funktion cld(). Wir adjustieren uns wieder die Vergleiche nach Bonferroni. Im Weiteren trenne wir die Vergleiche auch wieder nach den Leveln für den Faktor layer auf.\n\ncld_obj <- fit_1 %>% \n  emmeans(specs = ~ light_intensity | layer)  %>%\n  cld(Letters = letters, adjust = \"bonferroni\") \n\ncld_obj\n\nlayer = 1st:\n light_intensity emmean  SE df lower.CL upper.CL .group\n low              11.62 2.8 45     4.65     18.6  a    \n mid              13.05 2.8 45     6.08     20.0  a    \n high             15.32 2.8 45     8.35     22.3  a    \n\nlayer = 2nd:\n light_intensity emmean  SE df lower.CL upper.CL .group\n high             15.08 2.8 45     8.11     22.1  a    \n mid              15.40 2.8 45     8.43     22.4  a    \n low              17.30 2.8 45    10.33     24.3  a    \n\nlayer = 3rd:\n light_intensity emmean  SE df lower.CL upper.CL .group\n high              3.47 2.8 45    -3.50     10.4  a    \n mid              17.50 2.8 45    10.53     24.5   b   \n low              25.67 2.8 45    18.70     32.6   b   \n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 3 estimates \nP value adjustment: bonferroni method for 3 tests \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nWir sehen wieder, dass wir nur in dem 3rd Layer Buchstabenunterschiede haben. Daher haben wir auch nur im 3rd Layer signifikante Ergebnisse. Wichtig ist, dass wir die Buchstaben nur pro Level des Layers vergleichen können, aber auf keinen Fall über die Layer hinweg. Das geht dann leider nicht. Die Ausgabe der Funktion emmeans() schlägt noch andere Darstellungsformen für die Vergleiche vor, du kannst gerne einmal die Funktionen pairs(), pwpp() oder pwpm() ausprobieren und schauen, ob dir die Visualisierung mehr sagt. Im ?sec-posthoc-emmeans gehe ich nochmal auf die verschiedene Darstellungsformen in emmeans ein.\nWenn wir das compact letter display mit deinem Barplot verbinden wollen, müssen wir uns etwas strecken. Zuerst sortieren wir die Ausgabe von cld_obj wieder in die korrekte Reihenfolge der Faktorenlevel. Dann können wir die Spalte .group direkt in ggplot() verwenden.\n\ncld_sort_obj <- cld_obj %>% \n  as_tibble() %>% \n  select(light_intensity, layer, .group) %>% \n  arrange(layer, light_intensity)\n\nIn Abbildung 3.5 sehen wir die Ausgabe des Barplots für die Daten und dann an die Balken geschrieben das compact letter display. Wichtig ist hier, dass die Buchstaben immer nur für ein Layer gelten. Wir können wegen der Interaktion nicht die Layer untereinander mit den Buchstaben vergleichen. Wir sehen wiederum, dass wir keine relevanten signifikanten Ergebnisse aus dem Experiment mitnehmen können.\n\nstat_tbl <- light_tbl %>% \n  group_by(light_intensity, layer) %>% \n  summarise(mean = mean(growth),\n            sd = sd(growth))\n\nggplot(stat_tbl, aes(x = layer, y = mean, group = light_intensity, \n                     fill = light_intensity)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_errorbar(aes(ymin = mean-sd, ymax = mean+sd),\n                width = 0.2, position = position_dodge(0.9)) +\n  annotate(\"text\", \n           x = c(0.7, 1, 1.3, 1.7, 2, 2.3, 2.7, 3, 3.3), \n           y = c(22, 21, 23, 24, 20, 23, 39, 25, 9), \n           label = pluck(cld_sort_obj, \".group\")) +\n  theme_bw() +\n  labs(fill = \"Behandlung\")  +\n  scale_fill_okabeito()\n\n\n\n\nAbbildung 3.5— Barplots für die Mittelwerte mit den entsprechenden Standardabweichungen und dem compact letter display für die paarweisen Vergleiche. Achtung die letter gelten nur in einem Level des Layers."
  },
  {
    "objectID": "index.html#sec-contact-mail",
    "href": "index.html#sec-contact-mail",
    "title": "Beispielhafte Auswertungen",
    "section": "Kontakt",
    "text": "Kontakt\nWie erreichst du mich? Am einfachsten über die gute, alte E-Mail. Bitte bachte, dass gerade kurz vor den Prüfungen ich mehr E-Mails kriege. Leider kann es dann einen Tick dauern.\n\n\n\n\n\nEinfach an j.kruppa@hs-osnabrueck.de schreiben. Du findest hier auch eine kurze Formulierungshilfe. Einfach auf den Ausklapppfeil klicken.\n\n\n\n\n\n\nE-Mailvorlage mit beispielhafter Anrede\n\n\n\n\n\nHallo Herr Kruppa,\n… ich belege gerade Ihr Modul Modulname und hätte eine Bitte/Frage/Anregung…\n… ich benötige Hilfe bei der Planung/Auswertung meiner Bachelorarbeit…\nMit freundlichen Grüßen\nM. Muster"
  },
  {
    "objectID": "example-analysis-04.html",
    "href": "example-analysis-04.html",
    "title": "4  Multiples Testen mit Games Howell Test",
    "section": "",
    "text": "Version vom March 16, 2023 um 08:51:35\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, readxl, knitr,\n               rstatix, multcompView)\n\nIn diesem Abschnitt wollen wir einen einfaktoriellen Datensatz auswerten. Das heißt, wir haben eine Spalte mit unseren Variantenfaktor mit 11 Leveln und dann aber auch neun Outcomes. Wir müssen also für alle neun Outcomes einen Test rechnen. Dementsprechend bauen wir uns Liste in R. In jedem der Listeneinträge ist nur die Spalte für die Variante und einem Outcome. Wir haben dann also am Ende eine Liste mit neun Listeneinträgen und pro Liste einen Datensatz mit zwei Spalten. Wir brauchen noch das R Paket multcompView für die Darstellung des compact letter display und das R Paket rstatix für die Anwendung der R Funktion games_howell_test(). Wir gehen nämlich von einem normalverteilten Outcome mit Varianzheterogenität aus. Das nehmen wir für jedes Outcome an, dann können wir immer das Gleiche auf den Daten rechnen.\nAlso erstmal die Daten einlesen, dann die Varianten bilden und die Zahlen wieder auf eine angemessene Länge runden. Ich nehme da immer auf Zweikommastellen, aber das hat hier eher mit der Übersicht zu tun. Gerne kannst du da auch mehr Kommastellen zulassen.\n\nsoil_tbl <- read_excel(\"data/soil_1fac_data.xlsx\") %>% \n  mutate(variante = str_c(variante, \"_\", amount),\n         variante = as_factor(variante),\n         across(where(is.numeric), round, 2)) %>% \n  select(-amount)\n\nIn der Tabelle 4.1 sehen wir nochmal einen Ausschnitt aus den Daten. Wir schmeißen amount aus den Daten, da die Spalte dann in der Variante aufgeht.\n\n\n\n\nTabelle 4.1— Auszug aus dem Daten zu der Lichtintensität.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariante\nfe\nk\nno3\nno4\nmg\nca\ndrymatter\nfreshweight\nheight\n\n\n\n\nHolzfasern_0\n0.26\n3.7\n3.28\n4.1\n0.43\n0.45\n11.24\n95.3\n22.17\n\n\nHolzfasern_0\n0.33\n3.66\n3.24\n4.05\n0.44\n0.47\n11.48\n86.15\n23\n\n\nHolzfasern_0\n0.27\n3.83\n3.64\n4.3\n0.48\n0.47\n11.11\n86.93\n28.17\n\n\nHolzfasern_0\n0.31\n3.66\n3.24\n3.75\n0.31\n0.43\n11.95\n89.68\n26.83\n\n\nTorf_30\n0.46\n2.89\n5.02\n4.95\n0.37\n0.39\n11.21\n70.82\n23.83\n\n\nTorf_30\n0.37\n3.41\n9.44\n5.01\n0.49\n0.42\n11.29\n66.23\n22.33\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\nAufgefasertes_30\n0.57\n9.65\n18.98\n6.5\n0.33\n0.29\n10.28\n73.8\n22\n\n\nAufgefasertes_30\n0.78\n27.65\n16.96\n7\n0.34\n0.32\n13.91\n74.18\n22.5\n\n\nAufgefasertes_50\n1.78\n22.91\n59.38\n8\n0.34\n0.25\n13.61\n45.58\n18.67\n\n\nAufgefasertes_50\n1.3\n36.06\n58.02\n8\n0.35\n0.25\n9.62\n37.93\n20\n\n\nAufgefasertes_50\n1.08\n3.66\n56.85\n7.99\n0.32\n0.25\n9.5\n43.98\n18.67\n\n\nAufgefasertes_50\n1.06\n3.75\n44.39\n7.99\n0.39\n0.24\n10.67\n35.48\n19.67\n\n\n\n\n\n\nIm Folgenden werden wir immer wieder die Funktion map() aus dem R Paket purrr nutzen um auf Listen zu rechnen. Die Funktion map() erlaubt auf allen Listeneinträgen die gleiche Funktion durchzuführen. Das macht es natürlich sehr angenehm, wenn wir immer das gleiche Modell auf unsere Daten rechnen wollen.\n\n\nWir immer gibt es auch Tutorien im Netz, wenn du mehr über purrr::map() erfahren willst. Es gibt das\npurrr tutorial und die Zusammenfassung in Apply functions with purrr::CHEAT SHEET.\nAls erstes müssen wir aber unsere Daten in das Long-Format kriegen. Dann können wir die Daten nach dem Outcome in der Spalte key die Daten in neuen Listeneinträge aufspalten. Warum neun? Wir haben eben neun Outcomes. Dann haben wir auch neun Listeneinträge. In jedem Listeneintrag ist die Spalte variante und die Spalte value. Der Name des Listeneintrags enthält dann den Namen des Outcomes. Mit dem .x übergeben wir iterativ jeden Listeneintrag in die Funktion select().\n\nsoil_lst <- soil_tbl %>%\n  gather(key, value, fe:height) %>%\n  split(.$key) %>%\n  map(~select(.x, -key))\n\nSchauen wir uns von jedem Listeneintrag einmal die ersten beiden Zeilen an. Das machen wir auch wieder mit map() indem wir die Funktion head() auf jedem Listeneintrag ausführen.\n\nsoil_lst %>% \n  map(~head(.x, 2))\n\n$ca\n# A tibble: 2 × 2\n  variante     value\n  <fct>        <dbl>\n1 Holzfasern_0  0.45\n2 Holzfasern_0  0.47\n\n$drymatter\n# A tibble: 2 × 2\n  variante     value\n  <fct>        <dbl>\n1 Holzfasern_0  11.2\n2 Holzfasern_0  11.5\n\n$fe\n# A tibble: 2 × 2\n  variante     value\n  <fct>        <dbl>\n1 Holzfasern_0  0.26\n2 Holzfasern_0  0.33\n\n$freshweight\n# A tibble: 2 × 2\n  variante     value\n  <fct>        <dbl>\n1 Holzfasern_0  95.3\n2 Holzfasern_0  86.2\n\n$height\n# A tibble: 2 × 2\n  variante     value\n  <fct>        <dbl>\n1 Holzfasern_0  22.2\n2 Holzfasern_0  23  \n\n$k\n# A tibble: 2 × 2\n  variante     value\n  <fct>        <dbl>\n1 Holzfasern_0  3.7 \n2 Holzfasern_0  3.66\n\n$mg\n# A tibble: 2 × 2\n  variante     value\n  <fct>        <dbl>\n1 Holzfasern_0  0.43\n2 Holzfasern_0  0.44\n\n$no3\n# A tibble: 2 × 2\n  variante     value\n  <fct>        <dbl>\n1 Holzfasern_0  3.28\n2 Holzfasern_0  3.24\n\n$no4\n# A tibble: 2 × 2\n  variante     value\n  <fct>        <dbl>\n1 Holzfasern_0  4.1 \n2 Holzfasern_0  4.05\n\n\nJetzt geht kannst du einmal map() in seiner vollen Schönheit sehen. Wir rechnen auf jeden Listeneintrag einmal den Games Howell Test. Dann steht in jedem der Listeneinträge das Ergebnis des Games Howell Test. Wir müssen dann noch die Spalte contrast aus der Ausgabe des Games Howell Test abändern, damit wir das compact letter display über die Funktion multcompLetters() nutzen können. Dann brauchen wir die adjustierten p-Werte und den Kontrast. Beides schieben wir dann in die Funktion multcompLetters() und lassen uns nur die Buchstaben des compact letter display wiedergeben. Am Ende kleben wir noch alle Einträge der einzelnen Listen mit der Funktion bind_rows() zu einem Datensatz zusammen. Vermutlich musst du die einzelnen Funktion selber mal Schritt für Schritt ausführen. Aber das hier war ja auch jetzt mal fortgeschrittene Programmierung.\n\nsoil_lst %>% \n  map(~games_howell_test(value ~ variante, data = .x)) %>% \n  map(~mutate(.x, contrast = str_c(.x$group1, \"-\", .x$group2))) %>% \n  map(~pull(.x, p.adj, contrast)) %>% \n  map(~multcompLetters(.x)$Letters) %>% \n  bind_rows(.id = \"outcome\") \n\n# A tibble: 9 × 12\n  outcome     Holzfase…¹ Torf_30 Kompo…² Kompo…³ Kompo…⁴ Gehäc…⁵ Gehäc…⁶ Gehäc…⁷\n  <chr>       <chr>      <chr>   <chr>   <chr>   <chr>   <chr>   <chr>   <chr>  \n1 ca          ab         ab      ab      cd      e       abc     a       b      \n2 drymatter   a          a       ab      ab      ab      ab      ab      b      \n3 fe          a          abc     ab      abc     bc      a       abc     abc    \n4 freshweight a          b       ac      abcdef  de      abc     bcd     f      \n5 height      abcde      abc     abcde   acd     ade     abcd    abcde   e      \n6 k           a          a       ab      ab      ab      ab      ab      ab     \n7 mg          ab         ab      a       a       a       ab      ab      b      \n8 no3         a          ab      ab      abcd    abc     a       abc     abc    \n9 no4         a          b       bcdef   c       d       ae      cf      c      \n# … with 3 more variables: Aufgefasertes_10 <chr>, Aufgefasertes_30 <chr>,\n#   Aufgefasertes_50 <chr>, and abbreviated variable names ¹​Holzfasern_0,\n#   ²​Kompostiertes_10, ³​Kompostiertes_30, ⁴​Kompostiertes_50, ⁵​Gehäckseltes_10,\n#   ⁶​Gehäckseltes_30, ⁷​Gehäckseltes_50\n\n\nWie du dann mit dem compact letter display weiterarbeitest und interpretierst, findest du dann im ?sec-compact-letter über das compact letter display für den Games Howell Test. Hier ist erstmal Schluss, sonst wird hier alles sehr wiederholend."
  },
  {
    "objectID": "example-analysis-05.html",
    "href": "example-analysis-05.html",
    "title": "5  Wurzelqualität in einem zweifaktoriellen Design",
    "section": "",
    "text": "Version vom March 16, 2023 um 09:00:56\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, readxl, see,\n               ordinal, parameters, emmeans, multcomp,\n               conflicted)\n## resolve some conflicts with same function naming\nconflict_prefer(\"select\", \"dplyr\")\n\nSehr häufig haben wir es auch in den Agrarwissenschaften mit Boniturnoten auf der Likert-Skala zu tun. Das heißt wir haben ein Outcome \\(y\\) mit Noten zwischen 1 und 9. Die ganzen Beobachtungen haben wir dann meist noch irgendwie in einem Block wiederholt. Neben den Problemen eine gute und richtige Bonitur praktisch durchzuführen, ist natürlich die Normalverteilung für das Outcome der Noten nicht gegeben. Wir könnten hier zum einen mit einem paarweisen Wilcoxon Rangsummentest ran oder aber wir nutzen die Funktion clm() aus dem R Paket ordnial für die Schätzung der Gruppenunterschiede. Wir nehmen hier mal die Funktion clm() da wir auch den Block mit ins Modell nehmen können.\nZuerst laden wir wieder die Daten. Dabei müssen wir dann noch die Notenspalte grade einmal als numerisch lassen und einmal in einen Faktor mutieren. Wir brauchen ein Faktor für die Funktion clm() aber eine numerische Variable für die Funktion ggplot(). Deshalb eben die Noten in einer doppelten Spalte.\n\nroot_tbl <- read_excel(\"data/root_2fac_data.xlsx\") %>% \n  mutate(treatment = str_c(treatment, \"_\", fert, \"_\", mixture)) %>% \n  select(treatment, block, grade) %>% \n  mutate(grade_fctr = as_factor(grade),\n         block = as_factor(block))\n\nIn Abbildung 5.1 sehen wir einmal die Abbildung der Noten. Wie immer sind über so viele Behandlungsstufen dann die Abbildungen sehr unübersichtlich. Ich nutze hier einen Dotplot, da du dann die einzelnen Beobachtungen besser sehen kannst. Wir sehen aber, dass es Effekte zwischen den Behandlungen gibt.\n\nggplot(root_tbl, aes(treatment, grade, fill = block)) +\n  theme_bw() +\n  geom_dotplot(binaxis = \"y\", stackdir='center', \n              position=position_dodge(0.6), dotsize = 0.75) +\n  scale_y_continuous(breaks = 1:9, limits = c(1,9)) +\n  scale_fill_okabeito() +\n  theme(axis.text.x = element_text(angle = 45, hjust=1))\n\n\n\n\nAbbildung 5.1— Boniturnoten der Wurzel über alle Behandlungslevel.\n\n\n\n\nWir könnten jetzt einen Friedman Test rechen. Der Friedman Test ist das Äquivalent zur einer zweifaktorielle ANOVA. Wenn wir aber die Funktion friedman.test() durchführen sehen wir, dass wir kein complete block design vorliegen haben. Wir können hier also einen Friedman Test nicht rechnen.\n\nfriedman.test(grade_fctr ~ treatment | block, root_tbl)\n\nError in friedman.test: not an unreplicated complete block design\nDa es eben mit dem Friedman Test nicht geht, nutzen wir Cumulative Link Models mit der Funktion clm(). Die Modelle erlauben ein ordinales Outcome zusammen mit einem multiplen linearen Modell zu schätzen. Das ist was wir wollen, damit können wir den Block mit berücksichtigen. Dann geben wir mit threshold = \"symmetric\" noch an, dass die Abstände in unserem Outcome \\(y\\) symmetrisch sind. Die Abstände zwischen den Notenstufen sind ja immer gleich groß.\n\nclm_fit <- clm(grade_fctr ~ treatment + block, data = root_tbl,\n               threshold = \"symmetric\")\n\nMit dem Fit des Modells können wir dann auch eine ANOVA rechnen, um zu schauen, ob wir überhaupt einen signifikanten Effekt unser Behandlung vorliegen haben. Das haben wir, zum einen ist die Behandlung signifikant und leider auch der Block. Das ist natürlich nicht so schön. Eine Interaktion können wir nicht rechnen, da wir zu wenig Fallzahl vorliegen haben.\n\nanova(clm_fit) %>% \n  model_parameters()\n\nParameter |  Chi2 | df |      p\n-------------------------------\ntreatment | 54.46 | 13 | < .001\nblock     | 24.64 |  3 | < .001\n\nAnova Table (Type 1 tests)\n\n\nNun nutzen wir das Modell einmal um uns die compact letters wiedergeben zu lassen. Dafür nutzen wir wieder die Funktion emmeans() aus dem R Paket emmeans. Am Ende sortieren wir nochmal nach der Behandlung und dann lassen wir uns nur die Behandlung und die Buchstaben wiedergeben. Wenn du mehr möchtest, kannst du auch im Kapitel über die Posthoc Tests oder der ordinalen logistischen Regression mehr Optionen anlesen.\n\nclm_fit %>% \n  emmeans(~ treatment) %>% \n  cld(Letters = letters) %>% \n  arrange(treatment) %>% \n  select(treatment, .group)\n\n          treatment .group\n1     CRTL_high_100    cde\n2   CRTL_normal_100   bcde\n3    WF0_high_30-70      e\n4    WF0_high_60-40  abcd \n5  WF0_normal_30-70    cde\n6  WF0_normal_60-40  ab   \n7    WF1_high_30-70   bcde\n8    WF1_high_60-40  abcde\n9  WF1_normal_30-70  abc  \n10 WF1_normal_60-40  abcde\n11   WF3_high_30-70  abcde\n12   WF3_high_60-40  a    \n13 WF3_normal_30-70     de\n14 WF3_normal_60-40   bcde\n\n\nWenn wir für den Block mit adjustieren wollen würden, können wir den Aufruf wie folgt ändern emmeans(~ treatment | block). Leider erhalten wir dann in der df Spalte den Eintrag Inf für eng. infinity und das ist nie ein gutes Zeichen. Wir haben auch die sich immer gleich wiederholende Abfolge der compact letters. Also macht es mehr kaputt als heile und wir lassen das Modell so wie es ist. Wir nehmen den Block als reine Wiederholung mit in das Modell rein."
  }
]